/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.
  warnings.warn("Unable to import Axes3D. This may be due to multiple versions of "
[I 2024-11-07 02:42:55,958] A new study created in memory with name: no-name-bcdbe301-97b5-4dfd-be47-e38f4042775a
[I 2024-11-07 02:42:55,961] A new study created in memory with name: no-name-cc93766f-a3fb-40d4-a0e3-ed6094304379
[I 2024-11-07 02:42:55,961] A new study created in memory with name: no-name-727e14f9-a3ad-4c59-9982-de37a2cfe186
[I 2024-11-07 02:42:55,961] A new study created in memory with name: no-name-6f68ba91-18d3-40ee-b128-8b147b0617bb
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-c5e69fe1-beeb-4c61-b4c6-bb378159dfa7
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-06297dd3-a88a-45b5-aa4f-f556f662f062
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-4041a585-e113-4d8b-b522-244ee5d2ec60
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-cde3d6f7-5d84-49e5-ba10-81b256652d3c
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-e8ec2d54-3348-474b-8134-6fae807ef1fc
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-0c07d28f-22ae-4f63-a427-0e562c8dcc9e
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-2ca48c0b-7371-4708-a9ec-60fe71d7da6d
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-68f1070c-3abd-45b9-bed4-48f237ff47ee
[I 2024-11-07 02:42:55,962] A new study created in memory with name: no-name-ba3c2456-6ca1-42a2-9002-5e1aa2da600a
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-95455e00-8ef7-42aa-8f2f-fb6dc3e4f943
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-41c0f312-e075-4bf7-ab76-31db9c4e5567
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-a73bd037-30e3-4273-b81a-eca2184097d9
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-17ea3035-da95-4374-b184-bf8d92437a7c
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-6b815821-9d88-44c4-8bcd-e1324e2145de
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-b6a1c1a3-f904-4187-ad5a-9f37d34095c8
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-181b7780-bc30-4917-a39c-c2a57f16a1e5
[I 2024-11-07 02:42:55,963] A new study created in memory with name: no-name-27c2e09d-9ece-46a2-842a-c7112af24e66
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-095e7381-e73f-440a-8903-6b1d12fec190
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-1b967af4-c340-431d-ab66-76aabbc7b184
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-b2f1f623-b9e7-4e58-8d00-a50ad7290085
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-677eb4da-e98c-42aa-b048-72a0db273db1
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-3e15d4c5-4950-436e-b4a3-80dc0058e276
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-9d0d931e-dbd9-4b1a-a73d-bc73a25ebc32
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-adae59bc-4f01-4300-9636-ebeafce30115
[I 2024-11-07 02:42:55,964] A new study created in memory with name: no-name-0e00d141-b2ba-4673-a3a4-0b19237a9bdd
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-5bba016f-de0f-4bcb-a7cf-e81437e56cb8
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-10cd6324-8536-4e50-b3c3-3e3484e0662a
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-7916ad4e-dcc0-43d8-a22d-b6672bb3cddd
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-5d0b06bb-314b-4945-baf5-486cc815e0bc
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-f12b139d-2154-42ee-9183-b64cf8b5b05d
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-91e035ec-cb58-4d35-9a31-1ca93fcc3ce1
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-1764a224-1486-4940-abff-255ab0019fdc
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-07d6c9b8-c018-4615-b850-e796fa6f0633
[I 2024-11-07 02:42:55,965] A new study created in memory with name: no-name-9d80b1a1-25bb-4575-ac8d-7dc06afc1d2d
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-23a623f2-19b8-479e-82b1-194e1c422e8e
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-e2ec9932-3c7b-4a50-a2e6-8a7d65f59677
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-da81e391-e9be-4c40-a2a2-ff816a1fd3aa
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-6cddbdf9-60ee-4d02-8c25-f7079e47dba2
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-e3c979c3-e034-4b86-bf64-62116c6063b9
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-25dad2c0-4fb5-40fc-85a3-5a027b570aca
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-5e6a394b-bb71-4e70-92bd-a63236da39a7
[I 2024-11-07 02:42:55,966] A new study created in memory with name: no-name-0017ef3c-3f07-4407-a20a-ebe0454657d7
[I 2024-11-07 02:42:55,967] A new study created in memory with name: no-name-ae4f0d6e-59c2-48ce-9e7c-8875c3dbcfb9
[I 2024-11-07 02:42:55,967] A new study created in memory with name: no-name-b12181b9-5df8-4e25-83ab-4b813b353230
[I 2024-11-07 02:43:09,193] Trial 0 finished with value: 7.216425249044551e-16 and parameters: {'lr': 0.0001, 'batch_size': 64}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:43:23,088] Trial 1 finished with value: 0.0008939266904818419 and parameters: {'lr': 0.01, 'batch_size': 32}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:43:34,594] Trial 2 finished with value: 9.42964056443769e-16 and parameters: {'lr': 0.0001, 'batch_size': 64}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:43:43,541] Trial 3 finished with value: 4.247575824415396e-13 and parameters: {'lr': 0.0001, 'batch_size': 256}. Best is trial 0 with value: 7.216425249044551e-16.
Initialising parameters...
Loading landscapes...
Creating studies...
Running studies...
[32mOptimising hyperparameters for model: linear[39m
Optimising K=0
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.08680279132647392
Epoch: 1, Val loss: 0.03810748246172045
Epoch: 2, Val loss: 0.021099745287981808
Epoch: 3, Val loss: 0.011401176229756102
Epoch: 4, Val loss: 0.005557071146929366
Epoch: 5, Val loss: 0.0023503110258497745
Epoch: 6, Val loss: 0.0008291988096271569
Epoch: 7, Val loss: 0.0002346466956849202
Epoch: 8, Val loss: 5.057627278537804e-05
Epoch: 9, Val loss: 7.809132313969969e-06
Epoch: 10, Val loss: 7.901615747045087e-07
Epoch: 11, Val loss: 4.7253626834804095e-08
Epoch: 12, Val loss: 1.4545787475740548e-09
Epoch: 13, Val loss: 1.8477986272194144e-11
Epoch: 14, Val loss: 6.65251719007582e-13
Epoch: 15, Val loss: 3.997236722419864e-13
Epoch: 16, Val loss: 2.848702936795643e-13
Epoch: 17, Val loss: 1.7519895368905885e-13
Epoch: 18, Val loss: 1.0625119458400713e-13
Epoch: 19, Val loss: 7.077933368926198e-14
Epoch: 20, Val loss: 4.4971210326909853e-14
Epoch: 21, Val loss: 2.6142998895030403e-14
Epoch: 22, Val loss: 1.6590509917011585e-14
Epoch: 23, Val loss: 1.0139446873102372e-14
Epoch: 24, Val loss: 6.7555242803839905e-15
Epoch: 25, Val loss: 4.510817600333594e-15
Epoch: 26, Val loss: 2.672629277897006e-15
Epoch: 27, Val loss: 1.8740028391874137e-15
Epoch: 28, Val loss: 1.5107585423861816e-15
Epoch: 29, Val loss: 1.0702174331938532e-15
Epoch: 30, Val loss: 7.216425249044551e-16
Early stopping at epoch 30
Best validation loss this trial: 7.216425249044551e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.100555606835962e-15
Epoch: 1, Val loss: 1.2445524820384129e-15
Epoch: 2, Val loss: 4.301753849531112e-15
Epoch: 3, Val loss: 2.9563959349932673e-14
Epoch: 4, Val loss: 0.002901355660834915
Epoch: 5, Val loss: 2.997656164621122e-13
Epoch: 6, Val loss: 0.00021814581682869734
Epoch: 7, Val loss: 1.3105926669886737e-12
Epoch: 8, Val loss: 6.206357786265246e-05
Epoch: 9, Val loss: 8.430680173061664e-06
Epoch: 10, Val loss: 6.208750571795074e-06
Epoch: 11, Val loss: 1.014520017594074e-06
Epoch: 12, Val loss: 1.761144896093348e-05
Epoch: 13, Val loss: 3.1869004135559076e-10
Epoch: 14, Val loss: 6.652205273450812e-14
Epoch: 15, Val loss: 9.984273346547595e-07
Epoch: 16, Val loss: 1.26262219105734e-05
Epoch: 17, Val loss: 0.0003658673266082222
Epoch: 18, Val loss: 4.9695593409904905e-05
Epoch: 19, Val loss: 7.747373269027862e-05
Epoch: 20, Val loss: 0.0008939266904818419
Early stopping at epoch 20
Best validation loss this trial: 0.0008939266904818419
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.12641807919384068
Epoch: 1, Val loss: 0.06567220389842987
Epoch: 2, Val loss: 0.04042261596928295
Epoch: 3, Val loss: 0.024516699970978446
Epoch: 4, Val loss: 0.013813263879945645
Epoch: 5, Val loss: 0.007109772640829667
Epoch: 6, Val loss: 0.0032881148314724364
Epoch: 7, Val loss: 0.0013386355245358541
Epoch: 8, Val loss: 0.00046671416513964086
Epoch: 9, Val loss: 0.00013399723348601794
Epoch: 10, Val loss: 2.9305290575930252e-05
Epoch: 11, Val loss: 4.531542238035974e-06
Epoch: 12, Val loss: 4.3125344602122226e-07
Epoch: 13, Val loss: 2.242627505921746e-08
Epoch: 14, Val loss: 5.297377790461332e-10
Epoch: 15, Val loss: 5.041560114391029e-12
Epoch: 16, Val loss: 7.421849014068724e-13
Epoch: 17, Val loss: 4.809490185267612e-13
Epoch: 18, Val loss: 2.9051777815912487e-13
Epoch: 19, Val loss: 1.7801886125865538e-13
Epoch: 20, Val loss: 1.2218798054423506e-13
Epoch: 21, Val loss: 7.558391584651173e-14
Epoch: 22, Val loss: 4.361701505592184e-14
Epoch: 23, Val loss: 2.745637369359881e-14
Epoch: 24, Val loss: 1.6980444075378944e-14
Epoch: 25, Val loss: 1.0986490426367507e-14
Epoch: 26, Val loss: 7.107532067964179e-15
Epoch: 27, Val loss: 3.5533675447981944e-15
Epoch: 28, Val loss: 2.706165230772231e-15
Epoch: 29, Val loss: 1.4794758401327102e-15
Epoch: 30, Val loss: 1.2367663913913503e-15
Epoch: 31, Val loss: 9.987574578760064e-16
Epoch: 32, Val loss: 9.42964056443769e-16
Early stopping at epoch 32
Best validation loss this trial: 9.42964056443769e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.4015419026215871
Epoch: 1, Val loss: 0.31993840287129083
Epoch: 2, Val loss: 0.2538151890039444
Epoch: 3, Val loss: 0.2012064903974533
Epoch: 4, Val loss: 0.15997397328416507
Epoch: 5, Val loss: 0.12806810066103935
Epoch: 6, Val loss: 0.1038762534658114
Epoch: 7, Val loss: 0.08560189480582873
Epoch: 8, Val loss: 0.07192322512467703
Epoch: 9, Val loss: 0.061637934173146884
Epoch: 10, Val loss: 0.053745774738490584
Epoch: 11, Val loss: 0.047517398620645204
Epoch: 12, Val loss: 0.04241818655282259
Epoch: 13, Val loss: 0.03810834257553021
Epoch: 14, Val loss: 0.03433920492728551
Epoch: 15, Val loss: 0.030941212798158327
Epoch: 16, Val loss: 0.027827438960472742
Epoch: 17, Val loss: 0.024957328693320353
Epoch: 18, Val loss: 0.022297811911751828
Epoch: 19, Val loss: 0.01983231275031964
Epoch: 20, Val loss: 0.01754547090580066
Epoch: 21, Val loss: 0.015441527388369044
Epoch: 22, Val loss: 0.013511535168314973
Epoch: 23, Val loss: 0.011756970003868142
Epoch: 24, Val loss: 0.010162818028281133
Epoch: 25, Val loss: 0.00872436569382747
Epoch: 26, Val loss: 0.007440710347145796
Epoch: 27, Val loss: 0.006299748776170114
Epoch: 28, Val loss: 0.005294811143539846
Epoch: 29, Val loss: 0.00441354497646292
Epoch: 30, Val loss: 0.0036484911805018784
Epoch: 31, Val loss: 0.0029906372539699076
Epoch: 32, Val loss: 0.00242862820935746
Epoch: 33, Val loss: 0.0019527196573714415
Epoch: 34, Val loss: 0.0015546566108241677
Epoch: 35, Val loss: 0.0012245052862757196
Epoch: 36, Val loss: 0.0009540272178128362
Epoch: 37, Val loss: 0.0007337013560269649
Epoch: 38, Val loss: 0.0005572299346871053
Epoch: 39, Val loss: 0.0004178469719287629
Epoch: 40, Val loss: 0.0003086025346419774
Epoch: 41, Val loss: 0.00022473039619702224
Epoch: 42, Val loss: 0.00016092757311222765
Epoch: 43, Val loss: 0.00011322534701321274
Epoch: 44, Val loss: 7.820853000642577e-05
Epoch: 45, Val loss: 5.3000558546045794e-05
Epoch: 46, Val loss: 3.515368741015361e-05
Epoch: 47, Val loss: 2.2795684162701946e-05
Epoch: 48, Val loss: 1.444091327963785e-05
Epoch: 49, Val loss: 8.903990616696926e-06
Epoch: 50, Val loss: 5.350803561062397e-06
Epoch: 51, Val loss: 3.125559919681109e-06
Epoch: 52, Val loss: 1.7644922422732634e-06
Epoch: 53, Val loss: 9.61912555415741e-07
Epoch: 54, Val loss: 5.081001593983577e-07
Epoch: 55, Val loss: 2.580000237154915e-07
Epoch: 56, Val loss: 1.259592605625433e-07
Epoch: 57, Val loss: 5.8846397029318114e-08
Epoch: 58, Val loss: 2.6276385038196772e-08
Epoch: 59, Val loss: 1.116216020354462e-08
Epoch: 60, Val loss: 4.504067178482766e-09
Epoch: 61, Val loss: 1.717090847286329e-09
Epoch: 62, Val loss: 6.156239083325114e-10
Epoch: 63, Val loss: 2.0809814751017916e-10
Epoch: 64, Val loss: 6.556981739976505e-11
Epoch: 65, Val loss: 1.9155011893775183e-11
Epoch: 66, Val loss: 6.025344098080828e-12
Epoch: 67, Val loss: 1.8928574383539806e-12
Epoch: 68, Val loss: 8.753029786077772e-13
Epoch: 69, Val loss: 7.211113939771993e-13
Epoch: 70, Val loss: 6.238281140934452e-13
Epoch: 71, Val loss: 5.790148543642395e-13
Epoch: 72, Val loss: 5.099018447780648e-13
Epoch: 73, Val loss: 4.600674017460916e-13
Epoch: 74, Val loss: 4.247575824415396e-13
Early stopping at epoch 74
Best validation loss this trial: 4.247575824415396e-13
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.010008373321428642
Epoch: 1, Val loss: 0.0005124890012666583
Epoch: 2, Val loss: 1.1474062630132736e-05
Epoch: 3, Val loss: 9.264515994291904e-08
Epoch: 4, Val loss: 2.400913302914794e-10
Epoch: 5, Val loss: 2.2151093948297974e-13
Epoch: 6, Val loss: 9.443540800508264e-14
Epoch: 7, Val loss: 7.332625902464543e-14
Epoch: 8, Val loss: 6.129015376386582e-14
[I 2024-11-07 02:43:48,221] Trial 4 finished with value: 1.6546507291233222e-15 and parameters: {'lr': 0.001, 'batch_size': 128}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:43:55,708] Trial 5 finished with value: 7.541504131732548e-05 and parameters: {'lr': 0.01, 'batch_size': 64}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:44:00,553] Trial 6 finished with value: 2.8066125350880672e-15 and parameters: {'lr': 0.001, 'batch_size': 128}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:44:00,756] Trial 7 pruned. 
[I 2024-11-07 02:44:00,958] Trial 8 pruned. 
[I 2024-11-07 02:44:15,856] Trial 9 finished with value: 4.4504818383290677e-07 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:44:16,218] Trial 10 pruned. 
[I 2024-11-07 02:44:16,577] Trial 11 pruned. 
[I 2024-11-07 02:44:16,935] Trial 12 pruned. 
[I 2024-11-07 02:44:17,299] Trial 13 pruned. 
[I 2024-11-07 02:44:17,425] Trial 14 pruned. 
[I 2024-11-07 02:44:17,784] Trial 15 pruned. 
[I 2024-11-07 02:44:25,254] Trial 16 finished with value: 4.560354599927186e-05 and parameters: {'lr': 0.01, 'batch_size': 64}. Best is trial 0 with value: 7.216425249044551e-16.
[I 2024-11-07 02:44:25,612] Trial 17 pruned. 
[I 2024-11-07 02:44:26,279] Trial 18 pruned. 
[I 2024-11-07 02:44:28,906] Trial 19 finished with value: 7.476739708381212e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 0 with value: 7.216425249044551e-16.
Epoch: 9, Val loss: 4.527497209025194e-14
Epoch: 10, Val loss: 3.4712364885108217e-14
Epoch: 11, Val loss: 2.864770976533166e-14
Epoch: 12, Val loss: 2.3287822264740052e-14
Epoch: 13, Val loss: 1.9233585632345852e-14
Epoch: 14, Val loss: 1.4631506236271727e-14
Epoch: 15, Val loss: 1.2434973046928672e-14
Epoch: 16, Val loss: 9.259358429067541e-15
Epoch: 17, Val loss: 5.592637955927855e-15
Epoch: 18, Val loss: 5.2836877562580415e-15
Epoch: 19, Val loss: 3.887496215531485e-15
Epoch: 20, Val loss: 2.938628470187582e-15
Epoch: 21, Val loss: 2.4363770790266116e-15
Epoch: 22, Val loss: 2.030436905145499e-15
Epoch: 23, Val loss: 1.6546507291233222e-15
Early stopping at epoch 23
Best validation loss this trial: 1.6546507291233222e-15
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 3.4741957878754117e-15
Epoch: 1, Val loss: 1.8442697076951665e-15
Epoch: 2, Val loss: 1.4044023100481097e-15
Epoch: 3, Val loss: 1.3147107867141646e-15
Epoch: 4, Val loss: 1.756319383657466e-15
Epoch: 5, Val loss: 1.3130642542091726e-15
Epoch: 6, Val loss: 1.8767759743795396e-15
Epoch: 7, Val loss: 1.6790829639915491e-15
Epoch: 8, Val loss: 1.8265246841825945e-15
Epoch: 9, Val loss: 2.6448846087355227e-15
Epoch: 10, Val loss: 4.1197521353504045e-15
Epoch: 11, Val loss: 2.187678159789106e-14
Epoch: 12, Val loss: 1.654976297342265e-12
Epoch: 13, Val loss: 0.0001663715756099281
Epoch: 14, Val loss: 2.2146990192758938e-07
Epoch: 15, Val loss: 4.4575644322197375e-05
Epoch: 16, Val loss: 5.364589208639724e-07
Epoch: 17, Val loss: 5.852840710844263e-05
Epoch: 18, Val loss: 5.912376215657523e-07
Epoch: 19, Val loss: 1.785704395367882e-09
Epoch: 20, Val loss: 7.541504131732548e-05
Early stopping at epoch 20
Best validation loss this trial: 7.541504131732548e-05
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018509028937225624
Epoch: 1, Val loss: 0.0012473492514168433
Epoch: 2, Val loss: 4.660684135160409e-05
Epoch: 3, Val loss: 8.677402827018696e-07
Epoch: 4, Val loss: 6.912290559144702e-09
Epoch: 5, Val loss: 2.1441303093881075e-11
Epoch: 6, Val loss: 1.0423524131045139e-13
Epoch: 7, Val loss: 8.287709054263051e-14
Epoch: 8, Val loss: 6.689773686961046e-14
Epoch: 9, Val loss: 5.338561915742579e-14
Epoch: 10, Val loss: 4.501779841612298e-14
Epoch: 11, Val loss: 3.49547415827602e-14
Epoch: 12, Val loss: 2.6972592399834877e-14
Epoch: 13, Val loss: 2.2176738712066448e-14
Epoch: 14, Val loss: 1.699453503559925e-14
Epoch: 15, Val loss: 1.3659939002128385e-14
Epoch: 16, Val loss: 1.0862659395485082e-14
Epoch: 17, Val loss: 8.239867750477955e-15
Epoch: 18, Val loss: 6.7611603130080445e-15
Epoch: 19, Val loss: 5.749033423019093e-15
Epoch: 20, Val loss: 4.694717228863487e-15
Epoch: 21, Val loss: 4.072128807236234e-15
Epoch: 22, Val loss: 3.138028801376143e-15
Epoch: 23, Val loss: 2.8066125350880672e-15
Early stopping at epoch 23
Best validation loss this trial: 2.8066125350880672e-15
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.022409000544477316
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.2786014572543613
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 8.308714035997076e-06
Epoch: 1, Val loss: 1.4402026391056633e-13
Epoch: 2, Val loss: 5.996075113251933e-14
Epoch: 3, Val loss: 1.9931704620467516e-14
Epoch: 4, Val loss: 8.683627459603333e-15
Epoch: 5, Val loss: 3.2670357762000747e-15
Epoch: 6, Val loss: 1.2650223838462852e-15
Epoch: 7, Val loss: 6.012484847601922e-16
Epoch: 8, Val loss: 6.188658949766928e-16
Epoch: 9, Val loss: 1.237300580502104e-15
Epoch: 10, Val loss: 5.886749108651941e-15
Epoch: 11, Val loss: 6.453519038365275e-07
Epoch: 12, Val loss: 1.2322321239350918e-08
Epoch: 13, Val loss: 1.3203896531128571e-12
Epoch: 14, Val loss: 1.493401674582094e-06
Epoch: 15, Val loss: 1.6782286636537478e-06
Epoch: 16, Val loss: 3.9600863587391565e-08
Epoch: 17, Val loss: 5.548124206020156e-10
Epoch: 18, Val loss: 1.4874780741305367e-06
Epoch: 19, Val loss: 2.0451945274505547e-06
Epoch: 20, Val loss: 5.684252503448452e-07
Epoch: 21, Val loss: 4.4504818383290677e-07
Early stopping at epoch 21
Best validation loss this trial: 4.4504818383290677e-07
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.127953400532914
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.06970480374164051
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.23875069847473732
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.12462463178950497
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.4387331088383993
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.08465646844134371
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 7.109631448173445e-16
Epoch: 1, Val loss: 6.521610095080066e-16
Epoch: 2, Val loss: 6.254353467731377e-16
Epoch: 3, Val loss: 6.524703810523935e-16
Epoch: 4, Val loss: 9.803240771117327e-16
Epoch: 5, Val loss: 2.391148244370178e-15
Epoch: 6, Val loss: 1.4455794810730763e-15
Epoch: 7, Val loss: 4.393969392228587e-14
Epoch: 8, Val loss: 1.2488016458040373e-12
Epoch: 9, Val loss: 7.310078889041996e-05
Epoch: 10, Val loss: 3.304490974491152e-08
Epoch: 11, Val loss: 5.8305884226878036e-05
Epoch: 12, Val loss: 3.6106045184006245e-05
Epoch: 13, Val loss: 1.8765930621410778e-08
Epoch: 14, Val loss: 1.1082520028823761e-05
Epoch: 15, Val loss: 1.8585185824124107e-07
Epoch: 16, Val loss: 2.60749022919452e-10
Epoch: 17, Val loss: 0.00021642383832771046
Epoch: 18, Val loss: 1.8575275790456731e-09
Epoch: 19, Val loss: 1.5002815875908096e-10
Epoch: 20, Val loss: 4.560354599927186e-05
Early stopping at epoch 20
Best validation loss this trial: 4.560354599927186e-05
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.17522334880553758
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10003003372977941
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.8922600474979845e-06
Epoch: 1, Val loss: 1.8813762873992613e-11
Epoch: 2, Val loss: 1.8069770098574144e-15
Epoch: 3, Val loss: 1.6619851533767506e-15
Epoch: 4, Val loss: 1.916822105929836e-15
Epoch: 5, Val loss: 1.73031575298266e-15
Epoch: 6, Val loss: 1.7463576252674147e-15
Epoch: 7, Val loss: 1.7587467067894783e-15
Epoch: 8, Val loss: 1.395469748180612e-15
Epoch: 9, Val loss: 1.5774461548086562e-15
Epoch: 10, Val loss: 1.5401099765797432e-15
Epoch: 11, Val loss: 1.1907924774013264e-15
Epoch: 12, Val loss: 1.5601322825590983e-15
Epoch: 13, Val loss: 1.1621101277450672e-15
Epoch: 14, Val loss: 1.39309780534772e-15
Epoch: 15, Val loss: 1.3644311910931077e-15
Epoch: 16, Val loss: 1.0736339435729375e-15
Epoch: 17, Val loss: 9.303506337208763e-16
Epoch: 18, Val loss: 9.631954356288412e-16
Epoch: 19, Val loss: 7.536307035699074e-16
Epoch: 20, Val loss: 7.564909591322398e-16
Epoch: 21, Val loss: 7.476739708381212e-16
Early stopping at epoch 21
Best validation loss this trial: 7.476739708381212e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.3038632687312201e-06
Epoch: 1, Val loss: 4.968740985785332e-12
Epoch: 2, Val loss: 1.483010603279344e-15
Epoch: 3, Val loss: 1.1907468964408523e-15
Epoch: 4, Val loss: 1.1625310184165578e-15
Epoch: 5, Val loss: 1.0974728953093257e-15
Epoch: 6, Val loss: 8.405725122632135e-16
Epoch: 7, Val loss: 8.123326596772077e-16
[I 2024-11-07 02:44:31,530] Trial 20 finished with value: 4.751050593959727e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 20 with value: 4.751050593959727e-16.
[I 2024-11-07 02:44:34,045] Trial 21 finished with value: 8.125905759156908e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 20 with value: 4.751050593959727e-16.
[I 2024-11-07 02:44:36,582] Trial 22 finished with value: 5.470280351396101e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 20 with value: 4.751050593959727e-16.
[I 2024-11-07 02:44:39,306] Trial 23 finished with value: 4.577657585114772e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 23 with value: 4.577657585114772e-16.
[I 2024-11-07 02:44:39,544] Trial 24 pruned. 
[I 2024-11-07 02:44:39,664] Trial 25 pruned. 
[I 2024-11-07 02:44:42,171] Trial 26 finished with value: 1.029402804215648e-15 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 23 with value: 4.577657585114772e-16.
[I 2024-11-07 02:44:44,791] Trial 27 finished with value: 8.638789802666142e-16 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 23 with value: 4.577657585114772e-16.
[I 2024-11-07 02:44:45,032] Trial 28 pruned. 
[I 2024-11-07 02:44:45,274] Trial 29 pruned. 
Epoch: 8, Val loss: 6.772899021995751e-16
Epoch: 9, Val loss: 6.658747515128282e-16
Epoch: 10, Val loss: 6.713535864911215e-16
Epoch: 11, Val loss: 5.991347915492634e-16
Epoch: 12, Val loss: 5.967856868422116e-16
Epoch: 13, Val loss: 5.492435098113208e-16
Epoch: 14, Val loss: 5.360610213508239e-16
Epoch: 15, Val loss: 5.310345549059234e-16
Epoch: 16, Val loss: 5.131438091029859e-16
Epoch: 17, Val loss: 5.17891908757683e-16
Epoch: 18, Val loss: 4.965722624111857e-16
Epoch: 19, Val loss: 4.674065198751882e-16
Epoch: 20, Val loss: 5.057627710592198e-16
Epoch: 21, Val loss: 4.751050593959727e-16
Early stopping at epoch 21
Best validation loss this trial: 4.751050593959727e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 5.684827054134682e-07
Epoch: 1, Val loss: 2.779241817877272e-12
Epoch: 2, Val loss: 1.165082991043781e-15
Epoch: 3, Val loss: 1.0980142446538278e-15
Epoch: 4, Val loss: 9.709842265101556e-16
Epoch: 5, Val loss: 9.49150289992517e-16
Epoch: 6, Val loss: 8.081070911183679e-16
Epoch: 7, Val loss: 8.259176246658525e-16
Epoch: 8, Val loss: 7.51961284569512e-16
Epoch: 9, Val loss: 7.535304695731637e-16
Epoch: 10, Val loss: 7.249138796726949e-16
Epoch: 11, Val loss: 7.235756364374601e-16
Epoch: 12, Val loss: 7.174120812004299e-16
Epoch: 13, Val loss: 7.097117329113727e-16
Epoch: 14, Val loss: 7.078430705859593e-16
Epoch: 15, Val loss: 6.913152466761052e-16
Epoch: 16, Val loss: 6.856394483031436e-16
Epoch: 17, Val loss: 6.764519360341604e-16
Epoch: 18, Val loss: 6.988498341466887e-16
Epoch: 19, Val loss: 6.851397023935675e-16
Epoch: 20, Val loss: 8.125905759156908e-16
Early stopping at epoch 20
Best validation loss this trial: 8.125905759156908e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 6.596531155385795e-07
Epoch: 1, Val loss: 5.347167775111387e-12
Epoch: 2, Val loss: 8.395163398226743e-16
Epoch: 3, Val loss: 7.05380082299149e-16
Epoch: 4, Val loss: 6.616780958942472e-16
Epoch: 5, Val loss: 6.155901606137399e-16
Epoch: 6, Val loss: 5.967546589665625e-16
Epoch: 7, Val loss: 5.294283492685208e-16
Epoch: 8, Val loss: 5.064735856678508e-16
Epoch: 9, Val loss: 5.096401612105534e-16
Epoch: 10, Val loss: 5.478728318316511e-16
Epoch: 11, Val loss: 5.479942976797763e-16
Epoch: 12, Val loss: 5.084519462597045e-16
Epoch: 13, Val loss: 4.903394234083741e-16
Epoch: 14, Val loss: 5.181009917821048e-16
Epoch: 15, Val loss: 5.500109137205993e-16
Epoch: 16, Val loss: 4.630406226621159e-16
Epoch: 17, Val loss: 5.377929249129471e-16
Epoch: 18, Val loss: 5.386588749293568e-16
Epoch: 19, Val loss: 5.311755894209452e-16
Epoch: 20, Val loss: 5.470280351396101e-16
Early stopping at epoch 20
Best validation loss this trial: 5.470280351396101e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.1263472591357035e-06
Epoch: 1, Val loss: 5.0958349519236725e-12
Epoch: 2, Val loss: 1.3240327152388145e-15
Epoch: 3, Val loss: 1.3113055934554133e-15
Epoch: 4, Val loss: 1.070599839792137e-15
Epoch: 5, Val loss: 1.0535004751071671e-15
Epoch: 6, Val loss: 8.129463350474935e-16
Epoch: 7, Val loss: 7.707780985522906e-16
Epoch: 8, Val loss: 7.371020732141015e-16
Epoch: 9, Val loss: 6.611608525780284e-16
Epoch: 10, Val loss: 6.584958292867782e-16
Epoch: 11, Val loss: 6.508581102609106e-16
Epoch: 12, Val loss: 5.687478657313417e-16
Epoch: 13, Val loss: 5.42808461103524e-16
Epoch: 14, Val loss: 5.261956639071925e-16
Epoch: 15, Val loss: 4.500735646791893e-16
Epoch: 16, Val loss: 4.3792186237594155e-16
Epoch: 17, Val loss: 4.452323813975448e-16
Epoch: 18, Val loss: 4.154630105372724e-16
Epoch: 19, Val loss: 4.2297573838153486e-16
Epoch: 20, Val loss: 4.885412342241723e-16
Epoch: 21, Val loss: 4.577657585114772e-16
Early stopping at epoch 21
Best validation loss this trial: 4.577657585114772e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 5.280244295136072e-07
Epoch: 1, Val loss: 7.744249165553274e-12
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 2.238728169838093e-06
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 4.7078613363282784e-07
Epoch: 1, Val loss: 2.5123577552845406e-12
Epoch: 2, Val loss: 1.387832461494478e-15
Epoch: 3, Val loss: 1.3124153583763862e-15
Epoch: 4, Val loss: 1.2353227640904584e-15
Epoch: 5, Val loss: 1.2279805130970687e-15
Epoch: 6, Val loss: 1.0906813816089368e-15
Epoch: 7, Val loss: 1.08767082534407e-15
Epoch: 8, Val loss: 1.0666697410304272e-15
Epoch: 9, Val loss: 9.979047035198266e-16
Epoch: 10, Val loss: 9.942884039952948e-16
Epoch: 11, Val loss: 1.0204755004680613e-15
Epoch: 12, Val loss: 1.0234180082235282e-15
Epoch: 13, Val loss: 1.0097579414115455e-15
Epoch: 14, Val loss: 1.0059551252320282e-15
Epoch: 15, Val loss: 1.049174243741154e-15
Epoch: 16, Val loss: 1.0705982551346649e-15
Epoch: 17, Val loss: 1.0330081717212045e-15
Epoch: 18, Val loss: 1.0593593984740212e-15
Epoch: 19, Val loss: 1.0270364218011241e-15
Epoch: 20, Val loss: 1.029402804215648e-15
Early stopping at epoch 20
Best validation loss this trial: 1.029402804215648e-15
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.2850223924942838e-06
Epoch: 1, Val loss: 4.164308698220709e-12
Epoch: 2, Val loss: 1.208452357315882e-15
Epoch: 3, Val loss: 1.1109623079229123e-15
Epoch: 4, Val loss: 1.0839801298569897e-15
Epoch: 5, Val loss: 1.0059147535241796e-15
Epoch: 6, Val loss: 9.595852965043486e-16
Epoch: 7, Val loss: 9.392109336330312e-16
Epoch: 8, Val loss: 9.29333940074287e-16
Epoch: 9, Val loss: 9.009999468348018e-16
Epoch: 10, Val loss: 8.394692694959345e-16
Epoch: 11, Val loss: 8.481437692018053e-16
Epoch: 12, Val loss: 8.5610604301862925e-16
Epoch: 13, Val loss: 8.537758024411735e-16
Epoch: 14, Val loss: 8.585057438270638e-16
Epoch: 15, Val loss: 8.63136610592557e-16
Epoch: 16, Val loss: 8.433036438290189e-16
Epoch: 17, Val loss: 8.195936413886123e-16
Epoch: 18, Val loss: 8.175971220867889e-16
Epoch: 19, Val loss: 8.767746386660432e-16
Epoch: 20, Val loss: 8.424542638946399e-16
Epoch: 21, Val loss: 8.638789802666142e-16
Early stopping at epoch 21
Best validation loss this trial: 8.638789802666142e-16
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 1.2619651026094895e-06
Epoch: 1, Val loss: 5.6661270871986394e-12
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 6.135942934785514e-07
Epoch: 1, Val loss: 5.51281179271094e-12
Optimising K=1
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.11796771671812413
Epoch: 1, Val loss: 0.0780008283080691
Epoch: 2, Val loss: 0.059619586535934674
Epoch: 3, Val loss: 0.048614135738146505
Epoch: 4, Val loss: 0.04021787075167996
Epoch: 5, Val loss: 0.03342641274428974
Epoch: 6, Val loss: 0.028003320508336618
Epoch: 7, Val loss: 0.02375510600158724
Epoch: 8, Val loss: 0.020531279129623357
Epoch: 9, Val loss: 0.018162847528139413
Epoch: 10, Val loss: 0.01647191316331342
Epoch: 11, Val loss: 0.015313528209798416
Epoch: 12, Val loss: 0.014541528187692165
Epoch: 13, Val loss: 0.01405602011640193
Epoch: 14, Val loss: 0.013755254936799154
Epoch: 15, Val loss: 0.013591721432946496
Epoch: 16, Val loss: 0.013493900913429462
Epoch: 17, Val loss: 0.013438582988613742
Epoch: 18, Val loss: 0.01342421246029563
Epoch: 19, Val loss: 0.013413280453848637
Epoch: 20, Val loss: 0.013407344582601118
Epoch: 21, Val loss: 0.01340365597694102
Epoch: 22, Val loss: 0.013413022335429313
Epoch: 23, Val loss: 0.013402920121611175
Epoch: 24, Val loss: 0.01340993766878116
Epoch: 25, Val loss: 0.013401668866054487
Epoch: 26, Val loss: 0.013406167375081676
Epoch: 27, Val loss: 0.013400669351725255
Epoch: 28, Val loss: 0.01342807219253253
Epoch: 29, Val loss: 0.013410245194652323
[I 2024-11-07 02:44:55,691] Trial 0 finished with value: 0.01340264609178244 and parameters: {'lr': 0.0001, 'batch_size': 128}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:06,777] Trial 1 finished with value: 0.013445024888039146 and parameters: {'lr': 0.001, 'batch_size': 64}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:11,099] Trial 2 finished with value: 0.013892657739126076 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:16,007] Trial 3 finished with value: 0.013461625740184622 and parameters: {'lr': 0.001, 'batch_size': 128}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:20,357] Trial 4 finished with value: 0.014123534767935842 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:21,668] Trial 5 pruned. 
[I 2024-11-07 02:45:25,108] Trial 6 pruned. 
[I 2024-11-07 02:45:25,227] Trial 7 pruned. 
[I 2024-11-07 02:45:26,282] Trial 8 pruned. 
[I 2024-11-07 02:45:26,407] Trial 9 pruned. 
[I 2024-11-07 02:45:27,088] Trial 10 pruned. 
Epoch: 30, Val loss: 0.013403982681743169
Epoch: 31, Val loss: 0.013398094528938755
Epoch: 32, Val loss: 0.01340348363490933
Epoch: 33, Val loss: 0.013400406324131003
Epoch: 34, Val loss: 0.013405244734327672
Epoch: 35, Val loss: 0.013410821762251652
Epoch: 36, Val loss: 0.013418191175718429
Epoch: 37, Val loss: 0.013410230940681393
Epoch: 38, Val loss: 0.013406294555861061
Epoch: 39, Val loss: 0.013401349201419596
Epoch: 40, Val loss: 0.01340020561622361
Epoch: 41, Val loss: 0.013402423600397877
Epoch: 42, Val loss: 0.013402078979475013
Epoch: 43, Val loss: 0.013401193212780911
Epoch: 44, Val loss: 0.01340810918264975
Epoch: 45, Val loss: 0.013398944273969884
Epoch: 46, Val loss: 0.013402392519480092
Epoch: 47, Val loss: 0.013399198919660965
Epoch: 48, Val loss: 0.013410488301414555
Epoch: 49, Val loss: 0.013403219549711479
Epoch: 50, Val loss: 0.013399147681134232
Epoch: 51, Val loss: 0.01340264609178244
Early stopping at epoch 51
Best validation loss this trial: 0.01340264609178244
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01498820185342915
Epoch: 1, Val loss: 0.01344469614709035
Epoch: 2, Val loss: 0.013462432214401217
Epoch: 3, Val loss: 0.013439555413638934
Epoch: 4, Val loss: 0.013474263155307526
Epoch: 5, Val loss: 0.013463668214778105
Epoch: 6, Val loss: 0.013457163635036375
Epoch: 7, Val loss: 0.013499735647605525
Epoch: 8, Val loss: 0.013516721594282705
Epoch: 9, Val loss: 0.013523680134079395
Epoch: 10, Val loss: 0.013434387114631314
Epoch: 11, Val loss: 0.013490713480063993
Epoch: 12, Val loss: 0.013614961861545203
Epoch: 13, Val loss: 0.013454292924740376
Epoch: 14, Val loss: 0.013595166066900278
Epoch: 15, Val loss: 0.01345174427693471
Epoch: 16, Val loss: 0.013535454924990479
Epoch: 17, Val loss: 0.013531110727061063
Epoch: 18, Val loss: 0.013488576946477605
Epoch: 19, Val loss: 0.013449497887084627
Epoch: 20, Val loss: 0.01352677721148118
Epoch: 21, Val loss: 0.013530821555381657
Epoch: 22, Val loss: 0.01352014802555498
Epoch: 23, Val loss: 0.013507249717337962
Epoch: 24, Val loss: 0.013530579411512257
Epoch: 25, Val loss: 0.013501392104304753
Epoch: 26, Val loss: 0.013677961774106719
Epoch: 27, Val loss: 0.013483134607792411
Epoch: 28, Val loss: 0.0134824868132416
Epoch: 29, Val loss: 0.013508368753151506
Epoch: 30, Val loss: 0.013445024888039146
Early stopping at epoch 30
Best validation loss this trial: 0.013445024888039146
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.013668000177180364
Epoch: 1, Val loss: 0.013644671935777543
Epoch: 2, Val loss: 0.013710468107739748
Epoch: 3, Val loss: 0.014215043793290348
Epoch: 4, Val loss: 0.013896230418803328
Epoch: 5, Val loss: 0.01403211424979618
Epoch: 6, Val loss: 0.013988882287452787
Epoch: 7, Val loss: 0.014534259735906527
Epoch: 8, Val loss: 0.01406395629490331
Epoch: 9, Val loss: 0.014291337991164903
Epoch: 10, Val loss: 0.014351497672624508
Epoch: 11, Val loss: 0.01398955762260041
Epoch: 12, Val loss: 0.014083587927585942
Epoch: 13, Val loss: 0.014097021040269884
Epoch: 14, Val loss: 0.01414501599146653
Epoch: 15, Val loss: 0.013879176892213902
Epoch: 16, Val loss: 0.014214975980378814
Epoch: 17, Val loss: 0.014399643150805417
Epoch: 18, Val loss: 0.013856313901685052
Epoch: 19, Val loss: 0.013860432289035643
Epoch: 20, Val loss: 0.015050968123694598
Epoch: 21, Val loss: 0.013892657739126076
Early stopping at epoch 21
Best validation loss this trial: 0.013892657739126076
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03464757187007848
Epoch: 1, Val loss: 0.015843368719442415
Epoch: 2, Val loss: 0.013580534401965343
Epoch: 3, Val loss: 0.013418439096944818
Epoch: 4, Val loss: 0.013407430074856444
Epoch: 5, Val loss: 0.013412185186930633
Epoch: 6, Val loss: 0.013421369921731747
Epoch: 7, Val loss: 0.013434866064433323
Epoch: 8, Val loss: 0.013438467299407822
Epoch: 9, Val loss: 0.013429111248608364
Epoch: 10, Val loss: 0.013416452727959318
Epoch: 11, Val loss: 0.013437904212308132
Epoch: 12, Val loss: 0.013478472813079923
Epoch: 13, Val loss: 0.013414412373834747
Epoch: 14, Val loss: 0.013458680108947269
Epoch: 15, Val loss: 0.01342657110574892
Epoch: 16, Val loss: 0.013477985226249291
Epoch: 17, Val loss: 0.01345185485621125
Epoch: 18, Val loss: 0.013427660326962755
Epoch: 19, Val loss: 0.013454590845158544
Epoch: 20, Val loss: 0.013449353781544555
Epoch: 21, Val loss: 0.01344851630155818
Epoch: 22, Val loss: 0.01358383619305441
Epoch: 23, Val loss: 0.013499500056318307
Epoch: 24, Val loss: 0.013461625740184622
Early stopping at epoch 24
Best validation loss this trial: 0.013461625740184622
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01380648681799234
Epoch: 1, Val loss: 0.013762334330102145
Epoch: 2, Val loss: 0.013773790765869416
Epoch: 3, Val loss: 0.0139399149565626
Epoch: 4, Val loss: 0.014522086576385013
Epoch: 5, Val loss: 0.014016138279210713
Epoch: 6, Val loss: 0.013838368783689152
Epoch: 7, Val loss: 0.013811647165882386
Epoch: 8, Val loss: 0.013842790324430344
Epoch: 9, Val loss: 0.013805310825927782
Epoch: 10, Val loss: 0.014549216602818441
Epoch: 11, Val loss: 0.013984491133083731
Epoch: 12, Val loss: 0.014011164464182773
Epoch: 13, Val loss: 0.014015559422767769
Epoch: 14, Val loss: 0.013846965269895933
Epoch: 15, Val loss: 0.01416405779704199
Epoch: 16, Val loss: 0.014370460000078557
Epoch: 17, Val loss: 0.014300234315885325
Epoch: 18, Val loss: 0.013981583970204249
Epoch: 19, Val loss: 0.014255606269432326
Epoch: 20, Val loss: 0.014145108508103984
Epoch: 21, Val loss: 0.014123534767935842
Early stopping at epoch 21
Best validation loss this trial: 0.014123534767935842
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.014574810882441254
Epoch: 1, Val loss: 0.014626616761327172
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01354768611806429
Epoch: 1, Val loss: 0.014096525939718142
Epoch: 2, Val loss: 0.014062849678477998
Epoch: 3, Val loss: 0.013681859314694242
Epoch: 4, Val loss: 0.013672897134418204
Epoch: 5, Val loss: 0.0136870163319222
Epoch: 6, Val loss: 0.013799457826604277
Epoch: 7, Val loss: 0.013806773128650956
Epoch: 8, Val loss: 0.014121182736451342
Epoch: 9, Val loss: 0.013683400985042928
Epoch: 10, Val loss: 0.014022998527576358
Epoch: 11, Val loss: 0.01392024367163747
Epoch: 12, Val loss: 0.013983597410684926
Epoch: 13, Val loss: 0.013858535137595766
Epoch: 14, Val loss: 0.01410141345744921
Epoch: 15, Val loss: 0.015126474943580263
Epoch: 16, Val loss: 0.014256474861906747
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.037700773651401204
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.013752025599854115
Epoch: 1, Val loss: 0.014267195167386124
Epoch: 2, Val loss: 0.01450527840668065
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.5111144562562306
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04543428083197174
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.013859594320384866
Epoch: 1, Val loss: 0.013438251753075
Epoch: 2, Val loss: 0.013412641528516244
Epoch: 3, Val loss: 0.013404808253941372
Epoch: 4, Val loss: 0.013440736529664097
Epoch: 5, Val loss: 0.013511242464375801
Epoch: 6, Val loss: 0.013496941719681788
Epoch: 7, Val loss: 0.013478387935230365
Epoch: 8, Val loss: 0.013505630449861543
Epoch: 9, Val loss: 0.013491271294526055
Epoch: 10, Val loss: 0.013512575736221595
Epoch: 11, Val loss: 0.013471144530126173
Epoch: 12, Val loss: 0.013457106346758002
Epoch: 13, Val loss: 0.013481396982748793
Epoch: 14, Val loss: 0.013467245814828282
[I 2024-11-07 02:45:35,840] Trial 11 finished with value: 0.013477639183720462 and parameters: {'lr': 0.001, 'batch_size': 64}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:36,197] Trial 12 pruned. 
[I 2024-11-07 02:45:36,560] Trial 13 pruned. 
[I 2024-11-07 02:45:36,767] Trial 14 pruned. 
[I 2024-11-07 02:45:37,154] Trial 15 pruned. 
[I 2024-11-07 02:45:50,991] Trial 16 finished with value: 0.013602720143703314 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:45:51,112] Trial 17 pruned. 
[I 2024-11-07 02:45:51,310] Trial 18 pruned. 
[I 2024-11-07 02:45:51,660] Trial 19 pruned. 
[I 2024-11-07 02:45:52,011] Trial 20 pruned. 
[I 2024-11-07 02:45:52,209] Trial 21 pruned. 
[I 2024-11-07 02:45:52,406] Trial 22 pruned. 
[I 2024-11-07 02:45:52,606] Trial 23 pruned. 
[I 2024-11-07 02:45:52,804] Trial 24 pruned. 
[I 2024-11-07 02:45:53,002] Trial 25 pruned. 
[I 2024-11-07 02:45:53,122] Trial 26 pruned. 
[I 2024-11-07 02:46:08,959] Trial 27 finished with value: 0.013483311837682357 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 0 with value: 0.01340264609178244.
[I 2024-11-07 02:46:09,169] Trial 28 pruned. 
[I 2024-11-07 02:46:09,377] Trial 29 pruned. 
[I 2024-11-07 02:46:49,186] Trial 0 finished with value: 0.02033778266885724 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 0 with value: 0.02033778266885724.
Epoch: 15, Val loss: 0.01347670493185775
Epoch: 16, Val loss: 0.013459556489291355
Epoch: 17, Val loss: 0.013466761933050604
Epoch: 18, Val loss: 0.013539183646058425
Epoch: 19, Val loss: 0.013426117543290313
Epoch: 20, Val loss: 0.013476193603924196
Epoch: 21, Val loss: 0.013476810800150419
Epoch: 22, Val loss: 0.013471009185872016
Epoch: 23, Val loss: 0.013477639183720462
Early stopping at epoch 23
Best validation loss this trial: 0.013477639183720462
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.15045427123450825
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.26833769602653307
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01585403111589662
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.12280210718894616
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.013426768255189188
Epoch: 1, Val loss: 0.013456662509861028
Epoch: 2, Val loss: 0.013489273852374181
Epoch: 3, Val loss: 0.01358656534471382
Epoch: 4, Val loss: 0.01346858492336021
Epoch: 5, Val loss: 0.013540142800849982
Epoch: 6, Val loss: 0.013450753411405489
Epoch: 7, Val loss: 0.013571699177766712
Epoch: 8, Val loss: 0.013530159140260428
Epoch: 9, Val loss: 0.013509854541008925
Epoch: 10, Val loss: 0.01344226603794238
Epoch: 11, Val loss: 0.013446522188078389
Epoch: 12, Val loss: 0.013489141485168256
Epoch: 13, Val loss: 0.013462164918852285
Epoch: 14, Val loss: 0.013485107879337465
Epoch: 15, Val loss: 0.013503108770610431
Epoch: 16, Val loss: 0.013685015338656103
Epoch: 17, Val loss: 0.013484780302541887
Epoch: 18, Val loss: 0.013507012791262988
Epoch: 19, Val loss: 0.013488838602350945
Epoch: 20, Val loss: 0.013602720143703314
Early stopping at epoch 20
Best validation loss this trial: 0.013602720143703314
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.26379501620928447
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04305796350462962
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.3146767024046335
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01588025848325501
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03753591164693994
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03314178209688704
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04304428780609268
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02489235990885961
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03443034469955048
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10563629853228727
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.013423702747234676
Epoch: 1, Val loss: 0.013461476048597923
Epoch: 2, Val loss: 0.013495260863525108
Epoch: 3, Val loss: 0.013420526242743317
Epoch: 4, Val loss: 0.013587687441155825
Epoch: 5, Val loss: 0.013521650230957784
Epoch: 6, Val loss: 0.01355458359218115
Epoch: 7, Val loss: 0.013532011347830806
Epoch: 8, Val loss: 0.013438305286212992
Epoch: 9, Val loss: 0.013467533585543815
Epoch: 10, Val loss: 0.013502785394716466
Epoch: 11, Val loss: 0.0135743520123104
Epoch: 12, Val loss: 0.013507273836204639
Epoch: 13, Val loss: 0.01351509423345391
Epoch: 14, Val loss: 0.013507833817782693
Epoch: 15, Val loss: 0.013655338878146349
Epoch: 16, Val loss: 0.0136035500257475
Epoch: 17, Val loss: 0.013540500233698094
Epoch: 18, Val loss: 0.013420451902107805
Epoch: 19, Val loss: 0.013514750199312838
Epoch: 20, Val loss: 0.01366668956903502
Epoch: 21, Val loss: 0.01356702492150486
Epoch: 22, Val loss: 0.013505408428927772
Epoch: 23, Val loss: 0.013483311837682357
Early stopping at epoch 23
Best validation loss this trial: 0.013483311837682357
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.022429567398661275
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10580146098035877
Optimising K=2
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020271040471350282
Epoch: 1, Val loss: 0.02029427549897287
Epoch: 2, Val loss: 0.020344745160804853
Epoch: 3, Val loss: 0.020433683493612412
Epoch: 4, Val loss: 0.02051069682989365
Epoch: 5, Val loss: 0.020483622518487465
Epoch: 6, Val loss: 0.020502107391038384
Epoch: 7, Val loss: 0.0203370650131733
Epoch: 8, Val loss: 0.020331401692337204
Epoch: 9, Val loss: 0.020434444455796074
Epoch: 10, Val loss: 0.02027780867110078
Epoch: 11, Val loss: 0.02030993693579848
Epoch: 12, Val loss: 0.020292293730104327
Epoch: 13, Val loss: 0.02030182674001807
Epoch: 14, Val loss: 0.020428493189123962
Epoch: 15, Val loss: 0.020443220977058522
Epoch: 16, Val loss: 0.02023894220797552
Epoch: 17, Val loss: 0.02020712628061127
Epoch: 18, Val loss: 0.020330299814351093
Epoch: 19, Val loss: 0.020413657274240486
Epoch: 20, Val loss: 0.02025464099720439
Epoch: 21, Val loss: 0.020243095518607233
Epoch: 22, Val loss: 0.020478485235705588
Epoch: 23, Val loss: 0.02035770231149454
Epoch: 24, Val loss: 0.020259020153560445
Epoch: 25, Val loss: 0.02039037002091352
Epoch: 26, Val loss: 0.02035635997716369
Epoch: 27, Val loss: 0.02041754253113117
Epoch: 28, Val loss: 0.020258675902508773
Epoch: 29, Val loss: 0.02040131751479756
Epoch: 30, Val loss: 0.02025196060904453
Epoch: 31, Val loss: 0.020310177033942226
Epoch: 32, Val loss: 0.020217977102018066
Epoch: 33, Val loss: 0.020229650271308217
Epoch: 34, Val loss: 0.020235376360897835
Epoch: 35, Val loss: 0.02037698396234813
Epoch: 36, Val loss: 0.020195789083949905
Epoch: 37, Val loss: 0.02038541817878429
Epoch: 38, Val loss: 0.020188770724587843
Epoch: 39, Val loss: 0.020220138671863664
Epoch: 40, Val loss: 0.020325472654026542
Epoch: 41, Val loss: 0.020361411807119336
Epoch: 42, Val loss: 0.020418525538128666
Epoch: 43, Val loss: 0.0202070360756519
Epoch: 44, Val loss: 0.020329995474849757
Epoch: 45, Val loss: 0.020375211333107743
Epoch: 46, Val loss: 0.02065691352686566
Epoch: 47, Val loss: 0.020411437062912773
Epoch: 48, Val loss: 0.020419684998913962
Epoch: 49, Val loss: 0.02024467672324843
Epoch: 50, Val loss: 0.020449957392441157
Epoch: 51, Val loss: 0.020336410504343927
Epoch: 52, Val loss: 0.020351538971926156
Epoch: 53, Val loss: 0.02025810070335865
Epoch: 54, Val loss: 0.020370844413295515
Epoch: 55, Val loss: 0.02032969365668539
Epoch: 56, Val loss: 0.0204767992751848
Epoch: 57, Val loss: 0.020361735206893366
Epoch: 58, Val loss: 0.02033778266885724
Early stopping at epoch 58
Best validation loss this trial: 0.02033778266885724
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.027254997389548917
Epoch: 1, Val loss: 0.020442825473718725
Epoch: 2, Val loss: 0.020181273283847306
Epoch: 3, Val loss: 0.02020177115702023
Epoch: 4, Val loss: 0.020228266021457768
Epoch: 5, Val loss: 0.020221837622634437
Epoch: 6, Val loss: 0.020184931739912194
Epoch: 7, Val loss: 0.020216898550674066
Epoch: 8, Val loss: 0.02021327700023934
Epoch: 9, Val loss: 0.02016939835275634
Epoch: 10, Val loss: 0.020255912253159588
Epoch: 11, Val loss: 0.02019514819070444
Epoch: 12, Val loss: 0.020225681364536285
Epoch: 13, Val loss: 0.020271003909282766
[I 2024-11-07 02:46:55,209] Trial 1 finished with value: 0.0202235319983151 and parameters: {'lr': 0.001, 'batch_size': 128}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:46:57,785] Trial 2 finished with value: 0.020629532324771088 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:47:02,396] Trial 3 finished with value: 0.020304703832430354 and parameters: {'lr': 0.001, 'batch_size': 128}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:47:31,451] Trial 4 finished with value: 0.02184083817415258 and parameters: {'lr': 0.01, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:47:31,650] Trial 5 pruned. 
[I 2024-11-07 02:47:32,309] Trial 6 pruned. 
[I 2024-11-07 02:47:56,467] Trial 7 finished with value: 0.020321269921608206 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:47:56,662] Trial 8 pruned. 
[I 2024-11-07 02:47:56,882] Trial 9 pruned. 
[I 2024-11-07 02:47:57,438] Trial 10 pruned. 
[I 2024-11-07 02:47:57,738] Trial 11 pruned. 
[I 2024-11-07 02:47:58,097] Trial 12 pruned. 
[I 2024-11-07 02:47:58,435] Trial 13 pruned. 
[I 2024-11-07 02:47:58,793] Trial 14 pruned. 
[I 2024-11-07 02:47:59,353] Trial 15 pruned. 
[I 2024-11-07 02:47:59,645] Trial 16 pruned. 
[I 2024-11-07 02:47:59,981] Trial 17 pruned. 
[I 2024-11-07 02:48:00,323] Trial 18 pruned. 
[I 2024-11-07 02:48:00,917] Trial 19 pruned. 
[I 2024-11-07 02:48:01,233] Trial 20 pruned. 
Epoch: 14, Val loss: 0.02022089645013971
Epoch: 15, Val loss: 0.020318448448837814
Epoch: 16, Val loss: 0.02025699085098202
Epoch: 17, Val loss: 0.02025212656895993
Epoch: 18, Val loss: 0.020231148827884158
Epoch: 19, Val loss: 0.02027021531583899
Epoch: 20, Val loss: 0.02025586524504726
Epoch: 21, Val loss: 0.020231510970299526
Epoch: 22, Val loss: 0.02018205057513916
Epoch: 23, Val loss: 0.02022388122849545
Epoch: 24, Val loss: 0.020208983508459594
Epoch: 25, Val loss: 0.02018481852897143
Epoch: 26, Val loss: 0.020197619983957985
Epoch: 27, Val loss: 0.020238055642378534
Epoch: 28, Val loss: 0.020290331283615806
Epoch: 29, Val loss: 0.0202235319983151
Early stopping at epoch 29
Best validation loss this trial: 0.0202235319983151
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02023404563466708
Epoch: 1, Val loss: 0.020252870706220467
Epoch: 2, Val loss: 0.020332454765836398
Epoch: 3, Val loss: 0.020435510948300362
Epoch: 4, Val loss: 0.020357158966362476
Epoch: 5, Val loss: 0.02052080656091372
Epoch: 6, Val loss: 0.02047563170393308
Epoch: 7, Val loss: 0.020401312597095968
Epoch: 8, Val loss: 0.02035840426882108
Epoch: 9, Val loss: 0.0204759044572711
Epoch: 10, Val loss: 0.020739126950502396
Epoch: 11, Val loss: 0.020480587768057983
Epoch: 12, Val loss: 0.020349804684519766
Epoch: 13, Val loss: 0.02061265961577495
Epoch: 14, Val loss: 0.020772909621397655
Epoch: 15, Val loss: 0.020749865286052226
Epoch: 16, Val loss: 0.02040442731231451
Epoch: 17, Val loss: 0.020497473577658337
Epoch: 18, Val loss: 0.02032604751487573
Epoch: 19, Val loss: 0.020699823958178362
Epoch: 20, Val loss: 0.020629532324771088
Early stopping at epoch 20
Best validation loss this trial: 0.020629532324771088
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.026218918673062728
Epoch: 1, Val loss: 0.02024314372595084
Epoch: 2, Val loss: 0.020156797716173076
Epoch: 3, Val loss: 0.020183557865478225
Epoch: 4, Val loss: 0.020235442982639296
Epoch: 5, Val loss: 0.020205848297830356
Epoch: 6, Val loss: 0.02018120591291937
Epoch: 7, Val loss: 0.020209079829312988
Epoch: 8, Val loss: 0.020213872762554784
Epoch: 9, Val loss: 0.020205080193483223
Epoch: 10, Val loss: 0.02025224296849663
Epoch: 11, Val loss: 0.020269085795192394
Epoch: 12, Val loss: 0.020290868230542894
Epoch: 13, Val loss: 0.020231223586252182
Epoch: 14, Val loss: 0.02024754907115031
Epoch: 15, Val loss: 0.020253620410369613
Epoch: 16, Val loss: 0.020252361072827192
Epoch: 17, Val loss: 0.0202725271426015
Epoch: 18, Val loss: 0.020290133401246396
Epoch: 19, Val loss: 0.020294724758398736
Epoch: 20, Val loss: 0.02023196419290567
Epoch: 21, Val loss: 0.02023185053998131
Epoch: 22, Val loss: 0.020304703832430354
Early stopping at epoch 22
Best validation loss this trial: 0.020304703832430354
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.021358448928460862
Epoch: 1, Val loss: 0.021089343370025992
Epoch: 2, Val loss: 0.021891088165247288
Epoch: 3, Val loss: 0.02287716120799892
Epoch: 4, Val loss: 0.022894805038554802
Epoch: 5, Val loss: 0.021105653948636137
Epoch: 6, Val loss: 0.022596082594404873
Epoch: 7, Val loss: 0.022419644796297464
Epoch: 8, Val loss: 0.021518952085867397
Epoch: 9, Val loss: 0.021615420607451953
Epoch: 10, Val loss: 0.021279993299872447
Epoch: 11, Val loss: 0.022347403081277244
Epoch: 12, Val loss: 0.022262759033081114
Epoch: 13, Val loss: 0.02125689909896917
Epoch: 14, Val loss: 0.022370700971183613
Epoch: 15, Val loss: 0.021787712457948007
Epoch: 16, Val loss: 0.021799723339131754
Epoch: 17, Val loss: 0.023352319517960914
Epoch: 18, Val loss: 0.020891007537452076
Epoch: 19, Val loss: 0.021947920163217772
Epoch: 20, Val loss: 0.021602384730154633
Epoch: 21, Val loss: 0.021437234302950848
Epoch: 22, Val loss: 0.021734936302129783
Epoch: 23, Val loss: 0.02261970580245058
Epoch: 24, Val loss: 0.022261453185501136
Epoch: 25, Val loss: 0.022149942182482053
Epoch: 26, Val loss: 0.023563660896168306
Epoch: 27, Val loss: 0.021680653946967717
Epoch: 28, Val loss: 0.021670822663089402
Epoch: 29, Val loss: 0.02204147748187439
Epoch: 30, Val loss: 0.022537587501076806
Epoch: 31, Val loss: 0.02504860912051657
Epoch: 32, Val loss: 0.022808575079354465
Epoch: 33, Val loss: 0.021634303928854372
Epoch: 34, Val loss: 0.022462157417152427
Epoch: 35, Val loss: 0.022219993436756808
Epoch: 36, Val loss: 0.02240734437130328
Epoch: 37, Val loss: 0.021260292802610967
Epoch: 38, Val loss: 0.02184083817415258
Early stopping at epoch 38
Best validation loss this trial: 0.02184083817415258
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.29992352028687796
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02040629085721606
Epoch: 1, Val loss: 0.020580286759946307
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02031318864856775
Epoch: 1, Val loss: 0.02027166905040797
Epoch: 2, Val loss: 0.020320100992376734
Epoch: 3, Val loss: 0.020223375119300734
Epoch: 4, Val loss: 0.020323196063852973
Epoch: 5, Val loss: 0.02034351744283086
Epoch: 6, Val loss: 0.020316252614267998
Epoch: 7, Val loss: 0.020419909137213588
Epoch: 8, Val loss: 0.020329823387134023
Epoch: 9, Val loss: 0.02027382325524321
Epoch: 10, Val loss: 0.020315776523361858
Epoch: 11, Val loss: 0.02037135619097031
Epoch: 12, Val loss: 0.020365593011658162
Epoch: 13, Val loss: 0.020489533845749166
Epoch: 14, Val loss: 0.020352872729333293
Epoch: 15, Val loss: 0.020346967849689417
Epoch: 16, Val loss: 0.02027683976130226
Epoch: 17, Val loss: 0.02046883243144068
Epoch: 18, Val loss: 0.020275127588429004
Epoch: 19, Val loss: 0.02026861839187451
Epoch: 20, Val loss: 0.020371267086483985
Epoch: 21, Val loss: 0.020324417584949832
Epoch: 22, Val loss: 0.020364203438576724
Epoch: 23, Val loss: 0.020321269921608206
Early stopping at epoch 23
Best validation loss this trial: 0.020321269921608206
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.06235423497855663
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04892034145692984
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.0860993430718907
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04468123526391336
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03592932315827426
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.024175582011624917
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.026418764238892976
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.16888226110201615
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02374538132068464
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02672661780932192
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.28549591540280034
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02099561705612219
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.022457236360947964
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020249272350412913
Epoch: 1, Val loss: 0.020366589387512613
[I 2024-11-07 02:48:21,096] Trial 21 pruned. 
[I 2024-11-07 02:48:58,805] Trial 22 finished with value: 0.020229550836703334 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:49:41,805] Trial 23 finished with value: 0.02045342096517611 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:50:01,773] Trial 24 finished with value: 0.020342203677018993 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:50:02,105] Trial 25 pruned. 
[I 2024-11-07 02:50:32,747] Trial 26 finished with value: 0.020344191839775212 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.0202235319983151.
[I 2024-11-07 02:50:33,134] Trial 27 pruned. 
[I 2024-11-07 02:50:33,359] Trial 28 pruned. 
[I 2024-11-07 02:50:34,112] Trial 29 pruned. 
Epoch: 2, Val loss: 0.02041247509754239
Epoch: 3, Val loss: 0.020373378534061022
Epoch: 4, Val loss: 0.02024523910278311
Epoch: 5, Val loss: 0.02035839280559339
Epoch: 6, Val loss: 0.02035474924283086
Epoch: 7, Val loss: 0.020323157119445313
Epoch: 8, Val loss: 0.02036708306011736
Epoch: 9, Val loss: 0.02046239307611926
Epoch: 10, Val loss: 0.02037863038145961
Epoch: 11, Val loss: 0.02033132208415713
Epoch: 12, Val loss: 0.020292110508307815
Epoch: 13, Val loss: 0.020254061680732884
Epoch: 14, Val loss: 0.02037227980625362
Epoch: 15, Val loss: 0.02037109569328017
Epoch: 16, Val loss: 0.02028785512631393
Epoch: 17, Val loss: 0.020339472549688868
Epoch: 18, Val loss: 0.02043189089665683
Epoch: 19, Val loss: 0.0202672873209748
Epoch: 20, Val loss: 0.020267211840066135
Epoch: 21, Val loss: 0.02057922728216419
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020250473247092757
Epoch: 1, Val loss: 0.020276216041838
Epoch: 2, Val loss: 0.020334700357296273
Epoch: 3, Val loss: 0.02035056409609114
Epoch: 4, Val loss: 0.020280339092446063
Epoch: 5, Val loss: 0.020537171230261397
Epoch: 6, Val loss: 0.02025536937901798
Epoch: 7, Val loss: 0.020313448865667112
Epoch: 8, Val loss: 0.02043688839747228
Epoch: 9, Val loss: 0.02028966907204853
Epoch: 10, Val loss: 0.020326329584225502
Epoch: 11, Val loss: 0.020212209811354548
Epoch: 12, Val loss: 0.020582568935222097
Epoch: 13, Val loss: 0.020266976804496385
Epoch: 14, Val loss: 0.02044691238552332
Epoch: 15, Val loss: 0.020447881468452323
Epoch: 16, Val loss: 0.020336950591836985
Epoch: 17, Val loss: 0.02024430770259828
Epoch: 18, Val loss: 0.020206287525132552
Epoch: 19, Val loss: 0.020367801030222166
Epoch: 20, Val loss: 0.020300905192350477
Epoch: 21, Val loss: 0.02032674043479129
Epoch: 22, Val loss: 0.020374088008832347
Epoch: 23, Val loss: 0.020467829034846816
Epoch: 24, Val loss: 0.02035033402757512
Epoch: 25, Val loss: 0.020373953410830252
Epoch: 26, Val loss: 0.020382883994338605
Epoch: 27, Val loss: 0.02028719890408982
Epoch: 28, Val loss: 0.020362291035329938
Epoch: 29, Val loss: 0.02026864126101773
Epoch: 30, Val loss: 0.020367405709659316
Epoch: 31, Val loss: 0.020300870777195335
Epoch: 32, Val loss: 0.02038933988660574
Epoch: 33, Val loss: 0.02031828013734303
Epoch: 34, Val loss: 0.02027252600249699
Epoch: 35, Val loss: 0.020306073696328662
Epoch: 36, Val loss: 0.020345656185323358
Epoch: 37, Val loss: 0.020421124015672084
Epoch: 38, Val loss: 0.020229550836703334
Early stopping at epoch 38
Best validation loss this trial: 0.020229550836703334
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020227436167307388
Epoch: 1, Val loss: 0.020308701787143946
Epoch: 2, Val loss: 0.020240079070258345
Epoch: 3, Val loss: 0.02026811993131653
Epoch: 4, Val loss: 0.020482447030197862
Epoch: 5, Val loss: 0.020247346844969906
Epoch: 6, Val loss: 0.020278994776069734
Epoch: 7, Val loss: 0.020368702363413878
Epoch: 8, Val loss: 0.020222355145961046
Epoch: 9, Val loss: 0.02038773776899673
Epoch: 10, Val loss: 0.020326951060953558
Epoch: 11, Val loss: 0.020336821981761444
Epoch: 12, Val loss: 0.02022295667686396
Epoch: 13, Val loss: 0.020535309488574665
Epoch: 14, Val loss: 0.020413839867195256
Epoch: 15, Val loss: 0.02040253299821773
Epoch: 16, Val loss: 0.020252555013737746
Epoch: 17, Val loss: 0.020265724605474718
Epoch: 18, Val loss: 0.020382683616704665
Epoch: 19, Val loss: 0.020300468133412238
Epoch: 20, Val loss: 0.020273135192931082
Epoch: 21, Val loss: 0.020398212266433187
Epoch: 22, Val loss: 0.02024213505240205
Epoch: 23, Val loss: 0.020345831648088418
Epoch: 24, Val loss: 0.0201993717923442
Epoch: 25, Val loss: 0.02032320376119425
Epoch: 26, Val loss: 0.02027617880883507
Epoch: 27, Val loss: 0.02033358600197567
Epoch: 28, Val loss: 0.020389308492278952
Epoch: 29, Val loss: 0.020367756350618653
Epoch: 30, Val loss: 0.020400880312578928
Epoch: 31, Val loss: 0.020333518855210043
Epoch: 32, Val loss: 0.020345878747538623
Epoch: 33, Val loss: 0.020354727595551018
Epoch: 34, Val loss: 0.020283896786471207
Epoch: 35, Val loss: 0.0202919375589197
Epoch: 36, Val loss: 0.02027350203651521
Epoch: 37, Val loss: 0.020346165742152013
Epoch: 38, Val loss: 0.020244686430495266
Epoch: 39, Val loss: 0.020274739449796002
Epoch: 40, Val loss: 0.020242886058588185
Epoch: 41, Val loss: 0.020342682084291536
Epoch: 42, Val loss: 0.02056424381832282
Epoch: 43, Val loss: 0.020440172497581087
Epoch: 44, Val loss: 0.02045342096517611
Early stopping at epoch 44
Best validation loss this trial: 0.02045342096517611
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020231415485788103
Epoch: 1, Val loss: 0.020238753738534503
Epoch: 2, Val loss: 0.020327624062904052
Epoch: 3, Val loss: 0.02052410603222302
Epoch: 4, Val loss: 0.02043738720031121
Epoch: 5, Val loss: 0.020237300467366975
Epoch: 6, Val loss: 0.020286871042723458
Epoch: 7, Val loss: 0.020438029052705593
Epoch: 8, Val loss: 0.020303939962680012
Epoch: 9, Val loss: 0.020387310411335312
Epoch: 10, Val loss: 0.020330603391680326
Epoch: 11, Val loss: 0.020435327341636784
Epoch: 12, Val loss: 0.020443195174647193
Epoch: 13, Val loss: 0.020309717649132267
Epoch: 14, Val loss: 0.020231547037091773
Epoch: 15, Val loss: 0.020338984835160594
Epoch: 16, Val loss: 0.020330299054168992
Epoch: 17, Val loss: 0.02034970006157254
Epoch: 18, Val loss: 0.020399913070405014
Epoch: 19, Val loss: 0.02023409419438332
Epoch: 20, Val loss: 0.020342203677018993
Early stopping at epoch 20
Best validation loss this trial: 0.020342203677018993
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.025297310708437937
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020251910634036373
Epoch: 1, Val loss: 0.020320654661657337
Epoch: 2, Val loss: 0.02046742057427764
Epoch: 3, Val loss: 0.02032162220034207
Epoch: 4, Val loss: 0.020336831899948865
Epoch: 5, Val loss: 0.020309709537869845
Epoch: 6, Val loss: 0.0204662189930359
Epoch: 7, Val loss: 0.020268528250595316
Epoch: 8, Val loss: 0.020290588898361366
Epoch: 9, Val loss: 0.020270377409477264
Epoch: 10, Val loss: 0.020421807431321368
Epoch: 11, Val loss: 0.020477502176967952
Epoch: 12, Val loss: 0.02020716576431042
Epoch: 13, Val loss: 0.02021710290454137
Epoch: 14, Val loss: 0.02028729954861805
Epoch: 15, Val loss: 0.02034618069505335
Epoch: 16, Val loss: 0.02038075041392038
Epoch: 17, Val loss: 0.020294818219243206
Epoch: 18, Val loss: 0.02048121784757982
Epoch: 19, Val loss: 0.020280095358562265
Epoch: 20, Val loss: 0.020333875800506808
Epoch: 21, Val loss: 0.020336785811421454
Epoch: 22, Val loss: 0.020405385141762402
Epoch: 23, Val loss: 0.02032392469043724
Epoch: 24, Val loss: 0.020286068135203842
Epoch: 25, Val loss: 0.020439094547421124
Epoch: 26, Val loss: 0.020333758348392114
Epoch: 27, Val loss: 0.020366254366106458
Epoch: 28, Val loss: 0.020448719047837786
Epoch: 29, Val loss: 0.0205562349775026
Epoch: 30, Val loss: 0.02039413968833466
Epoch: 31, Val loss: 0.02039382236603743
Epoch: 32, Val loss: 0.020344191839775212
Early stopping at epoch 32
Best validation loss this trial: 0.020344191839775212
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.022249653505591247
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.18303975159839048
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.02273403221152277
Optimising K=3
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019400218069855858
Epoch: 1, Val loss: 0.02028589062870313
Epoch: 2, Val loss: 0.02405810897421633
Epoch: 3, Val loss: 0.020569712970342137
Epoch: 4, Val loss: 0.020416893323676452
Epoch: 5, Val loss: 0.020183595653591502
Epoch: 6, Val loss: 0.019760248334839556
[I 2024-11-07 02:50:51,576] Trial 0 finished with value: 0.019573180060674492 and parameters: {'lr': 0.01, 'batch_size': 32}. Best is trial 0 with value: 0.019573180060674492.
[I 2024-11-07 02:51:05,593] Trial 1 finished with value: 0.018613792040472865 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.018613792040472865.
[I 2024-11-07 02:51:21,546] Trial 2 finished with value: 0.020072903408485856 and parameters: {'lr': 0.01, 'batch_size': 32}. Best is trial 1 with value: 0.018613792040472865.
[I 2024-11-07 02:51:25,693] Trial 3 finished with value: 0.019368914429540352 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 1 with value: 0.018613792040472865.
[I 2024-11-07 02:51:33,549] Trial 4 finished with value: 0.0195351674584514 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 1 with value: 0.018613792040472865.
[I 2024-11-07 02:51:36,043] Trial 5 finished with value: 0.01893854346126318 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 1 with value: 0.018613792040472865.
[I 2024-11-07 02:51:36,391] Trial 6 pruned. 
[I 2024-11-07 02:51:36,744] Trial 7 pruned. 
[I 2024-11-07 02:51:36,865] Trial 8 pruned. 
[I 2024-11-07 02:51:37,063] Trial 9 pruned. 
Epoch: 7, Val loss: 0.020670207273055855
Epoch: 8, Val loss: 0.02163423154041426
Epoch: 9, Val loss: 0.020274286556383993
Epoch: 10, Val loss: 0.020838479841780715
Epoch: 11, Val loss: 0.021305345830658816
Epoch: 12, Val loss: 0.021220279403795034
Epoch: 13, Val loss: 0.02009956867028123
Epoch: 14, Val loss: 0.02070447976868122
Epoch: 15, Val loss: 0.021238780823241696
Epoch: 16, Val loss: 0.019781399351090957
Epoch: 17, Val loss: 0.020347020891296048
Epoch: 18, Val loss: 0.020222679819338597
Epoch: 19, Val loss: 0.020430645750214655
Epoch: 20, Val loss: 0.019573180060674492
Early stopping at epoch 20
Best validation loss this trial: 0.019573180060674492
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01848435716337373
Epoch: 1, Val loss: 0.018501227737491965
Epoch: 2, Val loss: 0.018577721105235763
Epoch: 3, Val loss: 0.018720398034152184
Epoch: 4, Val loss: 0.018539790810348514
Epoch: 5, Val loss: 0.018673526206746314
Epoch: 6, Val loss: 0.018542948208797056
Epoch: 7, Val loss: 0.018531422614334866
Epoch: 8, Val loss: 0.01860070184398538
Epoch: 9, Val loss: 0.01854703386521174
Epoch: 10, Val loss: 0.018585997454535503
Epoch: 11, Val loss: 0.01876324456002022
Epoch: 12, Val loss: 0.018555685024087627
Epoch: 13, Val loss: 0.01861350361504552
Epoch: 14, Val loss: 0.0185214208984254
Epoch: 15, Val loss: 0.018702607934211947
Epoch: 16, Val loss: 0.018606620991968702
Epoch: 17, Val loss: 0.018641650849857774
Epoch: 18, Val loss: 0.018590885516988415
Epoch: 19, Val loss: 0.018789049120556213
Epoch: 20, Val loss: 0.018613792040472865
Early stopping at epoch 20
Best validation loss this trial: 0.018613792040472865
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019607839114868488
Epoch: 1, Val loss: 0.02151533546578935
Epoch: 2, Val loss: 0.019731510295254044
Epoch: 3, Val loss: 0.019226154260751274
Epoch: 4, Val loss: 0.021073859638701647
Epoch: 5, Val loss: 0.019970112659323674
Epoch: 6, Val loss: 0.01973104124697737
Epoch: 7, Val loss: 0.019792033986260112
Epoch: 8, Val loss: 0.0199655974124614
Epoch: 9, Val loss: 0.01968709322520428
Epoch: 10, Val loss: 0.019770489902132087
Epoch: 11, Val loss: 0.020777555875098094
Epoch: 12, Val loss: 0.019655070188017484
Epoch: 13, Val loss: 0.020563805336530648
Epoch: 14, Val loss: 0.020580291823667083
Epoch: 15, Val loss: 0.019285015735982194
Epoch: 16, Val loss: 0.02057858449446722
Epoch: 17, Val loss: 0.01960900137758153
Epoch: 18, Val loss: 0.019961028893151853
Epoch: 19, Val loss: 0.020444572381834444
Epoch: 20, Val loss: 0.02039248613306345
Epoch: 21, Val loss: 0.020799453242912762
Epoch: 22, Val loss: 0.021568517071298428
Epoch: 23, Val loss: 0.020072903408485856
Early stopping at epoch 23
Best validation loss this trial: 0.020072903408485856
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018742768188654366
Epoch: 1, Val loss: 0.018862726561472577
Epoch: 2, Val loss: 0.01895139345867654
Epoch: 3, Val loss: 0.01920859845576145
Epoch: 4, Val loss: 0.019092456121944774
Epoch: 5, Val loss: 0.019071335888515086
Epoch: 6, Val loss: 0.019074239547096066
Epoch: 7, Val loss: 0.019214446577480285
Epoch: 8, Val loss: 0.019759006482564796
Epoch: 9, Val loss: 0.01902409750273672
Epoch: 10, Val loss: 0.019282058781107603
Epoch: 11, Val loss: 0.01896263509041677
Epoch: 12, Val loss: 0.019209876088267665
Epoch: 13, Val loss: 0.018907939178584996
Epoch: 14, Val loss: 0.01935163367602785
Epoch: 15, Val loss: 0.019558399694703393
Epoch: 16, Val loss: 0.018971848727787954
Epoch: 17, Val loss: 0.019000941761216875
Epoch: 18, Val loss: 0.0193520176650609
Epoch: 19, Val loss: 0.018987437725951104
Epoch: 20, Val loss: 0.019368914429540352
Early stopping at epoch 20
Best validation loss this trial: 0.019368914429540352
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018831350067156857
Epoch: 1, Val loss: 0.018793909842053712
Epoch: 2, Val loss: 0.01956707551696543
Epoch: 3, Val loss: 0.019196419313676275
Epoch: 4, Val loss: 0.01926778501562648
Epoch: 5, Val loss: 0.01889065971945302
Epoch: 6, Val loss: 0.019032856481055082
Epoch: 7, Val loss: 0.01915031854631537
Epoch: 8, Val loss: 0.019164130991419493
Epoch: 9, Val loss: 0.019045405563409046
Epoch: 10, Val loss: 0.01876244305680364
Epoch: 11, Val loss: 0.019246834519682295
Epoch: 12, Val loss: 0.019186877093072666
Epoch: 13, Val loss: 0.01916258226511842
Epoch: 14, Val loss: 0.0199676113582011
Epoch: 15, Val loss: 0.019156882192118693
Epoch: 16, Val loss: 0.019306301581278697
Epoch: 17, Val loss: 0.019208681075123406
Epoch: 18, Val loss: 0.019285395473115526
Epoch: 19, Val loss: 0.01874446922565921
Epoch: 20, Val loss: 0.018971681500137862
Epoch: 21, Val loss: 0.019256025615890147
Epoch: 22, Val loss: 0.0195029583775391
Epoch: 23, Val loss: 0.01897427138193684
Epoch: 24, Val loss: 0.018979683471054346
Epoch: 25, Val loss: 0.01924732969915968
Epoch: 26, Val loss: 0.019257146659922803
Epoch: 27, Val loss: 0.019054761140654655
Epoch: 28, Val loss: 0.019418558547809973
Epoch: 29, Val loss: 0.01947030788129669
Epoch: 30, Val loss: 0.01900922647535296
Epoch: 31, Val loss: 0.019367203384764112
Epoch: 32, Val loss: 0.01909610689065214
Epoch: 33, Val loss: 0.01930132702434972
Epoch: 34, Val loss: 0.01920913245086953
Epoch: 35, Val loss: 0.018957634693232632
Epoch: 36, Val loss: 0.019313901205057816
Epoch: 37, Val loss: 0.019509453168612415
Epoch: 38, Val loss: 0.019207412771628064
Epoch: 39, Val loss: 0.0195351674584514
Early stopping at epoch 39
Best validation loss this trial: 0.0195351674584514
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018495755828917028
Epoch: 1, Val loss: 0.018590660269061723
Epoch: 2, Val loss: 0.018545159852753082
Epoch: 3, Val loss: 0.018753193505108356
Epoch: 4, Val loss: 0.019072348500291506
Epoch: 5, Val loss: 0.019334842326740424
Epoch: 6, Val loss: 0.01906110035876433
Epoch: 7, Val loss: 0.018748488339285056
Epoch: 8, Val loss: 0.01897205226123333
Epoch: 9, Val loss: 0.0187862745175759
Epoch: 10, Val loss: 0.018679418911536533
Epoch: 11, Val loss: 0.019060464824239412
Epoch: 12, Val loss: 0.018798475402096906
Epoch: 13, Val loss: 0.018705122731626035
Epoch: 14, Val loss: 0.019176680656770865
Epoch: 15, Val loss: 0.01881535369902849
Epoch: 16, Val loss: 0.019020847355326016
Epoch: 17, Val loss: 0.01882807103296121
Epoch: 18, Val loss: 0.01893247359742721
Epoch: 19, Val loss: 0.01892784172669053
Epoch: 20, Val loss: 0.01893854346126318
Early stopping at epoch 20
Best validation loss this trial: 0.01893854346126318
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.199804047234038
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01955475013416547
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.043785323823491734
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.025266611254063702
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018535025413824707
Epoch: 1, Val loss: 0.018555720450165562
Epoch: 2, Val loss: 0.018590718400306426
Epoch: 3, Val loss: 0.018572719463218864
Epoch: 4, Val loss: 0.018573774693486016
Epoch: 5, Val loss: 0.018608650968720514
Epoch: 6, Val loss: 0.018687378857921585
Epoch: 7, Val loss: 0.01868868427367037
Epoch: 8, Val loss: 0.018606118883730635
Epoch: 9, Val loss: 0.0185565440935425
Epoch: 10, Val loss: 0.018520244584283512
Epoch: 11, Val loss: 0.018744981803923335
Epoch: 12, Val loss: 0.01867814153098525
Epoch: 13, Val loss: 0.018605177362377826
Epoch: 14, Val loss: 0.018631942060768094
Epoch: 15, Val loss: 0.018534723663320526
Epoch: 16, Val loss: 0.018641092046362977
Epoch: 17, Val loss: 0.018549849689166006
[I 2024-11-07 02:52:09,963] Trial 10 finished with value: 0.018572903035256345 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 10 with value: 0.018572903035256345.
[I 2024-11-07 02:52:38,373] Trial 11 finished with value: 0.018590104985090658 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 10 with value: 0.018572903035256345.
[I 2024-11-07 02:53:00,246] Trial 12 finished with value: 0.01868784049143776 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 10 with value: 0.018572903035256345.
[I 2024-11-07 02:53:14,110] Trial 13 finished with value: 0.01854950371874958 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 13 with value: 0.01854950371874958.
[I 2024-11-07 02:53:14,774] Trial 14 pruned. 
[I 2024-11-07 02:53:15,448] Trial 15 pruned. 
[I 2024-11-07 02:53:16,117] Trial 16 pruned. 
[I 2024-11-07 02:53:16,489] Trial 17 pruned. 
[I 2024-11-07 02:53:16,631] Trial 18 pruned. 
[I 2024-11-07 02:53:30,749] Trial 19 finished with value: 0.01853562534874321 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 19 with value: 0.01853562534874321.
[I 2024-11-07 02:53:31,437] Trial 20 pruned. 
[I 2024-11-07 02:53:32,125] Trial 21 pruned. 
Epoch: 18, Val loss: 0.0186076168424617
Epoch: 19, Val loss: 0.01858593572854486
Epoch: 20, Val loss: 0.018596694528117266
Epoch: 21, Val loss: 0.01857803527138427
Epoch: 22, Val loss: 0.01854357671820455
Epoch: 23, Val loss: 0.018616190990114696
Epoch: 24, Val loss: 0.018871979616805274
Epoch: 25, Val loss: 0.018723398289826308
Epoch: 26, Val loss: 0.01854860652675932
Epoch: 27, Val loss: 0.018590633116630662
Epoch: 28, Val loss: 0.01859919318300473
Epoch: 29, Val loss: 0.018497776383390792
Epoch: 30, Val loss: 0.018506269040318508
Epoch: 31, Val loss: 0.018605815889473017
Epoch: 32, Val loss: 0.018620803281585246
Epoch: 33, Val loss: 0.018575728381386936
Epoch: 34, Val loss: 0.018771314866140358
Epoch: 35, Val loss: 0.018695959992483895
Epoch: 36, Val loss: 0.01860760307361364
Epoch: 37, Val loss: 0.018575535868254736
Epoch: 38, Val loss: 0.018653719106880136
Epoch: 39, Val loss: 0.01855229352338192
Epoch: 40, Val loss: 0.018699551693713054
Epoch: 41, Val loss: 0.01850038222793458
Epoch: 42, Val loss: 0.01854803632849302
Epoch: 43, Val loss: 0.018665877712142263
Epoch: 44, Val loss: 0.01855522032795299
Epoch: 45, Val loss: 0.018575624268279117
Epoch: 46, Val loss: 0.018544382077410944
Epoch: 47, Val loss: 0.018513984522487745
Epoch: 48, Val loss: 0.01861356486940486
Epoch: 49, Val loss: 0.018572903035256345
Early stopping at epoch 49
Best validation loss this trial: 0.018572903035256345
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018521025946013566
Epoch: 1, Val loss: 0.018542508435491312
Epoch: 2, Val loss: 0.01865302978290452
Epoch: 3, Val loss: 0.018613238623922963
Epoch: 4, Val loss: 0.018502983535265822
Epoch: 5, Val loss: 0.018605299845226426
Epoch: 6, Val loss: 0.01860665771752023
Epoch: 7, Val loss: 0.01869138623149986
Epoch: 8, Val loss: 0.01856626317693064
Epoch: 9, Val loss: 0.018597942259576585
Epoch: 10, Val loss: 0.018533759869826145
Epoch: 11, Val loss: 0.018507965573738527
Epoch: 12, Val loss: 0.018584790285358317
Epoch: 13, Val loss: 0.018576184056826636
Epoch: 14, Val loss: 0.018569904361636594
Epoch: 15, Val loss: 0.018563804921144858
Epoch: 16, Val loss: 0.01849022985666863
Epoch: 17, Val loss: 0.01861813835775814
Epoch: 18, Val loss: 0.01857431987539316
Epoch: 19, Val loss: 0.018654559874254413
Epoch: 20, Val loss: 0.01865831504448548
Epoch: 21, Val loss: 0.01867011999193993
Epoch: 22, Val loss: 0.01846007036212354
Epoch: 23, Val loss: 0.018610840030492116
Epoch: 24, Val loss: 0.01865951890229351
Epoch: 25, Val loss: 0.01863950125586528
Epoch: 26, Val loss: 0.018906905001395535
Epoch: 27, Val loss: 0.01866173579827206
Epoch: 28, Val loss: 0.018582568078055087
Epoch: 29, Val loss: 0.018557023450047668
Epoch: 30, Val loss: 0.01870268378725164
Epoch: 31, Val loss: 0.01852857805868117
Epoch: 32, Val loss: 0.018638412340775005
Epoch: 33, Val loss: 0.01857698851257053
Epoch: 34, Val loss: 0.018523706730184518
Epoch: 35, Val loss: 0.0187097160726722
Epoch: 36, Val loss: 0.018856730598669786
Epoch: 37, Val loss: 0.018628152121160913
Epoch: 38, Val loss: 0.01854451387945531
Epoch: 39, Val loss: 0.018679633187368896
Epoch: 40, Val loss: 0.01859250324420058
Epoch: 41, Val loss: 0.018678656405108608
Epoch: 42, Val loss: 0.018590104985090658
Early stopping at epoch 42
Best validation loss this trial: 0.018590104985090658
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018515681346448567
Epoch: 1, Val loss: 0.018586479537348207
Epoch: 2, Val loss: 0.018659184418188837
Epoch: 3, Val loss: 0.018537165284650322
Epoch: 4, Val loss: 0.018611726665503193
Epoch: 5, Val loss: 0.01858648194127485
Epoch: 6, Val loss: 0.018539492588124085
Epoch: 7, Val loss: 0.01873367718166202
Epoch: 8, Val loss: 0.018570120506084118
Epoch: 9, Val loss: 0.018693176768799752
Epoch: 10, Val loss: 0.018650522244632498
Epoch: 11, Val loss: 0.018703200842421025
Epoch: 12, Val loss: 0.018508435644878026
Epoch: 13, Val loss: 0.01863314140126364
Epoch: 14, Val loss: 0.018620726535033084
Epoch: 15, Val loss: 0.018658519318550188
Epoch: 16, Val loss: 0.01869257078625453
Epoch: 17, Val loss: 0.01865253887242741
Epoch: 18, Val loss: 0.018571146678290944
Epoch: 19, Val loss: 0.018594927218161587
Epoch: 20, Val loss: 0.018539028946692362
Epoch: 21, Val loss: 0.01863472807642996
Epoch: 22, Val loss: 0.01865367302034273
Epoch: 23, Val loss: 0.01862262709973714
Epoch: 24, Val loss: 0.018623088046701416
Epoch: 25, Val loss: 0.018986615861774
Epoch: 26, Val loss: 0.018614220894618422
Epoch: 27, Val loss: 0.01868941317885541
Epoch: 28, Val loss: 0.018584839068353176
Epoch: 29, Val loss: 0.018537680803535458
Epoch: 30, Val loss: 0.01859095345577623
Epoch: 31, Val loss: 0.018558333524399333
Epoch: 32, Val loss: 0.01868784049143776
Early stopping at epoch 32
Best validation loss this trial: 0.01868784049143776
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01845527256831017
Epoch: 1, Val loss: 0.01860855782054301
Epoch: 2, Val loss: 0.018605734963909302
Epoch: 3, Val loss: 0.018544819074659012
Epoch: 4, Val loss: 0.01861947883342385
Epoch: 5, Val loss: 0.018539723784973223
Epoch: 6, Val loss: 0.018548200199476037
Epoch: 7, Val loss: 0.01864394101783888
Epoch: 8, Val loss: 0.018509319766512156
Epoch: 9, Val loss: 0.018551666550258666
Epoch: 10, Val loss: 0.01854945898342591
Epoch: 11, Val loss: 0.018580199674997702
Epoch: 12, Val loss: 0.018605404565285925
Epoch: 13, Val loss: 0.018581782734324027
Epoch: 14, Val loss: 0.018552651941282753
Epoch: 15, Val loss: 0.0185523541787496
Epoch: 16, Val loss: 0.018607947442075636
Epoch: 17, Val loss: 0.018610926034549873
Epoch: 18, Val loss: 0.018559606558778603
Epoch: 19, Val loss: 0.01858245346368824
Epoch: 20, Val loss: 0.01854950371874958
Early stopping at epoch 20
Best validation loss this trial: 0.01854950371874958
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09348906071968058
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01855107769370079
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01855855119120107
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019401879463758733
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.2030312240123749
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018492058237903137
Epoch: 1, Val loss: 0.018625910067532815
Epoch: 2, Val loss: 0.018546790206948154
Epoch: 3, Val loss: 0.018546505090899956
Epoch: 4, Val loss: 0.01862465735906974
Epoch: 5, Val loss: 0.018623394515508644
Epoch: 6, Val loss: 0.018682607401831027
Epoch: 7, Val loss: 0.018633856278899897
Epoch: 8, Val loss: 0.018641959368370663
Epoch: 9, Val loss: 0.018526242528524663
Epoch: 10, Val loss: 0.018572327161494356
Epoch: 11, Val loss: 0.018702000293992143
Epoch: 12, Val loss: 0.018664672763811216
Epoch: 13, Val loss: 0.018747271912204277
Epoch: 14, Val loss: 0.018579420355013292
Epoch: 15, Val loss: 0.01853058799607759
Epoch: 16, Val loss: 0.018628042738518525
Epoch: 17, Val loss: 0.018566072556293674
Epoch: 18, Val loss: 0.018546795695383325
Epoch: 19, Val loss: 0.018690043785728704
Epoch: 20, Val loss: 0.01853562534874321
Early stopping at epoch 20
Best validation loss this trial: 0.01853562534874321
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018541034870247684
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018528221712376062
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01848413145296976
Epoch: 1, Val loss: 0.01861754285458189
Epoch: 2, Val loss: 0.018548533933348637
[I 2024-11-07 02:53:55,129] Trial 22 finished with value: 0.018551559444182575 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 19 with value: 0.01853562534874321.
[I 2024-11-07 02:54:09,033] Trial 23 finished with value: 0.018664582511091717 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 19 with value: 0.01853562534874321.
[I 2024-11-07 02:54:09,684] Trial 24 pruned. 
[I 2024-11-07 02:54:10,339] Trial 25 pruned. 
[I 2024-11-07 02:54:10,460] Trial 26 pruned. 
[I 2024-11-07 02:54:10,811] Trial 27 pruned. 
[I 2024-11-07 02:54:11,008] Trial 28 pruned. 
[I 2024-11-07 02:54:11,651] Trial 29 pruned. 
[I 2024-11-07 02:54:44,481] Trial 0 finished with value: 0.018119742281925984 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 0 with value: 0.018119742281925984.
[I 2024-11-07 02:55:05,854] Trial 1 finished with value: 0.01794740429116238 and parameters: {'lr': 0.0001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
Epoch: 3, Val loss: 0.018509613373913825
Epoch: 4, Val loss: 0.01851314566153874
Epoch: 5, Val loss: 0.018648170416330934
Epoch: 6, Val loss: 0.01864385708139684
Epoch: 7, Val loss: 0.0189050157319627
Epoch: 8, Val loss: 0.018608055531214446
Epoch: 9, Val loss: 0.018528386297771055
Epoch: 10, Val loss: 0.01852038998602547
Epoch: 11, Val loss: 0.01853133578044482
Epoch: 12, Val loss: 0.018544834652422074
Epoch: 13, Val loss: 0.018561006668939166
Epoch: 14, Val loss: 0.018475712394405507
Epoch: 15, Val loss: 0.01867398629999823
Epoch: 16, Val loss: 0.01868815119497669
Epoch: 17, Val loss: 0.018610934344812844
Epoch: 18, Val loss: 0.018592818019290764
Epoch: 19, Val loss: 0.018706897295549765
Epoch: 20, Val loss: 0.018572593919741802
Epoch: 21, Val loss: 0.018735722053605013
Epoch: 22, Val loss: 0.01864918498083567
Epoch: 23, Val loss: 0.01858251081365678
Epoch: 24, Val loss: 0.0186191300332196
Epoch: 25, Val loss: 0.018653811823226448
Epoch: 26, Val loss: 0.01880599360148876
Epoch: 27, Val loss: 0.018491895222622488
Epoch: 28, Val loss: 0.01862117583051515
Epoch: 29, Val loss: 0.018520329637118638
Epoch: 30, Val loss: 0.018674642241631564
Epoch: 31, Val loss: 0.018731768724596143
Epoch: 32, Val loss: 0.018543656915426254
Epoch: 33, Val loss: 0.01854187360383634
Epoch: 34, Val loss: 0.018551559444182575
Early stopping at epoch 34
Best validation loss this trial: 0.018551559444182575
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01849328725734073
Epoch: 1, Val loss: 0.01853119371395208
Epoch: 2, Val loss: 0.01863222344356597
Epoch: 3, Val loss: 0.018568261836965878
Epoch: 4, Val loss: 0.018517235449204843
Epoch: 5, Val loss: 0.018669330889088474
Epoch: 6, Val loss: 0.018581896596468795
Epoch: 7, Val loss: 0.018624582156761844
Epoch: 8, Val loss: 0.01860116205066761
Epoch: 9, Val loss: 0.018526108270813513
Epoch: 10, Val loss: 0.01856651318928179
Epoch: 11, Val loss: 0.01859592688548514
Epoch: 12, Val loss: 0.018535376219954502
Epoch: 13, Val loss: 0.01863233449582297
Epoch: 14, Val loss: 0.018656451931684956
Epoch: 15, Val loss: 0.01861979444630635
Epoch: 16, Val loss: 0.018530004259803865
Epoch: 17, Val loss: 0.018532757820466008
Epoch: 18, Val loss: 0.018599887195433307
Epoch: 19, Val loss: 0.018662330427835893
Epoch: 20, Val loss: 0.018664582511091717
Early stopping at epoch 20
Best validation loss this trial: 0.018664582511091717
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018517116339415565
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.05341029714824807
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.05263538012901942
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.023153772474163108
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03430864309607926
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.11346193907670996
Optimising K=4
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018004307085568577
Epoch: 1, Val loss: 0.018091513834989224
Epoch: 2, Val loss: 0.018000107266518295
Epoch: 3, Val loss: 0.01810508267953992
Epoch: 4, Val loss: 0.018031044624363765
Epoch: 5, Val loss: 0.018147389734816603
Epoch: 6, Val loss: 0.018181851993386563
Epoch: 7, Val loss: 0.01809865627119429
Epoch: 8, Val loss: 0.018095015842690427
Epoch: 9, Val loss: 0.018221439220584355
Epoch: 10, Val loss: 0.018121080552665595
Epoch: 11, Val loss: 0.018201913855747025
Epoch: 12, Val loss: 0.018088384977199584
Epoch: 13, Val loss: 0.01817498487046259
Epoch: 14, Val loss: 0.018068150183361057
Epoch: 15, Val loss: 0.018108917340540733
Epoch: 16, Val loss: 0.018040082457228605
Epoch: 17, Val loss: 0.01797382417333941
Epoch: 18, Val loss: 0.01804749931121229
Epoch: 19, Val loss: 0.018060951828001402
Epoch: 20, Val loss: 0.018061389796372153
Epoch: 21, Val loss: 0.018057148931468398
Epoch: 22, Val loss: 0.018041127293697026
Epoch: 23, Val loss: 0.01813447191658565
Epoch: 24, Val loss: 0.018091286910681896
Epoch: 25, Val loss: 0.018055505485425137
Epoch: 26, Val loss: 0.018125879785252944
Epoch: 27, Val loss: 0.018112835951913625
Epoch: 28, Val loss: 0.017946336036309216
Epoch: 29, Val loss: 0.01823853359868129
Epoch: 30, Val loss: 0.018109041329823498
Epoch: 31, Val loss: 0.018028306094213173
Epoch: 32, Val loss: 0.01809709694666358
Epoch: 33, Val loss: 0.01819995425354976
Epoch: 34, Val loss: 0.01812365583072488
Epoch: 35, Val loss: 0.01807402239109461
Epoch: 36, Val loss: 0.018124228498588007
Epoch: 37, Val loss: 0.01807744990890989
Epoch: 38, Val loss: 0.018032305993330784
Epoch: 39, Val loss: 0.018085201040037677
Epoch: 40, Val loss: 0.018018447380099032
Epoch: 41, Val loss: 0.018014192402076263
Epoch: 42, Val loss: 0.018134622583882168
Epoch: 43, Val loss: 0.01808605458921729
Epoch: 44, Val loss: 0.018096541468467977
Epoch: 45, Val loss: 0.018053179399834737
Epoch: 46, Val loss: 0.01808476602483509
Epoch: 47, Val loss: 0.01832626250962544
Epoch: 48, Val loss: 0.018119742281925984
Early stopping at epoch 48
Best validation loss this trial: 0.018119742281925984
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03782653151493934
Epoch: 1, Val loss: 0.02370029219434175
Epoch: 2, Val loss: 0.0191442093007967
Epoch: 3, Val loss: 0.018124776363022562
Epoch: 4, Val loss: 0.017957335974798243
Epoch: 5, Val loss: 0.017950385072642665
Epoch: 6, Val loss: 0.017953704143118147
Epoch: 7, Val loss: 0.01793583386585626
Epoch: 8, Val loss: 0.017956403995521814
Epoch: 9, Val loss: 0.01795472737210683
Epoch: 10, Val loss: 0.01794221725824297
Epoch: 11, Val loss: 0.017931104462561954
Epoch: 12, Val loss: 0.017939339690388013
Epoch: 13, Val loss: 0.01794665356556702
Epoch: 14, Val loss: 0.017938109039145913
Epoch: 15, Val loss: 0.01793395658620657
Epoch: 16, Val loss: 0.017932627947093584
Epoch: 17, Val loss: 0.01794037958766278
Epoch: 18, Val loss: 0.01794736170902466
Epoch: 19, Val loss: 0.017946576902595084
Epoch: 20, Val loss: 0.01794660585319512
Epoch: 21, Val loss: 0.017941962597238965
Epoch: 22, Val loss: 0.017934323949182134
Epoch: 23, Val loss: 0.017933301711216185
Epoch: 24, Val loss: 0.017944262478436925
Epoch: 25, Val loss: 0.01794360541046048
Epoch: 26, Val loss: 0.017942063383057587
Epoch: 27, Val loss: 0.017941010487066884
Epoch: 28, Val loss: 0.017952267450686447
Epoch: 29, Val loss: 0.01796831916142096
Epoch: 30, Val loss: 0.017990364601564966
Epoch: 31, Val loss: 0.01794740429116238
Early stopping at epoch 31
Best validation loss this trial: 0.01794740429116238
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10125468130040373
Epoch: 1, Val loss: 0.0566740956189286
Epoch: 2, Val loss: 0.04072100602281399
Epoch: 3, Val loss: 0.030822585408504192
Epoch: 4, Val loss: 0.024565156644735582
Epoch: 5, Val loss: 0.020962360315024853
Epoch: 6, Val loss: 0.01915853316935464
Epoch: 7, Val loss: 0.018364439949265912
Epoch: 8, Val loss: 0.01806493039824005
Epoch: 9, Val loss: 0.01797445182903455
Epoch: 10, Val loss: 0.017934408094574753
Epoch: 11, Val loss: 0.0179301541712549
Epoch: 12, Val loss: 0.017931895282788154
Epoch: 13, Val loss: 0.017927074128300205
Epoch: 14, Val loss: 0.017929682165830053
Epoch: 15, Val loss: 0.017930926189909123
Epoch: 16, Val loss: 0.017931691713185392
Epoch: 17, Val loss: 0.01792501488652749
Epoch: 18, Val loss: 0.01792225243253076
Epoch: 19, Val loss: 0.01792332127244554
Epoch: 20, Val loss: 0.01792267859618888
Epoch: 21, Val loss: 0.01793150671431397
Epoch: 22, Val loss: 0.01792514541496833
Epoch: 23, Val loss: 0.017950504118751768
Epoch: 24, Val loss: 0.017924604213072196
Epoch: 25, Val loss: 0.017920774742005728
[I 2024-11-07 02:55:22,827] Trial 2 finished with value: 0.017947449850348327 and parameters: {'lr': 0.0001, 'batch_size': 64}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:55:37,119] Trial 3 finished with value: 0.01810651720286562 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:55:41,277] Trial 4 finished with value: 0.018706593615144998 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:55:41,476] Trial 5 pruned. 
[I 2024-11-07 02:55:42,136] Trial 6 pruned. 
[I 2024-11-07 02:55:42,335] Trial 7 pruned. 
[I 2024-11-07 02:55:42,706] Trial 8 pruned. 
[I 2024-11-07 02:55:42,906] Trial 9 pruned. 
[I 2024-11-07 02:55:43,028] Trial 10 pruned. 
[I 2024-11-07 02:55:43,378] Trial 11 pruned. 
[I 2024-11-07 02:55:43,737] Trial 12 pruned. 
[I 2024-11-07 02:55:43,862] Trial 13 pruned. 
[I 2024-11-07 02:55:44,224] Trial 14 pruned. 
[I 2024-11-07 02:56:06,614] Trial 15 finished with value: 0.018028977832510177 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:56:07,265] Trial 16 pruned. 
[I 2024-11-07 02:56:07,611] Trial 17 pruned. 
[I 2024-11-07 02:56:07,732] Trial 18 pruned. 
[I 2024-11-07 02:56:22,288] Trial 19 finished with value: 0.01821932449745826 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:56:22,645] Trial 20 pruned. 
[I 2024-11-07 02:56:23,315] Trial 21 pruned. 
Epoch: 26, Val loss: 0.017918129443612874
Epoch: 27, Val loss: 0.01792127325430385
Epoch: 28, Val loss: 0.017933172372798633
Epoch: 29, Val loss: 0.017949904044532877
Epoch: 30, Val loss: 0.017934438979460135
Epoch: 31, Val loss: 0.017928472170845058
Epoch: 32, Val loss: 0.01792420684081367
Epoch: 33, Val loss: 0.01793144043121073
Epoch: 34, Val loss: 0.017919946135555066
Epoch: 35, Val loss: 0.0179242473652857
Epoch: 36, Val loss: 0.017920569707758915
Epoch: 37, Val loss: 0.017919366996194053
Epoch: 38, Val loss: 0.017922446044145994
Epoch: 39, Val loss: 0.01792180024756071
Epoch: 40, Val loss: 0.017925967804641806
Epoch: 41, Val loss: 0.017936638731541287
Epoch: 42, Val loss: 0.017935345156325236
Epoch: 43, Val loss: 0.017932129164154712
Epoch: 44, Val loss: 0.01792409149213479
Epoch: 45, Val loss: 0.01792824397293421
Epoch: 46, Val loss: 0.017947449850348327
Early stopping at epoch 46
Best validation loss this trial: 0.017947449850348327
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.017956445220475778
Epoch: 1, Val loss: 0.018005786885499444
Epoch: 2, Val loss: 0.018067255437087554
Epoch: 3, Val loss: 0.018098004516532533
Epoch: 4, Val loss: 0.01811926842579602
Epoch: 5, Val loss: 0.018006433740767658
Epoch: 6, Val loss: 0.018145176462638073
Epoch: 7, Val loss: 0.018186531773903686
Epoch: 8, Val loss: 0.018081175836010113
Epoch: 9, Val loss: 0.018143939570738718
Epoch: 10, Val loss: 0.018093974265851017
Epoch: 11, Val loss: 0.018087786100964006
Epoch: 12, Val loss: 0.018004535156119075
Epoch: 13, Val loss: 0.018162846772207156
Epoch: 14, Val loss: 0.01805634255337919
Epoch: 15, Val loss: 0.018061339230332363
Epoch: 16, Val loss: 0.018083508416978467
Epoch: 17, Val loss: 0.017982260081280246
Epoch: 18, Val loss: 0.01811561573885827
Epoch: 19, Val loss: 0.018084255055102527
Epoch: 20, Val loss: 0.01810651720286562
Early stopping at epoch 20
Best validation loss this trial: 0.01810651720286562
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01813902806932643
Epoch: 1, Val loss: 0.018672679614086272
Epoch: 2, Val loss: 0.018812269493306086
Epoch: 3, Val loss: 0.018375630285275186
Epoch: 4, Val loss: 0.01844777162107876
Epoch: 5, Val loss: 0.01864689880571628
Epoch: 6, Val loss: 0.018517735493132622
Epoch: 7, Val loss: 0.01885853723575503
Epoch: 8, Val loss: 0.01913680300369101
Epoch: 9, Val loss: 0.018479501149790772
Epoch: 10, Val loss: 0.01944231136181092
Epoch: 11, Val loss: 0.01912555474219686
Epoch: 12, Val loss: 0.01899547110927307
Epoch: 13, Val loss: 0.018689552621947506
Epoch: 14, Val loss: 0.019214875980327697
Epoch: 15, Val loss: 0.018636640997887668
Epoch: 16, Val loss: 0.019166666840724017
Epoch: 17, Val loss: 0.019069629816054288
Epoch: 18, Val loss: 0.0189015929413549
Epoch: 19, Val loss: 0.018526287559988135
Epoch: 20, Val loss: 0.018706593615144998
Early stopping at epoch 20
Best validation loss this trial: 0.018706593615144998
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.15641115833136995
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.06564466367101568
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01841275952756405
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.11967080143781808
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018181296728425108
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.24520727495352426
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.03856047787345373
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.07031860240758994
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.409237007300059
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09612435006942505
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01806766548170111
Epoch: 1, Val loss: 0.017998422551740948
Epoch: 2, Val loss: 0.0180563420518978
Epoch: 3, Val loss: 0.01799567406758284
Epoch: 4, Val loss: 0.018135052162389725
Epoch: 5, Val loss: 0.018116437694710545
Epoch: 6, Val loss: 0.018052667932600804
Epoch: 7, Val loss: 0.018138294128112048
Epoch: 8, Val loss: 0.018111716812619798
Epoch: 9, Val loss: 0.018088693670832958
Epoch: 10, Val loss: 0.018102805747085404
Epoch: 11, Val loss: 0.01808624877793412
Epoch: 12, Val loss: 0.017988543623150922
Epoch: 13, Val loss: 0.01792616380426364
Epoch: 14, Val loss: 0.018085329160571862
Epoch: 15, Val loss: 0.018145576441803805
Epoch: 16, Val loss: 0.018100695616892006
Epoch: 17, Val loss: 0.018055814075578228
Epoch: 18, Val loss: 0.01813693995324847
Epoch: 19, Val loss: 0.018173663697850246
Epoch: 20, Val loss: 0.01809579797455261
Epoch: 21, Val loss: 0.018087516733819347
Epoch: 22, Val loss: 0.018077828459696382
Epoch: 23, Val loss: 0.018207852378423907
Epoch: 24, Val loss: 0.018032931332659517
Epoch: 25, Val loss: 0.017979107088703886
Epoch: 26, Val loss: 0.01810389546215789
Epoch: 27, Val loss: 0.01814909151588113
Epoch: 28, Val loss: 0.018230568279281385
Epoch: 29, Val loss: 0.01808532065528835
Epoch: 30, Val loss: 0.01807824533798883
Epoch: 31, Val loss: 0.018087323134144146
Epoch: 32, Val loss: 0.018180459514897093
Epoch: 33, Val loss: 0.018028977832510177
Early stopping at epoch 33
Best validation loss this trial: 0.018028977832510177
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.04917235266512785
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09269706178934146
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01818663692101836
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.017972004217788193
Epoch: 1, Val loss: 0.017958438716446742
Epoch: 2, Val loss: 0.018065096794540048
Epoch: 3, Val loss: 0.017959069460630417
Epoch: 4, Val loss: 0.018101422056460228
Epoch: 5, Val loss: 0.018058064457379345
Epoch: 6, Val loss: 0.018073210592230417
Epoch: 7, Val loss: 0.018151561359468944
Epoch: 8, Val loss: 0.01810312492804777
Epoch: 9, Val loss: 0.018173504648650557
Epoch: 10, Val loss: 0.018054475597878043
Epoch: 11, Val loss: 0.018121453423976388
Epoch: 12, Val loss: 0.01802791706405771
Epoch: 13, Val loss: 0.018157039900334217
Epoch: 14, Val loss: 0.018052144987015132
Epoch: 15, Val loss: 0.018119887715508032
Epoch: 16, Val loss: 0.01816141627665259
Epoch: 17, Val loss: 0.018157158966343373
Epoch: 18, Val loss: 0.018030319307158645
Epoch: 19, Val loss: 0.018099572509527206
Epoch: 20, Val loss: 0.01809734416504701
Epoch: 21, Val loss: 0.01821932449745826
Early stopping at epoch 21
Best validation loss this trial: 0.01821932449745826
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09722553900419137
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01815036413235924
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.017958361237572554
Epoch: 1, Val loss: 0.01797488122844161
Epoch: 2, Val loss: 0.018060635564387098
Epoch: 3, Val loss: 0.018201379133308798
Epoch: 4, Val loss: 0.01802307676173683
Epoch: 5, Val loss: 0.01800146848600135
Epoch: 6, Val loss: 0.018006795026266422
[I 2024-11-07 02:56:37,363] Trial 22 finished with value: 0.018057930649409436 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:56:51,329] Trial 23 finished with value: 0.01806478885710876 and parameters: {'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 0.01794740429116238.
[I 2024-11-07 02:57:10,292] Trial 24 pruned. 
[I 2024-11-07 02:57:10,948] Trial 25 pruned. 
[I 2024-11-07 02:57:11,299] Trial 26 pruned. 
[I 2024-11-07 02:57:11,420] Trial 27 pruned. 
[I 2024-11-07 02:57:12,073] Trial 28 pruned. 
[I 2024-11-07 02:57:12,731] Trial 29 pruned. 
[I 2024-11-07 02:57:23,935] Trial 0 finished with value: 0.019296632522446476 and parameters: {'lr': 0.0001, 'batch_size': 64}. Best is trial 0 with value: 0.019296632522446476.
[I 2024-11-07 02:57:52,563] Trial 1 finished with value: 0.019303166140348483 and parameters: {'lr': 0.0001, 'batch_size': 32}. Best is trial 0 with value: 0.019296632522446476.
Epoch: 7, Val loss: 0.018279941897823386
Epoch: 8, Val loss: 0.018239385589686595
Epoch: 9, Val loss: 0.018032766659704283
Epoch: 10, Val loss: 0.018146610377022088
Epoch: 11, Val loss: 0.018231613235150136
Epoch: 12, Val loss: 0.01802823744300339
Epoch: 13, Val loss: 0.018096643205509227
Epoch: 14, Val loss: 0.018032759949405733
Epoch: 15, Val loss: 0.01818535549359189
Epoch: 16, Val loss: 0.018103703474387143
Epoch: 17, Val loss: 0.018179172630079538
Epoch: 18, Val loss: 0.018031519188935686
Epoch: 19, Val loss: 0.018003942110599615
Epoch: 20, Val loss: 0.018057930649409436
Early stopping at epoch 20
Best validation loss this trial: 0.018057930649409436
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01794386487931777
Epoch: 1, Val loss: 0.017979430404897682
Epoch: 2, Val loss: 0.018058707850037985
Epoch: 3, Val loss: 0.018031562145194437
Epoch: 4, Val loss: 0.018109718206314705
Epoch: 5, Val loss: 0.018098451790169008
Epoch: 6, Val loss: 0.018082162492677696
Epoch: 7, Val loss: 0.018215587609407738
Epoch: 8, Val loss: 0.018034150167248953
Epoch: 9, Val loss: 0.018141322180183016
Epoch: 10, Val loss: 0.01806776065570422
Epoch: 11, Val loss: 0.017995811720243376
Epoch: 12, Val loss: 0.018063954173181303
Epoch: 13, Val loss: 0.018056742707665406
Epoch: 14, Val loss: 0.01806057486125929
Epoch: 15, Val loss: 0.01818491614017731
Epoch: 16, Val loss: 0.018117225004567042
Epoch: 17, Val loss: 0.018112060243789203
Epoch: 18, Val loss: 0.01821899902011849
Epoch: 19, Val loss: 0.018023490603280883
Epoch: 20, Val loss: 0.01806478885710876
Early stopping at epoch 20
Best validation loss this trial: 0.01806478885710876
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018000475333955806
Epoch: 1, Val loss: 0.018011406060053498
Epoch: 2, Val loss: 0.01801235805878527
Epoch: 3, Val loss: 0.017969403069657393
Epoch: 4, Val loss: 0.018293041258286208
Epoch: 5, Val loss: 0.01808515491967018
Epoch: 6, Val loss: 0.018013778297851484
Epoch: 7, Val loss: 0.018021002419802368
Epoch: 8, Val loss: 0.018090254296827264
Epoch: 9, Val loss: 0.018288122306967903
Epoch: 10, Val loss: 0.018098338757856533
Epoch: 11, Val loss: 0.01812551687590969
Epoch: 12, Val loss: 0.018130044639906567
Epoch: 13, Val loss: 0.018110426620413095
Epoch: 14, Val loss: 0.01806725732759278
Epoch: 15, Val loss: 0.017997611333958358
Epoch: 16, Val loss: 0.01817031169676373
Epoch: 17, Val loss: 0.018052100629792508
Epoch: 18, Val loss: 0.01810549231023233
Epoch: 19, Val loss: 0.018111457148741964
Epoch: 20, Val loss: 0.0179599205024031
Epoch: 21, Val loss: 0.018144096785153333
Epoch: 22, Val loss: 0.018022362011460923
Epoch: 23, Val loss: 0.018181977160752583
Epoch: 24, Val loss: 0.018177361394732427
Epoch: 25, Val loss: 0.018165776792627115
Epoch: 26, Val loss: 0.018037196443790298
Epoch: 27, Val loss: 0.01813700668609295
Epoch: 28, Val loss: 0.018032122602383804
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01813003392969696
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.07274741978726836
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.2316092019279798
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.021119374232605483
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.018018102201704796
Optimising K=5
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10245623152989608
Epoch: 1, Val loss: 0.06515909533979547
Epoch: 2, Val loss: 0.048072127338785395
Epoch: 3, Val loss: 0.03614825460836928
Epoch: 4, Val loss: 0.028249948245719966
Epoch: 5, Val loss: 0.023534924747088015
Epoch: 6, Val loss: 0.02102256815434776
Epoch: 7, Val loss: 0.019881092561170075
Epoch: 8, Val loss: 0.019448222616353095
Epoch: 9, Val loss: 0.019325013471464824
Epoch: 10, Val loss: 0.019288509383670285
Epoch: 11, Val loss: 0.01928634682081194
Epoch: 12, Val loss: 0.019294801223863903
Epoch: 13, Val loss: 0.01929334457963705
Epoch: 14, Val loss: 0.01929814977428088
Epoch: 15, Val loss: 0.019290971044355478
Epoch: 16, Val loss: 0.019297816058318328
Epoch: 17, Val loss: 0.019290421930197466
Epoch: 18, Val loss: 0.0192973058607079
Epoch: 19, Val loss: 0.019295188343614086
Epoch: 20, Val loss: 0.01929532451571053
Epoch: 21, Val loss: 0.01929472257088647
Epoch: 22, Val loss: 0.019297281932881754
Epoch: 23, Val loss: 0.019298491927866753
Epoch: 24, Val loss: 0.01929056603047583
Epoch: 25, Val loss: 0.019288421497067325
Epoch: 26, Val loss: 0.019306324215398893
Epoch: 27, Val loss: 0.019291680194755904
Epoch: 28, Val loss: 0.01929074164448131
Epoch: 29, Val loss: 0.019291898442639246
Epoch: 30, Val loss: 0.019294830715745434
Epoch: 31, Val loss: 0.019296632522446476
Early stopping at epoch 31
Best validation loss this trial: 0.019296632522446476
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.057145325371470206
Epoch: 1, Val loss: 0.033576621960562006
Epoch: 2, Val loss: 0.023794099795990266
Epoch: 3, Val loss: 0.020207054634443205
Epoch: 4, Val loss: 0.01941732040598479
Epoch: 5, Val loss: 0.019321684980303303
Epoch: 6, Val loss: 0.019298187906766295
Epoch: 7, Val loss: 0.019289920412991036
Epoch: 8, Val loss: 0.01928809179693588
Epoch: 9, Val loss: 0.01929267383236279
Epoch: 10, Val loss: 0.019298510554318245
Epoch: 11, Val loss: 0.019300605309729137
Epoch: 12, Val loss: 0.01928339984554511
Epoch: 13, Val loss: 0.019299865282403354
Epoch: 14, Val loss: 0.019294655233080316
Epoch: 15, Val loss: 0.019292249002008356
Epoch: 16, Val loss: 0.019301508839886922
Epoch: 17, Val loss: 0.019299226430937264
Epoch: 18, Val loss: 0.019284688593007814
Epoch: 19, Val loss: 0.019287831090295162
Epoch: 20, Val loss: 0.019283643171477776
Epoch: 21, Val loss: 0.019285643934948832
Epoch: 22, Val loss: 0.019278393616565526
Epoch: 23, Val loss: 0.019294933861710578
Epoch: 24, Val loss: 0.019313656299128238
Epoch: 25, Val loss: 0.01930239322221177
Epoch: 26, Val loss: 0.019290572052232474
Epoch: 27, Val loss: 0.019281405044130534
Epoch: 28, Val loss: 0.01930061016136255
Epoch: 29, Val loss: 0.019297992886227164
Epoch: 30, Val loss: 0.01929100877087977
Epoch: 31, Val loss: 0.019304820618982244
Epoch: 32, Val loss: 0.01928869216965559
Epoch: 33, Val loss: 0.019296296322957065
Epoch: 34, Val loss: 0.019291088960141454
Epoch: 35, Val loss: 0.019283657292556815
Epoch: 36, Val loss: 0.019293312679848865
Epoch: 37, Val loss: 0.019297315870435573
Epoch: 38, Val loss: 0.01929516316208447
Epoch: 39, Val loss: 0.01928488490705052
Epoch: 40, Val loss: 0.01930436898723372
Epoch: 41, Val loss: 0.019296477532858014
Epoch: 42, Val loss: 0.019303166140348483
Early stopping at epoch 42
Best validation loss this trial: 0.019303166140348483
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019286360933134954
Epoch: 1, Val loss: 0.019435415603220464
Epoch: 2, Val loss: 0.019399405177682637
Epoch: 3, Val loss: 0.019410513093074164
Epoch: 4, Val loss: 0.019533893465995787
Epoch: 5, Val loss: 0.019768474002679188
Epoch: 6, Val loss: 0.01947531644254923
Epoch: 7, Val loss: 0.01977405312160651
Epoch: 8, Val loss: 0.019653130571047464
Epoch: 9, Val loss: 0.019637129766245685
Epoch: 10, Val loss: 0.019760860999425253
Epoch: 11, Val loss: 0.019639230612665416
Epoch: 12, Val loss: 0.019662740640342236
Epoch: 13, Val loss: 0.019972217145065467
Epoch: 14, Val loss: 0.01988668031990528
Epoch: 15, Val loss: 0.019938686521102984
Epoch: 16, Val loss: 0.019907770430048308
Epoch: 17, Val loss: 0.019723374272386233
Epoch: 18, Val loss: 0.019665984126428764
Epoch: 19, Val loss: 0.019606952928006648
Epoch: 20, Val loss: 0.019686206554373104
Early stopping at epoch 20
[I 2024-11-07 02:57:55,041] Trial 2 finished with value: 0.019686206554373104 and parameters: {'lr': 0.01, 'batch_size': 256}. Best is trial 0 with value: 0.019296632522446476.
[I 2024-11-07 02:58:06,009] Trial 3 finished with value: 0.019325863347094283 and parameters: {'lr': 0.001, 'batch_size': 64}. Best is trial 0 with value: 0.019296632522446476.
[I 2024-11-07 02:58:13,806] Trial 4 finished with value: 0.020567012098381073 and parameters: {'lr': 0.01, 'batch_size': 128}. Best is trial 0 with value: 0.019296632522446476.
[I 2024-11-07 02:58:13,927] Trial 5 pruned. 
[I 2024-11-07 02:58:14,582] Trial 6 pruned. 
[I 2024-11-07 02:58:14,702] Trial 7 pruned. 
[I 2024-11-07 02:58:14,901] Trial 8 pruned. 
[I 2024-11-07 02:58:15,254] Trial 9 pruned. 
[I 2024-11-07 02:58:15,613] Trial 10 pruned. 
[I 2024-11-07 02:58:16,288] Trial 11 pruned. 
[I 2024-11-07 02:58:16,957] Trial 12 pruned. 
[I 2024-11-07 02:58:17,619] Trial 13 pruned. 
[I 2024-11-07 02:58:17,978] Trial 14 pruned. 
[I 2024-11-07 02:58:18,637] Trial 15 pruned. 
[I 2024-11-07 02:58:20,404] Trial 16 pruned. 
[I 2024-11-07 02:58:20,604] Trial 17 pruned. 
[I 2024-11-07 02:58:21,260] Trial 18 pruned. 
[I 2024-11-07 02:58:23,026] Trial 19 pruned. 
[I 2024-11-07 02:58:23,380] Trial 20 pruned. 
[I 2024-11-07 02:58:27,242] Trial 21 pruned. 
[I 2024-11-07 02:58:31,145] Trial 22 pruned. 
[I 2024-11-07 02:58:38,831] Trial 23 finished with value: 0.019421377394380223 and parameters: {'lr': 0.001, 'batch_size': 64}. Best is trial 0 with value: 0.019296632522446476.
[I 2024-11-07 02:58:39,182] Trial 24 pruned. 
Best validation loss this trial: 0.019686206554373104
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019598029967811372
Epoch: 1, Val loss: 0.01931530026257293
Epoch: 2, Val loss: 0.0193427019378441
Epoch: 3, Val loss: 0.019359917658516485
Epoch: 4, Val loss: 0.019354524648087658
Epoch: 5, Val loss: 0.019419921968036737
Epoch: 6, Val loss: 0.019446634544202916
Epoch: 7, Val loss: 0.01936771271702571
Epoch: 8, Val loss: 0.01936438526862707
Epoch: 9, Val loss: 0.019334841057912916
Epoch: 10, Val loss: 0.019276621194285713
Epoch: 11, Val loss: 0.019383212463914327
Epoch: 12, Val loss: 0.01946622479515962
Epoch: 13, Val loss: 0.019403283691240683
Epoch: 14, Val loss: 0.019298841468353048
Epoch: 15, Val loss: 0.019413763649252236
Epoch: 16, Val loss: 0.019422559147207145
Epoch: 17, Val loss: 0.019445714190538622
Epoch: 18, Val loss: 0.019430525346189484
Epoch: 19, Val loss: 0.01945668102329613
Epoch: 20, Val loss: 0.019376573256320424
Epoch: 21, Val loss: 0.01935035342939644
Epoch: 22, Val loss: 0.01942082454697189
Epoch: 23, Val loss: 0.019394658458156463
Epoch: 24, Val loss: 0.01947602494341186
Epoch: 25, Val loss: 0.01942702389170981
Epoch: 26, Val loss: 0.019422129433379214
Epoch: 27, Val loss: 0.019392598253221083
Epoch: 28, Val loss: 0.019388505568106968
Epoch: 29, Val loss: 0.019395587758885488
Epoch: 30, Val loss: 0.019325863347094283
Early stopping at epoch 30
Best validation loss this trial: 0.019325863347094283
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020177993844500034
Epoch: 1, Val loss: 0.020506728377382634
Epoch: 2, Val loss: 0.020423632146695913
Epoch: 3, Val loss: 0.02118520574423216
Epoch: 4, Val loss: 0.020074060865504256
Epoch: 5, Val loss: 0.020114311964203744
Epoch: 6, Val loss: 0.01972875406302638
Epoch: 7, Val loss: 0.020532201454674794
Epoch: 8, Val loss: 0.020130532919981722
Epoch: 9, Val loss: 0.020175351524504566
Epoch: 10, Val loss: 0.01984767865200164
Epoch: 11, Val loss: 0.01985567224101495
Epoch: 12, Val loss: 0.019688791437548096
Epoch: 13, Val loss: 0.020220395809007904
Epoch: 14, Val loss: 0.019962838535212864
Epoch: 15, Val loss: 0.020261559288127947
Epoch: 16, Val loss: 0.02003869000759165
Epoch: 17, Val loss: 0.020097614707961932
Epoch: 18, Val loss: 0.01957786945089445
Epoch: 19, Val loss: 0.020003726421776464
Epoch: 20, Val loss: 0.020085692011072475
Epoch: 21, Val loss: 0.019780256090906716
Epoch: 22, Val loss: 0.020174386926879315
Epoch: 23, Val loss: 0.020165151316609422
Epoch: 24, Val loss: 0.019983100799559537
Epoch: 25, Val loss: 0.021048184736805448
Epoch: 26, Val loss: 0.01996603382404073
Epoch: 27, Val loss: 0.02004830070408219
Epoch: 28, Val loss: 0.02003054110870018
Epoch: 29, Val loss: 0.019651795532238687
Epoch: 30, Val loss: 0.020265383724817784
Epoch: 31, Val loss: 0.019713678018395173
Epoch: 32, Val loss: 0.020490118760173603
Epoch: 33, Val loss: 0.01982915669805923
Epoch: 34, Val loss: 0.020039105109112748
Epoch: 35, Val loss: 0.020454895477426255
Epoch: 36, Val loss: 0.020307384352436512
Epoch: 37, Val loss: 0.01969385543301449
Epoch: 38, Val loss: 0.020567012098381073
Early stopping at epoch 38
Best validation loss this trial: 0.020567012098381073
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.05799412379662196
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.093825164800271
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.05023179315030575
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.023178996170981455
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09592322489390007
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.07051529219517341
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.07174681576016621
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.09930962833583865
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10548134721242465
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.11462973262000288
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.1584226828482416
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020104537240396708
Epoch: 1, Val loss: 0.020562349969887327
Epoch: 2, Val loss: 0.020474947567105804
Epoch: 3, Val loss: 0.020717129931172244
Epoch: 4, Val loss: 0.020334907560649082
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.19605457201852636
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.08161835003102946
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020157628843926977
Epoch: 1, Val loss: 0.02013517042192129
Epoch: 2, Val loss: 0.02025285066288506
Epoch: 3, Val loss: 0.02072901840711761
Epoch: 4, Val loss: 0.01991689456706373
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.10330157816155344
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019535106272460558
Epoch: 1, Val loss: 0.01935056255509456
Epoch: 2, Val loss: 0.0193791235359306
Epoch: 3, Val loss: 0.01938397549570371
Epoch: 4, Val loss: 0.019455959415461264
Epoch: 5, Val loss: 0.019391546965154827
Epoch: 6, Val loss: 0.0193875798334678
Epoch: 7, Val loss: 0.01933461972950106
Epoch: 8, Val loss: 0.01937140498119287
Epoch: 9, Val loss: 0.019528629146834724
Epoch: 10, Val loss: 0.01950756853653325
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019421849463485245
Epoch: 1, Val loss: 0.019330975870227713
Epoch: 2, Val loss: 0.01936124192757739
Epoch: 3, Val loss: 0.01952246006610047
Epoch: 4, Val loss: 0.0193490711812917
Epoch: 5, Val loss: 0.01938594813243701
Epoch: 6, Val loss: 0.019515669530528225
Epoch: 7, Val loss: 0.019306452005592167
Epoch: 8, Val loss: 0.019392217087567363
Epoch: 9, Val loss: 0.019387811756668948
Epoch: 10, Val loss: 0.019340249423224192
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019660017556614347
Epoch: 1, Val loss: 0.01928524324336113
Epoch: 2, Val loss: 0.01936784502851148
Epoch: 3, Val loss: 0.01930429848531882
Epoch: 4, Val loss: 0.01940483646865329
Epoch: 5, Val loss: 0.01936257879735313
Epoch: 6, Val loss: 0.019341178747833285
Epoch: 7, Val loss: 0.019348411359147638
Epoch: 8, Val loss: 0.019476009079088002
Epoch: 9, Val loss: 0.019397080231171388
Epoch: 10, Val loss: 0.019325083010217063
Epoch: 11, Val loss: 0.019421683592546698
Epoch: 12, Val loss: 0.019515369167057876
Epoch: 13, Val loss: 0.01939189301119146
Epoch: 14, Val loss: 0.019363627570052434
Epoch: 15, Val loss: 0.019356991602187484
Epoch: 16, Val loss: 0.019406024032296278
Epoch: 17, Val loss: 0.019470353276492693
Epoch: 18, Val loss: 0.01944579519172255
Epoch: 19, Val loss: 0.019475659363481224
Epoch: 20, Val loss: 0.019345659762620926
Epoch: 21, Val loss: 0.019421377394380223
Early stopping at epoch 21
Best validation loss this trial: 0.019421377394380223
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.0199815846231376
[I 2024-11-07 02:58:46,385] Trial 25 pruned. 
[I 2024-11-07 02:58:46,506] Trial 26 pruned. 
[I 2024-11-07 02:58:46,704] Trial 27 pruned. 
[I 2024-11-07 02:58:48,111] Trial 28 pruned. 
[I 2024-11-07 02:58:48,234] Trial 29 pruned. 
[I 2024-11-07 02:59:27,975] Trial 0 finished with value: 2.428445372922016e-07 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 2, 'hidden0_size': 256, 'hidden1_size': 64}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 02:59:59,742] Trial 1 finished with value: 1.1659646956705745e-06 and parameters: {'lr': 0.0001, 'batch_size': 128, 'n_hidden_layers': 3, 'hidden0_size': 256, 'hidden1_size': 32, 'hidden2_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.019354518809411515
Epoch: 1, Val loss: 0.019419696094452314
Epoch: 2, Val loss: 0.019436536953808405
Epoch: 3, Val loss: 0.019434655646387584
Epoch: 4, Val loss: 0.01933822687317291
Epoch: 5, Val loss: 0.01950947143153375
Epoch: 6, Val loss: 0.019549842776619215
Epoch: 7, Val loss: 0.01944928017293668
Epoch: 8, Val loss: 0.01947708235273504
Epoch: 9, Val loss: 0.019355272388674766
Epoch: 10, Val loss: 0.019481869362708595
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.18583878924449285
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.020618900167361155
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.01988396046953833
Epoch: 1, Val loss: 0.02044840395832673
Epoch: 2, Val loss: 0.020145241760163225
Epoch: 3, Val loss: 0.021143074951365463
<class 'architectures.SequenceRegressionLinear'>
<class 'architectures.SequenceRegressionLinear'>
Epoch: 0, Val loss: 0.1086739348868529
[32mOptimising hyperparameters for model: mlp[39m
Optimising K=0
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 4.6637350017116325e-05
Epoch: 1, Val loss: 2.9644704332335656e-05
Epoch: 2, Val loss: 1.2079032364811757e-05
Epoch: 3, Val loss: 2.281787269217043e-05
Epoch: 4, Val loss: 0.0007435944978332815
Epoch: 5, Val loss: 1.0397779183571962e-05
Epoch: 6, Val loss: 1.2609507073135226e-05
Epoch: 7, Val loss: 0.00016504156373517835
Epoch: 8, Val loss: 2.9026490233846766e-05
Epoch: 9, Val loss: 1.51784920393004e-05
Epoch: 10, Val loss: 2.642960047602663e-06
Epoch: 11, Val loss: 5.893947377638756e-05
Epoch: 12, Val loss: 3.7475309645684224e-06
Epoch: 13, Val loss: 7.218598929650133e-06
Epoch: 14, Val loss: 2.8616319829690574e-06
Epoch: 15, Val loss: 2.9080719525900818e-06
Epoch: 16, Val loss: 2.3193989549867197e-06
Epoch: 17, Val loss: 2.8839123391425633e-06
Epoch: 18, Val loss: 4.414176022493894e-06
Epoch: 19, Val loss: 4.538432001462417e-06
Epoch: 20, Val loss: 1.0095237721709927e-06
Epoch: 21, Val loss: 2.3617275890318656e-06
Epoch: 22, Val loss: 1.2486924354838222e-06
Epoch: 23, Val loss: 8.851334177743061e-07
Epoch: 24, Val loss: 1.4701595250893365e-06
Epoch: 25, Val loss: 2.481130538523532e-06
Epoch: 26, Val loss: 1.5291650983365402e-05
Epoch: 27, Val loss: 8.853310075934563e-07
Epoch: 28, Val loss: 3.6969973719265894e-05
Epoch: 29, Val loss: 9.826010129315293e-07
Epoch: 30, Val loss: 1.2252669837895373e-06
Epoch: 31, Val loss: 1.7902863936229613e-05
Epoch: 32, Val loss: 2.84314239221032e-05
Epoch: 33, Val loss: 2.503445108119257e-06
Epoch: 34, Val loss: 6.202450335838207e-06
Epoch: 35, Val loss: 0.00010261093207851896
Epoch: 36, Val loss: 6.527229580126751e-05
Epoch: 37, Val loss: 8.504365133098368e-07
Epoch: 38, Val loss: 2.8098225656376442e-05
Epoch: 39, Val loss: 3.163282384826582e-07
Epoch: 40, Val loss: 2.428445372922016e-07
Early stopping at epoch 40
Best validation loss this trial: 2.428445372922016e-07
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.013579998433716217
Epoch: 1, Val loss: 0.002102275250747926
Epoch: 2, Val loss: 0.00036163729323484634
Epoch: 3, Val loss: 0.00024780122488263554
Epoch: 4, Val loss: 0.0001816510969439474
Epoch: 5, Val loss: 0.00014313274177612137
Epoch: 6, Val loss: 0.00011649107650786606
Epoch: 7, Val loss: 9.349063142415893e-05
Epoch: 8, Val loss: 7.830971453123789e-05
Epoch: 9, Val loss: 6.718407203768917e-05
Epoch: 10, Val loss: 5.763357941019822e-05
Epoch: 11, Val loss: 5.0484362009999546e-05
Epoch: 12, Val loss: 4.52864900642268e-05
Epoch: 13, Val loss: 4.0109855559668343e-05
Epoch: 14, Val loss: 3.680164815504814e-05
Epoch: 15, Val loss: 3.33916458179898e-05
Epoch: 16, Val loss: 3.0942863951954136e-05
Epoch: 17, Val loss: 2.8696019075203183e-05
Epoch: 18, Val loss: 2.698510005587704e-05
Epoch: 19, Val loss: 2.5468323409598778e-05
Epoch: 20, Val loss: 2.4017754987067687e-05
Epoch: 21, Val loss: 2.4588584969702207e-05
Epoch: 22, Val loss: 2.182918473470451e-05
Epoch: 23, Val loss: 2.0875414628466448e-05
Epoch: 24, Val loss: 1.980976273860167e-05
Epoch: 25, Val loss: 1.866120661453983e-05
Epoch: 26, Val loss: 1.7808462933132078e-05
Epoch: 27, Val loss: 1.7251998265160413e-05
Epoch: 28, Val loss: 1.602901743430944e-05
Epoch: 29, Val loss: 1.529245044798314e-05
Epoch: 30, Val loss: 1.4192373752031649e-05
Epoch: 31, Val loss: 1.3753865120742051e-05
Epoch: 32, Val loss: 1.577352554319324e-05
Epoch: 33, Val loss: 1.2515555311638617e-05
Epoch: 34, Val loss: 1.175148266202857e-05
Epoch: 35, Val loss: 1.1066293216387394e-05
Epoch: 36, Val loss: 1.1165801756221098e-05
Epoch: 37, Val loss: 1.0199186509468699e-05
Epoch: 38, Val loss: 9.219564432542183e-06
Epoch: 39, Val loss: 9.68413747090134e-06
Epoch: 40, Val loss: 8.477863471945688e-06
Epoch: 41, Val loss: 7.969823253900405e-06
Epoch: 42, Val loss: 7.626901613980023e-06
Epoch: 43, Val loss: 7.24910252479199e-06
Epoch: 44, Val loss: 6.907071164570911e-06
Epoch: 45, Val loss: 6.523621261479447e-06
Epoch: 46, Val loss: 6.213647662600309e-06
Epoch: 47, Val loss: 5.857357699192184e-06
Epoch: 48, Val loss: 7.5017925016670805e-06
Epoch: 49, Val loss: 5.9832749977462206e-06
Epoch: 50, Val loss: 5.4065060147091206e-06
Epoch: 51, Val loss: 5.331050226108716e-06
Epoch: 52, Val loss: 4.601895093632481e-06
Epoch: 53, Val loss: 4.402640077048198e-06
Epoch: 54, Val loss: 4.223425911100171e-06
Epoch: 55, Val loss: 4.147393005719789e-06
Epoch: 56, Val loss: 4.27656670380803e-06
Epoch: 57, Val loss: 4.760732778422761e-06
Epoch: 58, Val loss: 3.5934960493959497e-06
Epoch: 59, Val loss: 3.526772311719685e-06
Epoch: 60, Val loss: 3.847028777164592e-06
Epoch: 61, Val loss: 3.272810394942792e-06
Epoch: 62, Val loss: 3.2861061553977393e-06
Epoch: 63, Val loss: 2.999328529058711e-06
Epoch: 64, Val loss: 2.994597838612932e-06
Epoch: 65, Val loss: 4.110317161313891e-06
Epoch: 66, Val loss: 2.65895044136969e-06
Epoch: 67, Val loss: 2.5919908709464975e-06
Epoch: 68, Val loss: 2.454528938006522e-06
Epoch: 69, Val loss: 3.1240343883775678e-06
Epoch: 70, Val loss: 2.274927456417541e-06
Epoch: 71, Val loss: 2.2899614346681766e-06
Epoch: 72, Val loss: 2.760449167569385e-06
Epoch: 73, Val loss: 2.0630687740160316e-06
Epoch: 74, Val loss: 2.163413010378663e-06
Epoch: 75, Val loss: 2.4256589999128927e-06
Epoch: 76, Val loss: 2.0643551502193293e-06
Epoch: 77, Val loss: 1.9705802191372458e-06
Epoch: 78, Val loss: 1.7565470909823365e-06
Epoch: 79, Val loss: 1.8005119396178288e-06
Epoch: 80, Val loss: 1.6858640492500854e-06
Epoch: 81, Val loss: 1.7260418833418151e-06
Epoch: 82, Val loss: 1.9736718652477404e-06
Epoch: 83, Val loss: 1.534244054679313e-06
Epoch: 84, Val loss: 1.4884944864777134e-06
Epoch: 85, Val loss: 1.4845339704044232e-06
Epoch: 86, Val loss: 1.4474810733976853e-06
Epoch: 87, Val loss: 1.3570747343800253e-06
Epoch: 88, Val loss: 1.6155662979915595e-06
Epoch: 89, Val loss: 1.9253022701840086e-06
Epoch: 90, Val loss: 2.064434476655521e-06
Epoch: 91, Val loss: 1.251204949667072e-06
Epoch: 92, Val loss: 1.3725623549069466e-06
Epoch: 93, Val loss: 1.461597020799241e-06
Epoch: 94, Val loss: 2.00339001698382e-06
Epoch: 95, Val loss: 1.1542527894084357e-06
Epoch: 96, Val loss: 1.3078145060316567e-06
Epoch: 97, Val loss: 1.0599143223062438e-06
Epoch: 98, Val loss: 1.42889742403633e-06
Epoch: 99, Val loss: 1.048193689134138e-06
Epoch: 100, Val loss: 1.3877543771309704e-06
Epoch: 101, Val loss: 1.3223145376703202e-06
Epoch: 102, Val loss: 1.066938847990537e-06
Epoch: 103, Val loss: 1.1659646956705745e-06
Early stopping at epoch 103
Best validation loss this trial: 1.1659646956705745e-06
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 2.161682861199544e-05
Epoch: 1, Val loss: 5.551356111528805e-06
Epoch: 2, Val loss: 2.4599653743153895e-06
Epoch: 3, Val loss: 1.4284766183145155e-06
Epoch: 4, Val loss: 9.244811019203731e-07
Epoch: 5, Val loss: 0.0002560346623888621
Epoch: 6, Val loss: 1.0709567592027045e-06
[I 2024-11-07 03:00:12,048] Trial 2 finished with value: 7.123002949375739e-05 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:00:28,758] Trial 3 finished with value: 2.1618140928725458e-05 and parameters: {'lr': 0.01, 'batch_size': 128, 'n_hidden_layers': 3, 'hidden0_size': 32, 'hidden1_size': 64, 'hidden2_size': 128}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:00:55,138] Trial 4 finished with value: 3.8414347377860955e-05 and parameters: {'lr': 0.001, 'batch_size': 32, 'n_hidden_layers': 1, 'hidden0_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:00:55,419] Trial 5 pruned. 
[I 2024-11-07 03:00:56,002] Trial 6 pruned. 
[I 2024-11-07 03:01:03,572] Trial 7 pruned. 
[I 2024-11-07 03:01:03,732] Trial 8 pruned. 
[I 2024-11-07 03:01:04,703] Trial 9 pruned. 
[I 2024-11-07 03:01:04,850] Trial 10 pruned. 
[I 2024-11-07 03:01:05,161] Trial 11 pruned. 
[I 2024-11-07 03:01:05,999] Trial 12 pruned. 
[I 2024-11-07 03:01:06,280] Trial 13 pruned. 
[I 2024-11-07 03:01:07,383] Trial 14 pruned. 
[I 2024-11-07 03:01:10,447] Trial 15 pruned. 
[I 2024-11-07 03:01:10,963] Trial 16 pruned. 
[I 2024-11-07 03:01:11,146] Trial 17 pruned. 
[I 2024-11-07 03:01:11,392] Trial 18 pruned. 
[I 2024-11-07 03:01:12,343] Trial 19 pruned. 
[I 2024-11-07 03:01:12,652] Trial 20 pruned. 
[I 2024-11-07 03:01:13,560] Trial 21 pruned. 
[I 2024-11-07 03:01:15,695] Trial 22 pruned. 
[I 2024-11-07 03:01:16,005] Trial 23 pruned. 
[I 2024-11-07 03:01:16,318] Trial 24 pruned. 
Epoch: 7, Val loss: 6.933677610100454e-05
Epoch: 8, Val loss: 1.135680839975727e-06
Epoch: 9, Val loss: 5.2150623742686275e-06
Epoch: 10, Val loss: 2.0417308417400377e-06
Epoch: 11, Val loss: 1.944487030802839e-06
Epoch: 12, Val loss: 0.00038897144557454455
Epoch: 13, Val loss: 3.121422135056771e-06
Epoch: 14, Val loss: 3.038085307521098e-06
Epoch: 15, Val loss: 2.002470002981452e-06
Epoch: 16, Val loss: 1.5711575229441876e-06
Epoch: 17, Val loss: 1.4458603858264155e-06
Epoch: 18, Val loss: 1.0643279935728558e-05
Epoch: 19, Val loss: 2.2114558799575416e-06
Epoch: 20, Val loss: 0.00011684097890427703
Epoch: 21, Val loss: 2.4074400811452164e-05
Epoch: 22, Val loss: 1.547299856393405e-05
Epoch: 23, Val loss: 7.123002949375739e-05
Early stopping at epoch 23
Best validation loss this trial: 7.123002949375739e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 3.967761462633261e-05
Epoch: 1, Val loss: 1.2015946440904803e-05
Epoch: 2, Val loss: 6.095294858854593e-06
Epoch: 3, Val loss: 5.400522591656427e-06
Epoch: 4, Val loss: 1.3728757317116784e-05
Epoch: 5, Val loss: 5.02415328980024e-06
Epoch: 6, Val loss: 4.25542314094516e-06
Epoch: 7, Val loss: 7.458906134546604e-06
Epoch: 8, Val loss: 2.6790876568233895e-05
Epoch: 9, Val loss: 3.162297931123896e-05
Epoch: 10, Val loss: 2.501316457402042e-06
Epoch: 11, Val loss: 2.9419168108919837e-06
Epoch: 12, Val loss: 2.889046472619619e-05
Epoch: 13, Val loss: 2.746744055110543e-05
Epoch: 14, Val loss: 4.433816699883348e-06
Epoch: 15, Val loss: 2.7841989009884546e-06
Epoch: 16, Val loss: 9.813666133981067e-06
Epoch: 17, Val loss: 5.916076832178633e-05
Epoch: 18, Val loss: 3.683716640499385e-06
Epoch: 19, Val loss: 4.8315561829347925e-06
Epoch: 20, Val loss: 2.161740282945637e-06
Epoch: 21, Val loss: 3.2672774145563165e-06
Epoch: 22, Val loss: 1.4873536641108192e-06
Epoch: 23, Val loss: 3.859268541048751e-06
Epoch: 24, Val loss: 1.5425522319365898e-06
Epoch: 25, Val loss: 7.973523725784123e-06
Epoch: 26, Val loss: 5.7294756069017764e-06
Epoch: 27, Val loss: 4.798224076925396e-06
Epoch: 28, Val loss: 6.467471528872088e-07
Epoch: 29, Val loss: 1.7802774799115545e-05
Epoch: 30, Val loss: 6.671873013717339e-06
Epoch: 31, Val loss: 2.6011768522178082e-06
Epoch: 32, Val loss: 2.8733580281476537e-05
Epoch: 33, Val loss: 4.254963730507031e-07
Epoch: 34, Val loss: 3.684518501898511e-05
Epoch: 35, Val loss: 5.042267715044406e-06
Epoch: 36, Val loss: 9.252818707386337e-07
Epoch: 37, Val loss: 1.8759153243667303e-06
Epoch: 38, Val loss: 1.2061168211218147e-06
Epoch: 39, Val loss: 1.9751589141347816e-06
Epoch: 40, Val loss: 0.00010289732745150104
Epoch: 41, Val loss: 8.1207982953332e-06
Epoch: 42, Val loss: 2.2803688456776198e-06
Epoch: 43, Val loss: 1.5214777981859467e-05
Epoch: 44, Val loss: 4.354319810510664e-06
Epoch: 45, Val loss: 1.2013226327233774e-05
Epoch: 46, Val loss: 2.714209316209081e-06
Epoch: 47, Val loss: 1.1014606472553307e-06
Epoch: 48, Val loss: 5.716786483266703e-06
Epoch: 49, Val loss: 8.068621430199636e-07
Epoch: 50, Val loss: 1.8179262793884105e-06
Epoch: 51, Val loss: 7.32939996604558e-07
Epoch: 52, Val loss: 1.108481163069975e-06
Epoch: 53, Val loss: 2.1618140928725458e-05
Early stopping at epoch 53
Best validation loss this trial: 2.1618140928725458e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0002010054863020718
Epoch: 1, Val loss: 6.132726321861538e-05
Epoch: 2, Val loss: 2.517675311459725e-05
Epoch: 3, Val loss: 9.379906377283536e-06
Epoch: 4, Val loss: 3.945918699647671e-06
Epoch: 5, Val loss: 1.3314453464994212e-06
Epoch: 6, Val loss: 1.5060205061913582e-06
Epoch: 7, Val loss: 1.3000545904004766e-06
Epoch: 8, Val loss: 7.675616860294998e-06
Epoch: 9, Val loss: 1.7772130524820717e-05
Epoch: 10, Val loss: 7.886643284041578e-07
Epoch: 11, Val loss: 9.651802116893208e-08
Epoch: 12, Val loss: 3.548894109562208e-06
Epoch: 13, Val loss: 2.3224765293454193e-06
Epoch: 14, Val loss: 8.38971975257796e-07
Epoch: 15, Val loss: 1.2978095500986658e-06
Epoch: 16, Val loss: 3.339362759343096e-08
Epoch: 17, Val loss: 1.930383689705778e-07
Epoch: 18, Val loss: 7.635471411832786e-09
Epoch: 19, Val loss: 6.953614662612307e-08
Epoch: 20, Val loss: 1.8039816910637983e-06
Epoch: 21, Val loss: 3.0247882399145546e-06
Epoch: 22, Val loss: 2.8138025299023525e-06
Epoch: 23, Val loss: 2.1557068645735977e-05
Epoch: 24, Val loss: 2.0065063939711808e-07
Epoch: 25, Val loss: 1.257286196597757e-07
Epoch: 26, Val loss: 8.02376400459361e-08
Epoch: 27, Val loss: 5.998109214439423e-09
Epoch: 28, Val loss: 1.3728744946085e-05
Epoch: 29, Val loss: 6.91553180077201e-08
Epoch: 30, Val loss: 3.4735982582703597e-09
Epoch: 31, Val loss: 3.8414347377860955e-05
Early stopping at epoch 31
Best validation loss this trial: 3.8414347377860955e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0001700597488907387
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 5.7217646298161904e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 5.052331335489601e-06
Epoch: 1, Val loss: 5.45887094788738e-06
Epoch: 2, Val loss: 1.1084106774479807e-05
Epoch: 3, Val loss: 0.00010286073905488469
Epoch: 4, Val loss: 7.263024080257154e-05
Epoch: 5, Val loss: 6.117680788877953e-06
Epoch: 6, Val loss: 9.20425028687952e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0001712557413460066
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0012258503958682378
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00014097539024078288
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.01861418778929165
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0009576419056701657
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00801287395721775
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00034182932902959327
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 4.4473526879486044e-05
Epoch: 1, Val loss: 1.392538117709021e-05
Epoch: 2, Val loss: 7.100189616151403e-06
Epoch: 3, Val loss: 4.130337843134377e-06
Epoch: 4, Val loss: 2.7329460574222495e-06
Epoch: 5, Val loss: 2.578964480855606e-06
Epoch: 6, Val loss: 1.520069121921798e-05
Epoch: 7, Val loss: 3.653069929356916e-06
Epoch: 8, Val loss: 2.536375152777833e-05
Epoch: 9, Val loss: 3.807583596935112e-06
Epoch: 10, Val loss: 3.670982065251969e-06
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0006475443787610111
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00013986014552453224
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.02152456332124391
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 5.541867919504527e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.010189638110035556
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 2.0904243853183904e-05
Epoch: 1, Val loss: 1.9308169905374504e-05
Epoch: 2, Val loss: 6.326214436015461e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 4.506368081897331e-05
Epoch: 1, Val loss: 1.7331297462756776e-05
Epoch: 2, Val loss: 9.337968684399206e-06
Epoch: 3, Val loss: 4.711048248636717e-06
Epoch: 4, Val loss: 8.561989442707765e-06
Epoch: 5, Val loss: 5.3301333167128485e-05
Epoch: 6, Val loss: 4.690292766392335e-06
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00011499717047781352
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00021583057636142535
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 8.031215223585854e-06
Epoch: 1, Val loss: 0.00010449066428561162
Epoch: 2, Val loss: 8.14108637772982e-05
Epoch: 3, Val loss: 6.8683072046182e-06
Epoch: 4, Val loss: 6.582856390468028e-06
Epoch: 5, Val loss: 2.8128599495129234e-05
Epoch: 6, Val loss: 2.461325280842758e-06
Epoch: 7, Val loss: 3.937621702081944e-06
Epoch: 8, Val loss: 2.4824553392598814e-06
Epoch: 9, Val loss: 2.6431556489892228e-05
Epoch: 10, Val loss: 2.4711884788124207e-06
Epoch: 11, Val loss: 4.655781492407482e-06
Epoch: 12, Val loss: 3.9646157244307815e-06
[I 2024-11-07 03:01:32,671] Trial 25 pruned. 
[I 2024-11-07 03:01:34,905] Trial 26 pruned. 
[I 2024-11-07 03:01:35,069] Trial 27 pruned. 
[I 2024-11-07 03:01:35,379] Trial 28 pruned. 
[I 2024-11-07 03:01:35,894] Trial 29 pruned. 
[I 2024-11-07 03:02:13,466] Trial 30 finished with value: 3.137439428731644e-05 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 128}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:02:20,078] Trial 31 pruned. 
[I 2024-11-07 03:02:22,905] Trial 32 pruned. 
[I 2024-11-07 03:02:29,556] Trial 33 pruned. 
[I 2024-11-07 03:02:30,394] Trial 34 pruned. 
[I 2024-11-07 03:02:31,390] Trial 35 pruned. 
[I 2024-11-07 03:02:31,714] Trial 36 pruned. 
[I 2024-11-07 03:02:32,716] Trial 37 pruned. 
[I 2024-11-07 03:02:33,240] Trial 38 pruned. 
[I 2024-11-07 03:02:33,391] Trial 39 pruned. 
[I 2024-11-07 03:02:33,709] Trial 40 pruned. 
[I 2024-11-07 03:02:34,542] Trial 41 pruned. 
[I 2024-11-07 03:02:35,353] Trial 42 pruned. 
[I 2024-11-07 03:02:36,164] Trial 43 pruned. 
[I 2024-11-07 03:02:36,985] Trial 44 pruned. 
[I 2024-11-07 03:02:38,046] Trial 45 pruned. 
[I 2024-11-07 03:02:38,362] Trial 46 pruned. 
[I 2024-11-07 03:02:38,507] Trial 47 pruned. 
[I 2024-11-07 03:02:39,570] Trial 48 pruned. 
[I 2024-11-07 03:02:39,819] Trial 49 pruned. 
[I 2024-11-07 03:02:46,593] Trial 50 pruned. 
[I 2024-11-07 03:02:58,711] Trial 51 finished with value: 2.803172138268347e-05 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:03:02,216] Trial 52 pruned. 
[I 2024-11-07 03:03:13,720] Trial 53 finished with value: 2.1437409945313906e-05 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
Epoch: 13, Val loss: 5.837268687702087e-05
Epoch: 14, Val loss: 8.747713347171763e-06
Epoch: 15, Val loss: 6.888821996817102e-05
Epoch: 16, Val loss: 9.762270001685356e-06
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.3014535759280896e-05
Epoch: 1, Val loss: 1.70530769319779e-05
Epoch: 2, Val loss: 9.904028086325563e-06
Epoch: 3, Val loss: 4.742459860320888e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00013480949128279461
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 9.215633413000664e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.015527543349143786
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 8.757630879676397e-06
Epoch: 1, Val loss: 1.8527415696965356e-05
Epoch: 2, Val loss: 1.1862573322486063e-05
Epoch: 3, Val loss: 0.00023978185705874418
Epoch: 4, Val loss: 1.9231274530898383e-05
Epoch: 5, Val loss: 4.637607527647016e-05
Epoch: 6, Val loss: 2.2084553126415603e-06
Epoch: 7, Val loss: 0.000490415395082285
Epoch: 8, Val loss: 4.513102801391717e-06
Epoch: 9, Val loss: 1.5059718695046573e-05
Epoch: 10, Val loss: 3.1268915050924647e-06
Epoch: 11, Val loss: 2.3908549356595663e-06
Epoch: 12, Val loss: 1.7856843049694012e-06
Epoch: 13, Val loss: 2.8881955067864846e-06
Epoch: 14, Val loss: 1.729007861283021e-06
Epoch: 15, Val loss: 2.5463853282813848e-06
Epoch: 16, Val loss: 3.008538136098526e-05
Epoch: 17, Val loss: 8.760068505238389e-06
Epoch: 18, Val loss: 1.0571272582394981e-06
Epoch: 19, Val loss: 6.68587218709805e-06
Epoch: 20, Val loss: 5.0181252538453155e-06
Epoch: 21, Val loss: 1.829657495581447e-06
Epoch: 22, Val loss: 1.0653690229498982e-06
Epoch: 23, Val loss: 8.447058157371353e-07
Epoch: 24, Val loss: 3.482751808638941e-05
Epoch: 25, Val loss: 9.90832758163425e-06
Epoch: 26, Val loss: 2.5869467931326468e-05
Epoch: 27, Val loss: 3.3380553533913146e-05
Epoch: 28, Val loss: 5.175911589045178e-06
Epoch: 29, Val loss: 4.1247516523242695e-05
Epoch: 30, Val loss: 7.218557164433967e-07
Epoch: 31, Val loss: 9.493124290459235e-07
Epoch: 32, Val loss: 2.2225301966842444e-06
Epoch: 33, Val loss: 2.080799199937219e-06
Epoch: 34, Val loss: 1.2389872466898936e-05
Epoch: 35, Val loss: 1.2337317204542276e-06
Epoch: 36, Val loss: 1.8221097693387694e-06
Epoch: 37, Val loss: 9.620082559754686e-06
Epoch: 38, Val loss: 3.137439428731644e-05
Early stopping at epoch 38
Best validation loss this trial: 3.137439428731644e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 4.469252710972579e-06
Epoch: 1, Val loss: 7.164399921048943e-06
Epoch: 2, Val loss: 9.635049713728626e-06
Epoch: 3, Val loss: 3.41638706076137e-05
Epoch: 4, Val loss: 2.1205758488747993e-05
Epoch: 5, Val loss: 9.439536409204304e-06
Epoch: 6, Val loss: 3.8843954600576355e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.41114626952695e-05
Epoch: 1, Val loss: 8.120768584804032e-05
Epoch: 2, Val loss: 2.5473300974825288e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.2725733855544464e-05
Epoch: 1, Val loss: 1.564005910600267e-05
Epoch: 2, Val loss: 7.318335756903035e-06
Epoch: 3, Val loss: 0.0007453092544799289
Epoch: 4, Val loss: 2.617656926879571e-05
Epoch: 5, Val loss: 1.6425644164098005e-05
Epoch: 6, Val loss: 1.0571062812932023e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00020681485373518852
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00014878317074945723
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 5.2513128678783995e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 5.4557004410976995e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 9.876432687812285e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00019978024211013689
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.010728632368273655
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00013130755910540943
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00023228613754016004
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00015747059756111598
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.00011628290881978897
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0007350646863834789
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.0001378755745679235
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.09752207398414611
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 9.646932504927882e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 0.01976456082725929
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 8.12149208745062e-06
Epoch: 1, Val loss: 2.4028624368986486e-05
Epoch: 2, Val loss: 1.667081134352581e-05
Epoch: 3, Val loss: 3.672307153585715e-06
Epoch: 4, Val loss: 4.818070676117356e-06
Epoch: 5, Val loss: 9.392175446235218e-06
Epoch: 6, Val loss: 4.9275977600591745e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.822117106463789e-05
Epoch: 1, Val loss: 5.786014275957461e-06
Epoch: 2, Val loss: 2.585440005615856e-06
Epoch: 3, Val loss: 1.4786128987181585e-06
Epoch: 4, Val loss: 9.29645117718445e-07
Epoch: 5, Val loss: 6.425680086025319e-07
Epoch: 6, Val loss: 6.698062558765524e-07
Epoch: 7, Val loss: 7.790609844151833e-06
Epoch: 8, Val loss: 9.5019234363346e-07
Epoch: 9, Val loss: 3.17255355153804e-05
Epoch: 10, Val loss: 1.958395217632984e-06
Epoch: 11, Val loss: 1.624338303885545e-06
Epoch: 12, Val loss: 2.7351430352975247e-05
Epoch: 13, Val loss: 1.8272703430886752e-06
Epoch: 14, Val loss: 2.104751131763549e-06
Epoch: 15, Val loss: 7.520937770018036e-06
Epoch: 16, Val loss: 2.247352259691357e-06
Epoch: 17, Val loss: 7.214714083275818e-06
Epoch: 18, Val loss: 0.00022363395598800614
Epoch: 19, Val loss: 8.941783620778155e-06
Epoch: 20, Val loss: 6.535665703174742e-06
Epoch: 21, Val loss: 2.140057653412861e-06
Epoch: 22, Val loss: 5.679427530571374e-06
Epoch: 23, Val loss: 2.803172138268347e-05
Early stopping at epoch 23
Best validation loss this trial: 2.803172138268347e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.415258142394597e-05
Epoch: 1, Val loss: 4.203340296271563e-06
Epoch: 2, Val loss: 2.4892389432290315e-06
Epoch: 3, Val loss: 2.4614738951920746e-05
Epoch: 4, Val loss: 0.00012030000080758483
Epoch: 5, Val loss: 4.033790464046223e-06
Epoch: 6, Val loss: 9.882152423811838e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.4322753276456542e-05
Epoch: 1, Val loss: 2.8471615863046695e-06
Epoch: 2, Val loss: 1.0453092246905758e-06
Epoch: 3, Val loss: 4.978133656559916e-07
Epoch: 4, Val loss: 5.221373859392335e-07
Epoch: 5, Val loss: 2.982583022940084e-05
Epoch: 6, Val loss: 1.013378473602736e-06
Epoch: 7, Val loss: 1.1825083862455886e-06
Epoch: 8, Val loss: 6.987781997826786e-05
Epoch: 9, Val loss: 1.6374638745087123e-05
Epoch: 10, Val loss: 1.5216533033412038e-06
Epoch: 11, Val loss: 6.5336451596397865e-06
Epoch: 12, Val loss: 3.4599305060051597e-06
Epoch: 13, Val loss: 7.946958699655166e-06
Epoch: 14, Val loss: 2.155854995996775e-05
Epoch: 15, Val loss: 2.706218348672943e-06
Epoch: 16, Val loss: 3.394317747974999e-06
Epoch: 17, Val loss: 6.093823347514561e-06
Epoch: 18, Val loss: 1.8153611316027573e-06
Epoch: 19, Val loss: 3.7289972327086548e-06
Epoch: 20, Val loss: 2.4657212846164825e-06
Epoch: 21, Val loss: 3.3055714247427368e-06
Epoch: 22, Val loss: 2.1437409945313906e-05
Early stopping at epoch 22
Best validation loss this trial: 2.1437409945313906e-05
<class 'architectures.SequenceRegressionMLP'>
Epoch: 0, Val loss: 1.9072386558580812e-05
Epoch: 1, Val loss: 3.929385908359731e-06
Epoch: 2, Val loss: 2.4909428633337584e-06
Epoch: 3, Val loss: 2.4815919980619194e-05
Epoch: 4, Val loss: 1.1457877414718846e-06
Epoch: 5, Val loss: 4.390560764702844e-06
Epoch: 6, Val loss: 5.710006751884128e-06
Epoch: 7, Val loss: 1.2942598187559906e-06
Epoch: 8, Val loss: 7.176823768314247e-06
Epoch: 9, Val loss: 1.2787823640178236e-05
[I 2024-11-07 03:03:26,010] Trial 54 finished with value: 3.278129765814624e-06 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:03:37,321] Trial 55 finished with value: 3.166945185351005e-06 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:03:38,830] Trial 56 pruned. 
[I 2024-11-07 03:03:39,339] Trial 57 pruned. 
[I 2024-11-07 03:03:50,929] Trial 58 finished with value: 1.3822507539246615e-06 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
[I 2024-11-07 03:04:03,240] Trial 59 finished with value: 6.3905615991869e-06 and parameters: {'lr': 0.01, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 2.428445372922016e-07.
