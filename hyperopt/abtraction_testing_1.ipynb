{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3e378c-0a8c-45fa-832e-1908c298c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna as opt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/nk-paper-2024-1/pscapes')\n",
    "sys.path.append('/home/ubuntu/nk-paper-2024-1/nk-ml-2024')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pscapes.landscape_class import ProteinLandscape\n",
    "from pscapes.utils import dict_to_np_array, np_array_to_dict\n",
    "\n",
    "from src.architectures.architectures import SequenceRegressionCNN, SequenceRegressionLSTM, SequenceRegressionMLP, SequenceRegressionLinear, SequenceRegressionTransformer\n",
    "from src.architectures.ml_utils import train_val_test_split_ohe, train_model\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from hyperopt import optimise_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858be91a-c32e-455f-a64e-d0d6ee136b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6\n",
    "AA_ALPHABET = 'ACDEFG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90aaa7d-7538-4bd4-bd9f-b36f40ea605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load NK landscapes -- only a single replicate for hparam tuning \n",
    "\n",
    "LANDSCAPES = []\n",
    "for k in range(6): \n",
    "    for r in range(1): \n",
    "        landscape = ProteinLandscape(csv_path='../data/nk_landscapes/k{0}_r{1}.csv'.format(k,r), amino_acids=AA_ALPHABET)\n",
    "        LANDSCAPES.append(landscape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de296c25-c04e-4028-b841-10299ccdc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDSCAPES = [i.fit_OHE() for i in LANDSCAPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112696d6-ddac-40aa-a468-a56810985990",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscapes_ohe, xy_train, xy_val, xy_test, x_test, y_test = train_val_test_split_ohe(LANDSCAPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a776c8f-9f07-43b7-b395-b331b7e4d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape0_xy_train = xy_train[0]\n",
    "landscape0_xy_val   = xy_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34569c1-e4d7-40ea-a80b-00550851e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    num_conv_layers = trial.suggest_int('num_conv_layers', 1, 2)\n",
    "    \n",
    "    num_kernels = [int(trial.suggest_discrete_uniform(\"n_kernels\", 16, 128, 16))\n",
    "                   for i in range(num_conv_layers)]  \n",
    "    \n",
    "    kernel_sizes = [int(trial.suggest_discrete_uniform(\"kernel_sizes\", 2, 6, 1))\n",
    "                   for i in range(num_conv_layers)]\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = trial.suggest_discrete_uniform('batch_size', 32, 128, 32)\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionCNN(input_channels=len(AA_ALPHABET), sequence_length=SEQ_LEN, \n",
    "                                  num_conv_layers=num_conv_layers, n_kernels=num_kernels, kernel_sizes=kernel_sizes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dummy training and validation data loaders\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e907bfbd-9e56-4129-b680-b447d9bdb5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSequenceRegressionMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malphabet_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SequenceRegressionMLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4da4c8-c114-45e9-b649-2a45c8e5e202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5a7783-80cd-4f78-9b1c-5230c09550bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    n_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 4)\n",
    "    hidden_sizes    = [int(trial.suggest_categorical(\"hidden{}_size\".format(i), [32,64, 96, 128, 256])) \n",
    "                       for i in range(n_hidden_layers)]\n",
    "    print(hidden_sizes)\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 128, 32))\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionMLP(alphabet_size=len(amino_acids), sequence_length=seq_length, hidden_sizes=hidden_sizes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bafd400a-a09f-45dd-8d2a-9e76975dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unidirectional_lstm_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_size    = trial.suggest_categorical(\"hidden_size\", [32,64, 96, 128, 256])\n",
    "                       \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionLSTM(input_size=len(amino_acids), hidden_size=hidden_size, num_layers=num_layers, bidirectional=False)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f60dce-8a86-4bfe-b4a9-087c387ac9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_size    = trial.suggest_categorical(\"hidden_size\", [32,64, 96, 128, 256])\n",
    "                       \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionLSTM(input_size=len(amino_acids), hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba89e1cf-1b9e-4e91-920a-2ff27d71c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_valid_combinations_transformer(embed_dim_options, max_heads):\n",
    "    valid_combinations = []\n",
    "    \n",
    "    for embed_dim in embed_dim_options:\n",
    "        for num_heads in range(1, max_heads + 1):\n",
    "            if embed_dim % num_heads == 0:\n",
    "                valid_combinations.append((embed_dim, num_heads))\n",
    "    \n",
    "    return valid_combinations\n",
    "embed_dim_options = [16, 32, 64, 128]\n",
    "max_heads = 8\n",
    "\n",
    "valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d5f96bf-1210-4c22-8b04-ce786c17cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    input_dim = len(amino_acids)\n",
    "    #d_model is the embedding \n",
    "\n",
    "    embed_dim_options = [16, 32, 64, 128]\n",
    "    max_heads = 8\n",
    "    valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)\n",
    "\n",
    "    d_model, nhead = trial.suggest_categorical(\"embed_dim_num_heads\", valid_combinations)\n",
    "        \n",
    "    num_layers      = trial.suggest_int('num_layers', 1, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256, 512]) \n",
    "    max_seq_length  = trial.suggest_categorical(\"max_seq_length\", [6, 8, 10, 12])                  \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionTransformer(input_dim=input_dim, d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                         max_seq_length=max_seq_length)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0e7a8-8e67-4e52-805b-5910a8d19c47",
   "metadata": {},
   "outputs": [],
   "source": [
    " SequenceRegressionLinear(alphabet_size=5, sequence_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa57ca2-f9a1-4228-b810-e3b82486a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    model = SequenceRegressionLinear(alphabet_size=len(amino_acids), sequence_length=seq_length)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f1ff715-6045-4794-bb17-b4583fcba008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:51:24,796] A new study created in memory with name: no-name-80664c61-0757-4a88-84a8-e0e124b059ea\n",
      "[I 2024-10-30 06:51:24,797] A new study created in memory with name: no-name-3644e0d0-e967-4b9c-a6db-a56ca8250d6d\n",
      "[I 2024-10-30 06:51:24,798] A new study created in memory with name: no-name-56ad802a-a4d7-4112-b72f-d32c5cdc0787\n",
      "[I 2024-10-30 06:51:24,799] A new study created in memory with name: no-name-5a625edb-c9e0-4eb0-ac71-4ad7e2bd9ae6\n",
      "[I 2024-10-30 06:51:24,799] A new study created in memory with name: no-name-537a34c0-aa29-4cb6-acdb-06cba6c79b9d\n",
      "[I 2024-10-30 06:51:24,800] A new study created in memory with name: no-name-9a67bddf-149f-483a-9510-0b3cf2786b3e\n"
     ]
    }
   ],
   "source": [
    "#cnn_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#mlp_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#lstm_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#transformer_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "linear_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51402487-3306-42cc-a9a5-c1900f13118e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3441/689578029.py:8: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 1.2402881593693669e-15\n",
      "Epoch: 1, Val loss: 1.3197617229092686e-15\n",
      "Epoch: 2, Val loss: 1.828292697591813e-15\n",
      "Epoch: 3, Val loss: 2.6505454891931755e-14\n",
      "Epoch: 4, Val loss: 2.836986701585662e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:12,775] Trial 0 finished with value: 3.3416055099907804e-05 and parameters: {'lr': 0.01, 'batch_size': 32.0}. Best is trial 0 with value: 3.3416055099907804e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 3.3416055099907804e-05\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 3.3416055099907804e-05\n",
      "Epoch: 0, Val loss: 0.5427766327674572\n",
      "Epoch: 1, Val loss: 0.41609393251247895\n",
      "Epoch: 2, Val loss: 0.31599770066065663\n",
      "Epoch: 3, Val loss: 0.2385255109805327\n",
      "Epoch: 4, Val loss: 0.17974791083580408\n",
      "Epoch: 5, Val loss: 0.13632374295057395\n",
      "Epoch: 6, Val loss: 0.10488661894431481\n",
      "Epoch: 7, Val loss: 0.08247372355216588\n",
      "Epoch: 8, Val loss: 0.06673415100727326\n",
      "Epoch: 9, Val loss: 0.05559699351970966\n",
      "Epoch: 10, Val loss: 0.04750126227736473\n",
      "Epoch: 11, Val loss: 0.041304399665349566\n",
      "Epoch: 12, Val loss: 0.03627334119608769\n",
      "Epoch: 13, Val loss: 0.031975625321651116\n",
      "Epoch: 14, Val loss: 0.02813585475087166\n",
      "Epoch: 15, Val loss: 0.02464491420258314\n",
      "Epoch: 16, Val loss: 0.021449160881531544\n",
      "Epoch: 17, Val loss: 0.018525369751911897\n",
      "Epoch: 18, Val loss: 0.01585659236671069\n",
      "Epoch: 19, Val loss: 0.013443658844782757\n",
      "Epoch: 20, Val loss: 0.011282187337294603\n",
      "Epoch: 21, Val loss: 0.00936904530494641\n",
      "Epoch: 22, Val loss: 0.007692156156572776\n",
      "Epoch: 23, Val loss: 0.006241272346904645\n",
      "Epoch: 24, Val loss: 0.004997267895258772\n",
      "Epoch: 25, Val loss: 0.003949393566029194\n",
      "Epoch: 26, Val loss: 0.0030796525713342885\n",
      "Epoch: 27, Val loss: 0.002361623080781637\n",
      "Epoch: 28, Val loss: 0.0017841608406832586\n",
      "Epoch: 29, Val loss: 0.0013251441324917744\n",
      "Epoch: 30, Val loss: 0.0009665800980889263\n",
      "Epoch: 31, Val loss: 0.0006917628805893353\n",
      "Epoch: 32, Val loss: 0.0004849773085413453\n",
      "Epoch: 33, Val loss: 0.0003322921327661532\n",
      "Epoch: 34, Val loss: 0.00022270031630073508\n",
      "Epoch: 35, Val loss: 0.00014526259940631018\n",
      "Epoch: 36, Val loss: 9.205435848203846e-05\n",
      "Epoch: 37, Val loss: 5.671010568711119e-05\n",
      "Epoch: 38, Val loss: 3.3822353520335106e-05\n",
      "Epoch: 39, Val loss: 1.9406003794313456e-05\n",
      "Epoch: 40, Val loss: 1.0708806141151283e-05\n",
      "Epoch: 41, Val loss: 5.65313575862092e-06\n",
      "Epoch: 42, Val loss: 2.8527522900754448e-06\n",
      "Epoch: 43, Val loss: 1.3567542644010152e-06\n",
      "Epoch: 44, Val loss: 6.146582219839612e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:21,165] Trial 1 finished with value: 1.0280315493469644e-07 and parameters: {'lr': 0.0001, 'batch_size': 192.0}. Best is trial 1 with value: 1.0280315493469644e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Val loss: 2.597567284003442e-07\n",
      "Epoch: 46, Val loss: 1.0280315493469644e-07\n",
      "Early stopping at epoch 46\n",
      "Best validation loss this trial: 1.0280315493469644e-07\n",
      "Epoch: 0, Val loss: 1.1686066424031757e-15\n",
      "Epoch: 1, Val loss: 8.534083752269086e-16\n",
      "Epoch: 2, Val loss: 1.3835006612488165e-15\n",
      "Epoch: 3, Val loss: 3.4778827241108188e-15\n",
      "Epoch: 4, Val loss: 1.6579740071181353e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:25,983] Trial 2 finished with value: 5.789339382514129e-07 and parameters: {'lr': 0.01, 'batch_size': 32.0}. Best is trial 1 with value: 1.0280315493469644e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 5.789339382514129e-07\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 5.789339382514129e-07\n",
      "Epoch: 0, Val loss: 9.602559066874984e-12\n",
      "Epoch: 1, Val loss: 3.861126931364208e-15\n",
      "Epoch: 2, Val loss: 3.3969584681219523e-15\n",
      "Epoch: 3, Val loss: 3.323207618069069e-15\n",
      "Epoch: 4, Val loss: 1.9991530783922904e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:27,374] Trial 3 finished with value: 2.1383718084064995e-15 and parameters: {'lr': 0.01, 'batch_size': 128.0}. Best is trial 3 with value: 2.1383718084064995e-15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 2.1383718084064995e-15\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 2.1383718084064995e-15\n",
      "Epoch: 0, Val loss: 1.7162506732401745e-11\n",
      "Epoch: 1, Val loss: 2.385245493703519e-15\n",
      "Epoch: 2, Val loss: 1.816726808318831e-15\n",
      "Epoch: 3, Val loss: 1.3257842886287517e-15\n",
      "Epoch: 4, Val loss: 1.2220400790797309e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:28,789] Trial 4 finished with value: 9.055279846977788e-16 and parameters: {'lr': 0.01, 'batch_size': 128.0}. Best is trial 4 with value: 9.055279846977788e-16.\n",
      "[I 2024-10-30 06:52:28,967] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 9.055279846977788e-16\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 9.055279846977788e-16\n",
      "Epoch: 0, Val loss: 3.7535180934921115e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,272] Trial 6 pruned. \n",
      "[I 2024-10-30 06:52:29,451] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.005632241068479533\n",
      "Epoch: 0, Val loss: 1.1533647085825986e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,659] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 4.051625075629742e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,866] Trial 9 pruned. \n",
      "[I 2024-10-30 06:52:30,019] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.03763038308379498\n",
      "Epoch: 0, Val loss: 0.2619664028286934\n",
      "Epoch: 0, Val loss: 2.706894636996159e-15\n",
      "Epoch: 1, Val loss: 1.1132916213793695e-15\n",
      "Epoch: 2, Val loss: 1.0640098259994128e-15\n",
      "Epoch: 3, Val loss: 9.036677236618428e-16\n",
      "Epoch: 4, Val loss: 1.0032199525453325e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:31,866] Trial 11 finished with value: 9.132470893898361e-16 and parameters: {'lr': 0.01, 'batch_size': 96.0}. Best is trial 4 with value: 9.055279846977788e-16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 9.132470893898361e-16\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 9.132470893898361e-16\n",
      "Epoch: 0, Val loss: 0.01872760296250001\n",
      "Epoch: 1, Val loss: 0.013238088621829566\n",
      "Epoch: 2, Val loss: 0.013127447404444981\n",
      "Epoch: 3, Val loss: 0.01316770919574759\n",
      "Epoch: 4, Val loss: 0.013139345372716585\n",
      "Epoch: 5, Val loss: 0.013149433637945315\n",
      "Epoch: 6, Val loss: 0.013173393738002349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:34,327] Trial 0 finished with value: 0.013194964112093052 and parameters: {'lr': 0.001, 'batch_size': 96.0}. Best is trial 0 with value: 0.013194964112093052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Val loss: 0.013194964112093052\n",
      "Early stopping at epoch 7\n",
      "Best validation loss this trial: 0.013194964112093052\n",
      "Epoch: 0, Val loss: 0.013426726435621579\n",
      "Epoch: 1, Val loss: 0.013361607057352861\n",
      "Epoch: 2, Val loss: 0.013364329561591148\n",
      "Epoch: 3, Val loss: 0.013495281680176656\n",
      "Epoch: 4, Val loss: 0.013549954661478599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:35,355] Trial 1 finished with value: 0.013650519866496324 and parameters: {'lr': 0.01, 'batch_size': 256.0}. Best is trial 0 with value: 0.013194964112093052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 0.013469094689935446\n",
      "Epoch: 6, Val loss: 0.013650519866496324\n",
      "Early stopping at epoch 6\n",
      "Best validation loss this trial: 0.013650519866496324\n",
      "Epoch: 0, Val loss: 0.15713797280421624\n",
      "Epoch: 1, Val loss: 0.08408040562883401\n",
      "Epoch: 2, Val loss: 0.05444951470081623\n",
      "Epoch: 3, Val loss: 0.0410430608317256\n",
      "Epoch: 4, Val loss: 0.032807060971091956\n",
      "Epoch: 5, Val loss: 0.026718489945125885\n",
      "Epoch: 6, Val loss: 0.022184215306949157\n",
      "Epoch: 7, Val loss: 0.018878418522385452\n",
      "Epoch: 8, Val loss: 0.016593351637801297\n",
      "Epoch: 9, Val loss: 0.015089014986864267\n",
      "Epoch: 10, Val loss: 0.014170311964475192\n",
      "Epoch: 11, Val loss: 0.013640978254186802\n",
      "Epoch: 12, Val loss: 0.01335692268390304\n",
      "Epoch: 13, Val loss: 0.013225386026673593\n",
      "Epoch: 14, Val loss: 0.013158675963775469\n",
      "Epoch: 15, Val loss: 0.013132614214928487\n",
      "Epoch: 16, Val loss: 0.013125629844860388\n",
      "Epoch: 17, Val loss: 0.013124644457816314\n",
      "Epoch: 18, Val loss: 0.013123012386644498\n",
      "Epoch: 19, Val loss: 0.013120713607909588\n",
      "Epoch: 20, Val loss: 0.013123722818608467\n",
      "Epoch: 21, Val loss: 0.013123371614477573\n",
      "Epoch: 22, Val loss: 0.013125468391734056\n",
      "Epoch: 23, Val loss: 0.01312193600460887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:43,064] Trial 2 finished with value: 0.013119734608783172 and parameters: {'lr': 0.0001, 'batch_size': 96.0}. Best is trial 2 with value: 0.013119734608783172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Val loss: 0.013119734608783172\n",
      "Early stopping at epoch 24\n",
      "Best validation loss this trial: 0.013119734608783172\n",
      "Epoch: 0, Val loss: 0.01327836134025238\n",
      "Epoch: 1, Val loss: 0.013182869539237939\n",
      "Epoch: 2, Val loss: 0.013335156829581939\n",
      "Epoch: 3, Val loss: 0.013334817105792781\n",
      "Epoch: 4, Val loss: 0.013320462553340018\n",
      "Epoch: 5, Val loss: 0.013265026118367529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:48,746] Trial 3 finished with value: 0.013296562073847804 and parameters: {'lr': 0.001, 'batch_size': 32.0}. Best is trial 2 with value: 0.013119734608783172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Val loss: 0.013296562073847804\n",
      "Early stopping at epoch 6\n",
      "Best validation loss this trial: 0.013296562073847804\n",
      "Epoch: 0, Val loss: 0.013379126668293426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-30 06:52:49,279] Trial 4 failed with parameters: {'lr': 0.01, 'batch_size': 160.0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3441/3185484137.py\", line 2, in <lambda>\n",
      "    study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n",
      "  File \"/tmp/ipykernel_3441/689578029.py\", line 24, in linear_objective_NK\n",
      "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader,\n",
      "  File \"/home/ubuntu/nk-paper-2024-1/nk-ml-2024/hyperopt/hyperopt.py\", line 74, in optimise_hparams\n",
      "    predictions = model(x_batch)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\", line 23, in forward\n",
      "    x = self.linear(x)  # Shape: (batch_size, 1)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1528, in _wrapped_call_impl\n",
      "    def _wrapped_call_impl(self, *args, **kwargs):\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-30 06:52:49,280] Trial 4 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Val loss: 0.013429664372605211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3441/3185484137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_studies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3441/3185484137.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_studies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)\n",
      "\u001b[0;32m/tmp/ipykernel_3441/689578029.py\u001b[0m in \u001b[0;36mlinear_objective_NK\u001b[0;34m(trial, train_data, val_data, seq_length, amino_acids, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#run train/val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n\u001b[0m\u001b[1;32m     25\u001b[0m                                \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamino_acids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamino_acids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
      "\u001b[0;32m~/nk-paper-2024-1/nk-ml-2024/hyperopt/hyperopt.py\u001b[0m in \u001b[0;36moptimise_hparams\u001b[0;34m(trial, model, loss_fn, optimizer, train_dataloader, val_dataloader, seq_length, amino_acids, device, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Flatten the input from (batch_size, sequence_length, alphabet_size) to (batch_size, sequence_length * alphabet_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Pass through the linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (batch_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, study in enumerate(linear_studies):\n",
    "    study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n",
    "                                               amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e6cd91b-0322-4473-b9c2-75b25ff2fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'a':1, 'b':2, 'd':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60e99d95-f5b2-4034-8f54-d22ac23d70d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbd95a00-471d-4abf-91ec-c2c4308b7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a, b, c): \n",
    "    return a+b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8caa32b-6fc1-45da-9f49-6f076e0306c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def generate_valid_combinations_transformer(embed_dim_options, max_heads):\n",
    "    valid_combinations = []\n",
    "    \n",
    "    for embed_dim in embed_dim_options:\n",
    "        for num_heads in range(1, max_heads + 1):\n",
    "            if embed_dim % num_heads == 0:\n",
    "                valid_combinations.append((embed_dim, num_heads))\n",
    "    \n",
    "    return valid_combinations\n",
    "\n",
    "def objective_NK(trial, h_param_search_space, model, train_data, val_data, n_epochs=30, patience=5, min_delta=1e-5, device='cuda'):\n",
    "    \"\"\"\n",
    "    High-level function to perform hyperparameter optimisation on models. Define the search space in h_param_search_space (dict) \n",
    "    by specifying model parameter names as keys, and values as optuna trial samplers. Please also specify model parameters needed \n",
    "    for instantiation but that are not being optimised (otherwise model will return error). \n",
    "\n",
    "    trial:                           optuna trial object\n",
    "    h_param_search_space (dict):     dict of hyperparameters {hparam_name: hparam_value}. Specify search space with optuna.trial sampler as value if \n",
    "                                     wanting to optimise that hyperparameter. Example: {'learning_rate': trial.suggest_categorical('lr', [0.01, 0.001, 0.0001]), 'sequence_length':5 }\n",
    "    model (nn.Module):               model to optimise. Do NOT instantiate model with model() on passing.\n",
    "    train_data:                      train data\n",
    "    val_data:                        val data\n",
    "    n_epochs (int):                  number of epochs to train for \n",
    "    patience (int):                  patience for early stopping \n",
    "    mind_delta(float):               min_delta for early stopping   \n",
    "    \"\"\"           \n",
    "\n",
    "    #define search spaces based on model\n",
    "    hpss= h_param_search_space\n",
    "    learning_rate = trial.suggest_categorical('lr', hpss['learning_rate'])\n",
    "    batch_size    = trial.suggest_categorical('batch_size', hpss['batch_size'])\n",
    "    \n",
    "    if model==SequenceRegressionLinear:\n",
    "        model_instance = model(alphabet_size=hpss['alphabet_size'], sequence_length=hpss['sequence_length'])\n",
    "        \n",
    "    elif model==SequenceRegressionMLP:\n",
    "        n_hidden_layers = trial.suggest_int('n_hidden_layers',1, hpss['max_hidden_layers']) #max_hidden_sizes should be an int\n",
    "        hidden_sizes    = [int(trial.suggest_categorical(\"hidden{}_size\".format(i), hpss['hidden_sizes_categorical'])) #hidden_sizes_categorical should be a list of hidden sizes\n",
    "                            for i in range(n_hidden_layers)]\n",
    "        model_instance = model(alphabet_size=hpss['alphabet_size'], sequence_length=hpss['sequence_length'], hidden_sizes=hidden_sizes)\n",
    "\n",
    "    elif model==SequenceRegressionCNN:\n",
    "        num_conv_layers = trial.suggest_int('num_conv_layers', 1, hpss['max_conv_layers']) #max_conv_layers should be an int\n",
    "        n_kernels = [int(trial.suggest_discrete_uniform(\"n_kernels\", hpss['n_kernels_min'], hpss['n_kernels_max'] , hpss['n_kernels_step']))for i in range(num_conv_layers)]      \n",
    "        kernel_sizes = [int(trial.suggest_discrete_uniform(\"kernel_sizes\", hpss['kernel_sizes_min'], hpss['kernel_sizes_max'], 1))for i in range(num_conv_layers)]\n",
    "        model_instance = model(input_channels=hpss['alphabet_size'], sequence_length=hpss['sequence_length'], num_conv_layers=num_conv_layers,\n",
    "                              n_kernels=n_kernels, kernel_sizes=kernel_sizes)\n",
    "    elif model==SequenceRegressionLSTM: \n",
    "        num_layers     = trial.suggest_int('num_layers', 1, hpss['max_lstm_layers']) #max_lstm_layers should be int\n",
    "        hidden_size    = trial.suggest_categorical(\"hidden_size\", hpss['hidden_sizes']) #hidden_sizes should be a list of possible hidden layuer sizes\n",
    "        bidirectional  = hpss['bidirectional']               \n",
    "        model_instance = model(input_size=hpss['alphabet_size'], hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "    \n",
    "    elif model==SequenceRegressionTransformer:\n",
    "        embed_dim_options = hpss['embed_dim_options']\n",
    "        max_heads = hpss['max_heads']\n",
    "        valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)\n",
    "    \n",
    "        d_model, nhead = trial.suggest_categorical(\"embed_dim_num_heads\", valid_combinations)\n",
    "            \n",
    "        num_layers      = trial.suggest_int('num_layers', 1, hpss['max_layers']) #should be int\n",
    "        dim_feedforward = trial.suggest_categorical('dim_feedforward', hpss['feedforward_dims']) # should be list of ints possible dims \n",
    "        max_seq_length  = trial.suggest_categorical(\"max_seq_length\", hpss['max_seq_lengths']) #shold be list of ints of possible max seq lengths                   \n",
    "        model_instance  = model(input_dim=hpss['alphabet_size'], d_model=d_model, nhead=nhead,dim_feedforward=dim_feedforward,\n",
    "                               max_seq_length=max_seq_length)\n",
    "        \n",
    "    # Initialize model with the trial’s hyperparameters\n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_instance.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model_instance, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f18e9420-59dc-4057-a404-3c1c1de71da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:13:32,862] A new study created in memory with name: no-name-25179e94-3cf6-45c8-a3e9-832b9d4f2c06\n",
      "[I 2024-10-30 08:13:32,863] A new study created in memory with name: no-name-b5288090-9856-4902-892c-2480807deac1\n",
      "[I 2024-10-30 08:13:32,864] A new study created in memory with name: no-name-b2a4c1da-9884-493d-a416-37539082c4ef\n",
      "[I 2024-10-30 08:13:32,865] A new study created in memory with name: no-name-24721ba4-7e88-4387-b105-a402bb30fe7b\n",
      "[I 2024-10-30 08:13:32,865] A new study created in memory with name: no-name-2e006a1b-300c-4d92-969f-bea2fa336508\n",
      "[I 2024-10-30 08:13:32,866] A new study created in memory with name: no-name-6c005807-a339-47ff-bf38-de1c11b6b1a8\n"
     ]
    }
   ],
   "source": [
    "linear_hparam_space = {'learning_rate': [0.01, 0.001, 0.0001], 'batch_size': [32, 64, 128, 256], \n",
    "                       'alphabet_size':6, 'sequence_length':6} \n",
    "mlp_hparam_space   = {'learning_rate': [0.01, 0.001, 0.0001], 'batch_size': [32, 64, 128, 256], \n",
    "                       'alphabet_size':6, 'sequence_length':6, 'max_hidden_layers':3,\n",
    "                     'hidden_sizes_categorical': [32,64, 128, 256]} \n",
    "\n",
    "#linear_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "mlp_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5517d1-be87-48f8-b432-ae5f5d260896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.00017392281043319567\n",
      "Epoch: 1, Val loss: 5.126011955464985e-05\n",
      "Epoch: 2, Val loss: 2.6944504097011334e-05\n",
      "Epoch: 3, Val loss: 1.6409008498219735e-05\n",
      "Epoch: 4, Val loss: 1.051667187173241e-05\n",
      "Epoch: 5, Val loss: 7.364031477926015e-06\n",
      "Epoch: 6, Val loss: 4.08083922023144e-06\n",
      "Epoch: 7, Val loss: 3.482142265981519e-06\n",
      "Epoch: 8, Val loss: 2.0205503842127224e-06\n",
      "Epoch: 9, Val loss: 1.6925319740085659e-06\n",
      "Epoch: 10, Val loss: 1.067970995856771e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:13:55,466] Trial 0 finished with value: 9.093471661244466e-07 and parameters: {'lr': 0.01, 'batch_size': 128, 'n_hidden_layers': 3, 'hidden0_size': 128, 'hidden1_size': 256, 'hidden2_size': 32}. Best is trial 0 with value: 9.093471661244466e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Val loss: 9.093471661244466e-07\n",
      "Early stopping at epoch 11\n",
      "Best validation loss this trial: 9.093471661244466e-07\n",
      "Epoch: 0, Val loss: 0.0009504179281190548\n",
      "Epoch: 1, Val loss: 0.00040564202463739895\n",
      "Epoch: 2, Val loss: 0.0003051735728413949\n",
      "Epoch: 3, Val loss: 0.00021636482633144842\n",
      "Epoch: 4, Val loss: 0.00017115103401010856\n",
      "Epoch: 5, Val loss: 0.0001525707238425429\n",
      "Epoch: 6, Val loss: 0.00013904524909861735\n",
      "Epoch: 7, Val loss: 7.99493770275273e-05\n",
      "Epoch: 8, Val loss: 6.163378236676034e-05\n",
      "Epoch: 9, Val loss: 6.351220561664065e-05\n",
      "Epoch: 10, Val loss: 4.9304417779760705e-05\n",
      "Epoch: 11, Val loss: 4.2378104831769655e-05\n",
      "Epoch: 12, Val loss: 4.0326292744591934e-05\n",
      "Epoch: 13, Val loss: 3.0692004952276264e-05\n",
      "Epoch: 14, Val loss: 3.102085472949737e-05\n",
      "Epoch: 15, Val loss: 3.8290762276096175e-05\n",
      "Epoch: 16, Val loss: 2.2425908969320222e-05\n",
      "Epoch: 17, Val loss: 2.862617916862369e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:14:07,179] Trial 0 finished with value: 2.2894871372589743e-05 and parameters: {'lr': 0.001, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 128, 'hidden1_size': 64}. Best is trial 0 with value: 2.2894871372589743e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Val loss: 2.2894871372589743e-05\n",
      "Early stopping at epoch 18\n",
      "Best validation loss this trial: 2.2894871372589743e-05\n",
      "Epoch: 0, Val loss: 0.007956855015980447\n",
      "Epoch: 1, Val loss: 0.005038755915016254\n",
      "Epoch: 2, Val loss: 0.0039705223549141464\n",
      "Epoch: 3, Val loss: 0.003580946085226332\n",
      "Epoch: 4, Val loss: 0.0048224216448461525\n",
      "Epoch: 5, Val loss: 0.003953304169122448\n",
      "Epoch: 6, Val loss: 0.00341567943075624\n",
      "Epoch: 7, Val loss: 0.003365459873810665\n",
      "Epoch: 8, Val loss: 0.003135322858320756\n",
      "Epoch: 9, Val loss: 0.0033804976334596355\n",
      "Epoch: 10, Val loss: 0.0031459285707499543\n",
      "Epoch: 11, Val loss: 0.0034445363726729574\n",
      "Epoch: 12, Val loss: 0.002786370167413201\n",
      "Epoch: 13, Val loss: 0.003170101470552767\n",
      "Epoch: 14, Val loss: 0.0028750169380472447\n",
      "Epoch: 15, Val loss: 0.002870042857606537\n",
      "Epoch: 16, Val loss: 0.002973034053057846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:14:25,179] Trial 0 finished with value: 0.0029827114498107415 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 1, 'hidden0_size': 256}. Best is trial 0 with value: 0.0029827114498107415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Val loss: 0.0029827114498107415\n",
      "Early stopping at epoch 17\n",
      "Best validation loss this trial: 0.0029827114498107415\n",
      "Epoch: 0, Val loss: 0.017838632612306084\n",
      "Epoch: 1, Val loss: 0.016260554705563392\n",
      "Epoch: 2, Val loss: 0.014092595987101523\n",
      "Epoch: 3, Val loss: 0.014102688216421211\n",
      "Epoch: 4, Val loss: 0.012350788568823129\n",
      "Epoch: 5, Val loss: 0.012262025993301446\n",
      "Epoch: 6, Val loss: 0.010714968389823522\n",
      "Epoch: 7, Val loss: 0.010431886441671314\n",
      "Epoch: 8, Val loss: 0.009608377837854573\n",
      "Epoch: 9, Val loss: 0.009360926826763103\n",
      "Epoch: 10, Val loss: 0.008979608794339957\n",
      "Epoch: 11, Val loss: 0.008701346516131591\n",
      "Epoch: 12, Val loss: 0.00844527532159486\n",
      "Epoch: 13, Val loss: 0.008192596477496192\n",
      "Epoch: 14, Val loss: 0.008238772741264194\n",
      "Epoch: 15, Val loss: 0.007836056532911383\n",
      "Epoch: 16, Val loss: 0.007676330473648113\n",
      "Epoch: 17, Val loss: 0.008153558713702373\n",
      "Epoch: 18, Val loss: 0.007631317220835222\n",
      "Epoch: 19, Val loss: 0.007745776819980616\n",
      "Epoch: 20, Val loss: 0.007472964725059131\n",
      "Epoch: 21, Val loss: 0.0074541593093870795\n",
      "Epoch: 22, Val loss: 0.00731719263160649\n",
      "Epoch: 23, Val loss: 0.006957829937848271\n",
      "Epoch: 24, Val loss: 0.007039446764800729\n",
      "Epoch: 25, Val loss: 0.00713912488367313\n",
      "Epoch: 26, Val loss: 0.006877246117776531\n",
      "Epoch: 27, Val loss: 0.006764068190629284\n",
      "Epoch: 28, Val loss: 0.007378285492046012\n",
      "Epoch: 29, Val loss: 0.006916626648077916\n",
      "Epoch: 30, Val loss: 0.006692037466937342\n",
      "Epoch: 31, Val loss: 0.006931861942140465\n",
      "Epoch: 32, Val loss: 0.006787344424300787\n",
      "Epoch: 33, Val loss: 0.006672434250099791\n",
      "Epoch: 34, Val loss: 0.006603838891809822\n",
      "Epoch: 35, Val loss: 0.0066676214566199574\n",
      "Epoch: 36, Val loss: 0.0066414779434219385\n",
      "Epoch: 37, Val loss: 0.006671451420212786\n",
      "Epoch: 38, Val loss: 0.006946488611925489\n",
      "Epoch: 39, Val loss: 0.00654652014048372\n",
      "Epoch: 40, Val loss: 0.006729409038925018\n",
      "Epoch: 41, Val loss: 0.006481029671362132\n",
      "Epoch: 42, Val loss: 0.006372909155140957\n",
      "Epoch: 43, Val loss: 0.006539556201802105\n",
      "Epoch: 44, Val loss: 0.006598982641584853\n",
      "Epoch: 45, Val loss: 0.006521967044657367\n",
      "Epoch: 46, Val loss: 0.006489160204003764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:28,981] Trial 0 finished with value: 0.006539100508452353 and parameters: {'lr': 0.001, 'batch_size': 32, 'n_hidden_layers': 3, 'hidden0_size': 32, 'hidden1_size': 256, 'hidden2_size': 256}. Best is trial 0 with value: 0.006539100508452353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Val loss: 0.006539100508452353\n",
      "Early stopping at epoch 47\n",
      "Best validation loss this trial: 0.006539100508452353\n",
      "Epoch: 0, Val loss: 0.01864490962276856\n",
      "Epoch: 1, Val loss: 0.018579411755005517\n",
      "Epoch: 2, Val loss: 0.01862091158206264\n",
      "Epoch: 3, Val loss: 0.01873551824440559\n",
      "Epoch: 4, Val loss: 0.01847749805698792\n",
      "Epoch: 5, Val loss: 0.01838134943197171\n",
      "Epoch: 6, Val loss: 0.0184380360879004\n",
      "Epoch: 7, Val loss: 0.018319017812609674\n",
      "Epoch: 8, Val loss: 0.01829816078146299\n",
      "Epoch: 9, Val loss: 0.018225470433632533\n",
      "Epoch: 10, Val loss: 0.018240938832362493\n",
      "Epoch: 11, Val loss: 0.018282002490013837\n",
      "Epoch: 12, Val loss: 0.018381074878076713\n",
      "Epoch: 13, Val loss: 0.01828776200612386\n",
      "Epoch: 14, Val loss: 0.018172867471973102\n",
      "Epoch: 15, Val loss: 0.018188741182287534\n",
      "Epoch: 16, Val loss: 0.018419481161981822\n",
      "Epoch: 17, Val loss: 0.018176968737194935\n",
      "Epoch: 18, Val loss: 0.018162983749061824\n",
      "Epoch: 19, Val loss: 0.018124219651023548\n",
      "Epoch: 20, Val loss: 0.01808136502901713\n",
      "Epoch: 21, Val loss: 0.018171203757325807\n",
      "Epoch: 22, Val loss: 0.0180281954507033\n",
      "Epoch: 23, Val loss: 0.018156274035573006\n",
      "Epoch: 24, Val loss: 0.018249166881044707\n",
      "Epoch: 25, Val loss: 0.018079485868414243\n",
      "Epoch: 26, Val loss: 0.018227955574790636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:34,076] Trial 0 finished with value: 0.018230341374874115 and parameters: {'lr': 0.01, 'batch_size': 256, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 0.018230341374874115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Val loss: 0.018230341374874115\n",
      "Early stopping at epoch 27\n",
      "Best validation loss this trial: 0.018230341374874115\n",
      "Epoch: 0, Val loss: 0.019459166150126193\n",
      "Epoch: 1, Val loss: 0.01932551234992396\n",
      "Epoch: 2, Val loss: 0.019207427628402017\n",
      "Epoch: 3, Val loss: 0.019099191778427005\n",
      "Epoch: 4, Val loss: 0.01930389311323818\n",
      "Epoch: 5, Val loss: 0.01994352389731978\n",
      "Epoch: 6, Val loss: 0.019216043603980642\n",
      "Epoch: 7, Val loss: 0.01946127998172982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:43,086] Trial 0 finished with value: 0.019110098513018373 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 1, 'hidden0_size': 128}. Best is trial 0 with value: 0.019110098513018373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Val loss: 0.019110098513018373\n",
      "Early stopping at epoch 8\n",
      "Best validation loss this trial: 0.019110098513018373\n"
     ]
    }
   ],
   "source": [
    "for index, study in enumerate(mlp_studies):\n",
    "    study.optimize(lambda trial: objective_NK(trial, mlp_hparam_space, SequenceRegressionMLP, \n",
    "                                              train_data= xy_train[index], val_data=xy_val[index], n_epochs=50), n_trials=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ff23f-bb00-414a-8ce1-4738626b008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
