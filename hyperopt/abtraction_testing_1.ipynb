{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3e378c-0a8c-45fa-832e-1908c298c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahakaran/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna as opt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mahakaran/NK-paper-12-5-24-version/nk-ml-paper2-2024/pscapes')\n",
    "sys.path.append('/home/mahakaran/NK-paper-12-5-24-version/nk-ml-paper2-2024/nk-ml-2024/')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pscapes.landscape_class import ProteinLandscape\n",
    "from pscapes.utils import dict_to_np_array, np_array_to_dict\n",
    "\n",
    "from src.architectures.architectures import SequenceRegressionCNN, SequenceRegressionLSTM, SequenceRegressionMLP, SequenceRegressionLinear, SequenceRegressionTransformer\n",
    "from src.architectures.ml_utils import train_val_test_split_ohe, train_model, landscapes_ohe_to_numpy\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from src.hyperopt import optimise_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ff1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858be91a-c32e-455f-a64e-d0d6ee136b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6\n",
    "AA_ALPHABET = 'ACDEFG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90aaa7d-7538-4bd4-bd9f-b36f40ea605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load NK landscapes -- only a single replicate for hparam tuning \n",
    "\n",
    "LANDSCAPES = []\n",
    "for k in range(6): \n",
    "    for r in range(1): \n",
    "        landscape = ProteinLandscape(csv_path='../data/nk_landscapes/k{0}_r{1}.csv'.format(k,r), amino_acids=AA_ALPHABET)\n",
    "        LANDSCAPES.append(landscape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de296c25-c04e-4028-b841-10299ccdc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDSCAPES = [i.fit_OHE() for i in LANDSCAPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112696d6-ddac-40aa-a468-a56810985990",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscapes_ohe, xy_train, xy_val, xy_test, x_test, y_test = train_val_test_split_ohe(LANDSCAPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcee5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, xy_train1, xy_val1, xy_test1, x_test1, y_test1 = train_val_test_split_ohe(LANDSCAPES, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e631d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, y_train_1 = landscapes_ohe_to_numpy(xy_train1)\n",
    "x_val1, y_val1      = landscapes_ohe_to_numpy(xy_val1)\n",
    "x_test1, y_test1    = landscapes_ohe_to_numpy(xy_test1)\n",
    "\n",
    "x_train, y_train = landscapes_ohe_to_numpy(xy_train)\n",
    "x_val, y_val      = landscapes_ohe_to_numpy(xy_val)\n",
    "x_test, y_test    = landscapes_ohe_to_numpy(xy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5119c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(x_train1==x_train))\n",
    "print(np.all(y_train_1==y_train))\n",
    "\n",
    "print(np.all(x_val1==x_val))\n",
    "print(np.all(x_test1==x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f4d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = landscapes_ohe_to_numpy(xy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9108d41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(x_train1== x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67bb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a776c8f-9f07-43b7-b395-b331b7e4d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape0_xy_train = xy_train[0]\n",
    "landscape0_xy_val   = xy_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34569c1-e4d7-40ea-a80b-00550851e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    num_conv_layers = trial.suggest_int('num_conv_layers', 1, 2)\n",
    "    \n",
    "    num_kernels = [int(trial.suggest_discrete_uniform(\"n_kernels\", 16, 128, 16))\n",
    "                   for i in range(num_conv_layers)]  \n",
    "    \n",
    "    kernel_sizes = [int(trial.suggest_discrete_uniform(\"kernel_sizes\", 2, 6, 1))\n",
    "                   for i in range(num_conv_layers)]\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = trial.suggest_discrete_uniform('batch_size', 32, 128, 32)\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionCNN(input_channels=len(AA_ALPHABET), sequence_length=SEQ_LEN, \n",
    "                                  num_conv_layers=num_conv_layers, n_kernels=num_kernels, kernel_sizes=kernel_sizes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dummy training and validation data loaders\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e907bfbd-9e56-4129-b680-b447d9bdb5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSequenceRegressionMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malphabet_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SequenceRegressionMLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4da4c8-c114-45e9-b649-2a45c8e5e202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5a7783-80cd-4f78-9b1c-5230c09550bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    n_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 4)\n",
    "    hidden_sizes    = [int(trial.suggest_categorical(\"hidden{}_size\".format(i), [32,64, 96, 128, 256])) \n",
    "                       for i in range(n_hidden_layers)]\n",
    "    print(hidden_sizes)\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 128, 32))\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionMLP(alphabet_size=len(amino_acids), sequence_length=seq_length, hidden_sizes=hidden_sizes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bafd400a-a09f-45dd-8d2a-9e76975dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unidirectional_lstm_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_size    = trial.suggest_categorical(\"hidden_size\", [32,64, 96, 128, 256])\n",
    "                       \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionLSTM(input_size=len(amino_acids), hidden_size=hidden_size, num_layers=num_layers, bidirectional=False)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f60dce-8a86-4bfe-b4a9-087c387ac9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_size    = trial.suggest_categorical(\"hidden_size\", [32,64, 96, 128, 256])\n",
    "                       \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionLSTM(input_size=len(amino_acids), hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba89e1cf-1b9e-4e91-920a-2ff27d71c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_valid_combinations_transformer(embed_dim_options, max_heads):\n",
    "    valid_combinations = []\n",
    "    \n",
    "    for embed_dim in embed_dim_options:\n",
    "        for num_heads in range(1, max_heads + 1):\n",
    "            if embed_dim % num_heads == 0:\n",
    "                valid_combinations.append((embed_dim, num_heads))\n",
    "    \n",
    "    return valid_combinations\n",
    "embed_dim_options = [16, 32, 64, 128]\n",
    "max_heads = 8\n",
    "\n",
    "valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d5f96bf-1210-4c22-8b04-ce786c17cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "    input_dim = len(amino_acids)\n",
    "    #d_model is the embedding \n",
    "\n",
    "    embed_dim_options = [16, 32, 64, 128]\n",
    "    max_heads = 8\n",
    "    valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)\n",
    "\n",
    "    d_model, nhead = trial.suggest_categorical(\"embed_dim_num_heads\", valid_combinations)\n",
    "        \n",
    "    num_layers      = trial.suggest_int('num_layers', 1, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256, 512]) \n",
    "    max_seq_length  = trial.suggest_categorical(\"max_seq_length\", [6, 8, 10, 12])                  \n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionTransformer(input_dim=input_dim, d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                         max_seq_length=max_seq_length)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0e7a8-8e67-4e52-805b-5910a8d19c47",
   "metadata": {},
   "outputs": [],
   "source": [
    " SequenceRegressionLinear(alphabet_size=5, sequence_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa57ca2-f9a1-4228-b810-e3b82486a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_objective_NK(trial, train_data, val_data, seq_length, amino_acids, n_epochs=30,\n",
    "                    patience=5, min_delta=1e-5):\n",
    "    # Define the search space\n",
    "\n",
    "    \n",
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    model = SequenceRegressionLinear(alphabet_size=len(amino_acids), sequence_length=seq_length)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, seq_length=seq_length, amino_acids=amino_acids,\n",
    "                               n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f1ff715-6045-4794-bb17-b4583fcba008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:51:24,796] A new study created in memory with name: no-name-80664c61-0757-4a88-84a8-e0e124b059ea\n",
      "[I 2024-10-30 06:51:24,797] A new study created in memory with name: no-name-3644e0d0-e967-4b9c-a6db-a56ca8250d6d\n",
      "[I 2024-10-30 06:51:24,798] A new study created in memory with name: no-name-56ad802a-a4d7-4112-b72f-d32c5cdc0787\n",
      "[I 2024-10-30 06:51:24,799] A new study created in memory with name: no-name-5a625edb-c9e0-4eb0-ac71-4ad7e2bd9ae6\n",
      "[I 2024-10-30 06:51:24,799] A new study created in memory with name: no-name-537a34c0-aa29-4cb6-acdb-06cba6c79b9d\n",
      "[I 2024-10-30 06:51:24,800] A new study created in memory with name: no-name-9a67bddf-149f-483a-9510-0b3cf2786b3e\n"
     ]
    }
   ],
   "source": [
    "#cnn_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#mlp_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#lstm_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "#transformer_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "linear_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51402487-3306-42cc-a9a5-c1900f13118e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3441/689578029.py:8: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 1.2402881593693669e-15\n",
      "Epoch: 1, Val loss: 1.3197617229092686e-15\n",
      "Epoch: 2, Val loss: 1.828292697591813e-15\n",
      "Epoch: 3, Val loss: 2.6505454891931755e-14\n",
      "Epoch: 4, Val loss: 2.836986701585662e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:12,775] Trial 0 finished with value: 3.3416055099907804e-05 and parameters: {'lr': 0.01, 'batch_size': 32.0}. Best is trial 0 with value: 3.3416055099907804e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 3.3416055099907804e-05\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 3.3416055099907804e-05\n",
      "Epoch: 0, Val loss: 0.5427766327674572\n",
      "Epoch: 1, Val loss: 0.41609393251247895\n",
      "Epoch: 2, Val loss: 0.31599770066065663\n",
      "Epoch: 3, Val loss: 0.2385255109805327\n",
      "Epoch: 4, Val loss: 0.17974791083580408\n",
      "Epoch: 5, Val loss: 0.13632374295057395\n",
      "Epoch: 6, Val loss: 0.10488661894431481\n",
      "Epoch: 7, Val loss: 0.08247372355216588\n",
      "Epoch: 8, Val loss: 0.06673415100727326\n",
      "Epoch: 9, Val loss: 0.05559699351970966\n",
      "Epoch: 10, Val loss: 0.04750126227736473\n",
      "Epoch: 11, Val loss: 0.041304399665349566\n",
      "Epoch: 12, Val loss: 0.03627334119608769\n",
      "Epoch: 13, Val loss: 0.031975625321651116\n",
      "Epoch: 14, Val loss: 0.02813585475087166\n",
      "Epoch: 15, Val loss: 0.02464491420258314\n",
      "Epoch: 16, Val loss: 0.021449160881531544\n",
      "Epoch: 17, Val loss: 0.018525369751911897\n",
      "Epoch: 18, Val loss: 0.01585659236671069\n",
      "Epoch: 19, Val loss: 0.013443658844782757\n",
      "Epoch: 20, Val loss: 0.011282187337294603\n",
      "Epoch: 21, Val loss: 0.00936904530494641\n",
      "Epoch: 22, Val loss: 0.007692156156572776\n",
      "Epoch: 23, Val loss: 0.006241272346904645\n",
      "Epoch: 24, Val loss: 0.004997267895258772\n",
      "Epoch: 25, Val loss: 0.003949393566029194\n",
      "Epoch: 26, Val loss: 0.0030796525713342885\n",
      "Epoch: 27, Val loss: 0.002361623080781637\n",
      "Epoch: 28, Val loss: 0.0017841608406832586\n",
      "Epoch: 29, Val loss: 0.0013251441324917744\n",
      "Epoch: 30, Val loss: 0.0009665800980889263\n",
      "Epoch: 31, Val loss: 0.0006917628805893353\n",
      "Epoch: 32, Val loss: 0.0004849773085413453\n",
      "Epoch: 33, Val loss: 0.0003322921327661532\n",
      "Epoch: 34, Val loss: 0.00022270031630073508\n",
      "Epoch: 35, Val loss: 0.00014526259940631018\n",
      "Epoch: 36, Val loss: 9.205435848203846e-05\n",
      "Epoch: 37, Val loss: 5.671010568711119e-05\n",
      "Epoch: 38, Val loss: 3.3822353520335106e-05\n",
      "Epoch: 39, Val loss: 1.9406003794313456e-05\n",
      "Epoch: 40, Val loss: 1.0708806141151283e-05\n",
      "Epoch: 41, Val loss: 5.65313575862092e-06\n",
      "Epoch: 42, Val loss: 2.8527522900754448e-06\n",
      "Epoch: 43, Val loss: 1.3567542644010152e-06\n",
      "Epoch: 44, Val loss: 6.146582219839612e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:21,165] Trial 1 finished with value: 1.0280315493469644e-07 and parameters: {'lr': 0.0001, 'batch_size': 192.0}. Best is trial 1 with value: 1.0280315493469644e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Val loss: 2.597567284003442e-07\n",
      "Epoch: 46, Val loss: 1.0280315493469644e-07\n",
      "Early stopping at epoch 46\n",
      "Best validation loss this trial: 1.0280315493469644e-07\n",
      "Epoch: 0, Val loss: 1.1686066424031757e-15\n",
      "Epoch: 1, Val loss: 8.534083752269086e-16\n",
      "Epoch: 2, Val loss: 1.3835006612488165e-15\n",
      "Epoch: 3, Val loss: 3.4778827241108188e-15\n",
      "Epoch: 4, Val loss: 1.6579740071181353e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:25,983] Trial 2 finished with value: 5.789339382514129e-07 and parameters: {'lr': 0.01, 'batch_size': 32.0}. Best is trial 1 with value: 1.0280315493469644e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 5.789339382514129e-07\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 5.789339382514129e-07\n",
      "Epoch: 0, Val loss: 9.602559066874984e-12\n",
      "Epoch: 1, Val loss: 3.861126931364208e-15\n",
      "Epoch: 2, Val loss: 3.3969584681219523e-15\n",
      "Epoch: 3, Val loss: 3.323207618069069e-15\n",
      "Epoch: 4, Val loss: 1.9991530783922904e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:27,374] Trial 3 finished with value: 2.1383718084064995e-15 and parameters: {'lr': 0.01, 'batch_size': 128.0}. Best is trial 3 with value: 2.1383718084064995e-15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 2.1383718084064995e-15\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 2.1383718084064995e-15\n",
      "Epoch: 0, Val loss: 1.7162506732401745e-11\n",
      "Epoch: 1, Val loss: 2.385245493703519e-15\n",
      "Epoch: 2, Val loss: 1.816726808318831e-15\n",
      "Epoch: 3, Val loss: 1.3257842886287517e-15\n",
      "Epoch: 4, Val loss: 1.2220400790797309e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:28,789] Trial 4 finished with value: 9.055279846977788e-16 and parameters: {'lr': 0.01, 'batch_size': 128.0}. Best is trial 4 with value: 9.055279846977788e-16.\n",
      "[I 2024-10-30 06:52:28,967] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 9.055279846977788e-16\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 9.055279846977788e-16\n",
      "Epoch: 0, Val loss: 3.7535180934921115e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,272] Trial 6 pruned. \n",
      "[I 2024-10-30 06:52:29,451] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.005632241068479533\n",
      "Epoch: 0, Val loss: 1.1533647085825986e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,659] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 4.051625075629742e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:29,866] Trial 9 pruned. \n",
      "[I 2024-10-30 06:52:30,019] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.03763038308379498\n",
      "Epoch: 0, Val loss: 0.2619664028286934\n",
      "Epoch: 0, Val loss: 2.706894636996159e-15\n",
      "Epoch: 1, Val loss: 1.1132916213793695e-15\n",
      "Epoch: 2, Val loss: 1.0640098259994128e-15\n",
      "Epoch: 3, Val loss: 9.036677236618428e-16\n",
      "Epoch: 4, Val loss: 1.0032199525453325e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:31,866] Trial 11 finished with value: 9.132470893898361e-16 and parameters: {'lr': 0.01, 'batch_size': 96.0}. Best is trial 4 with value: 9.055279846977788e-16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 9.132470893898361e-16\n",
      "Early stopping at epoch 5\n",
      "Best validation loss this trial: 9.132470893898361e-16\n",
      "Epoch: 0, Val loss: 0.01872760296250001\n",
      "Epoch: 1, Val loss: 0.013238088621829566\n",
      "Epoch: 2, Val loss: 0.013127447404444981\n",
      "Epoch: 3, Val loss: 0.01316770919574759\n",
      "Epoch: 4, Val loss: 0.013139345372716585\n",
      "Epoch: 5, Val loss: 0.013149433637945315\n",
      "Epoch: 6, Val loss: 0.013173393738002349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:34,327] Trial 0 finished with value: 0.013194964112093052 and parameters: {'lr': 0.001, 'batch_size': 96.0}. Best is trial 0 with value: 0.013194964112093052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Val loss: 0.013194964112093052\n",
      "Early stopping at epoch 7\n",
      "Best validation loss this trial: 0.013194964112093052\n",
      "Epoch: 0, Val loss: 0.013426726435621579\n",
      "Epoch: 1, Val loss: 0.013361607057352861\n",
      "Epoch: 2, Val loss: 0.013364329561591148\n",
      "Epoch: 3, Val loss: 0.013495281680176656\n",
      "Epoch: 4, Val loss: 0.013549954661478599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:35,355] Trial 1 finished with value: 0.013650519866496324 and parameters: {'lr': 0.01, 'batch_size': 256.0}. Best is trial 0 with value: 0.013194964112093052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Val loss: 0.013469094689935446\n",
      "Epoch: 6, Val loss: 0.013650519866496324\n",
      "Early stopping at epoch 6\n",
      "Best validation loss this trial: 0.013650519866496324\n",
      "Epoch: 0, Val loss: 0.15713797280421624\n",
      "Epoch: 1, Val loss: 0.08408040562883401\n",
      "Epoch: 2, Val loss: 0.05444951470081623\n",
      "Epoch: 3, Val loss: 0.0410430608317256\n",
      "Epoch: 4, Val loss: 0.032807060971091956\n",
      "Epoch: 5, Val loss: 0.026718489945125885\n",
      "Epoch: 6, Val loss: 0.022184215306949157\n",
      "Epoch: 7, Val loss: 0.018878418522385452\n",
      "Epoch: 8, Val loss: 0.016593351637801297\n",
      "Epoch: 9, Val loss: 0.015089014986864267\n",
      "Epoch: 10, Val loss: 0.014170311964475192\n",
      "Epoch: 11, Val loss: 0.013640978254186802\n",
      "Epoch: 12, Val loss: 0.01335692268390304\n",
      "Epoch: 13, Val loss: 0.013225386026673593\n",
      "Epoch: 14, Val loss: 0.013158675963775469\n",
      "Epoch: 15, Val loss: 0.013132614214928487\n",
      "Epoch: 16, Val loss: 0.013125629844860388\n",
      "Epoch: 17, Val loss: 0.013124644457816314\n",
      "Epoch: 18, Val loss: 0.013123012386644498\n",
      "Epoch: 19, Val loss: 0.013120713607909588\n",
      "Epoch: 20, Val loss: 0.013123722818608467\n",
      "Epoch: 21, Val loss: 0.013123371614477573\n",
      "Epoch: 22, Val loss: 0.013125468391734056\n",
      "Epoch: 23, Val loss: 0.01312193600460887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:43,064] Trial 2 finished with value: 0.013119734608783172 and parameters: {'lr': 0.0001, 'batch_size': 96.0}. Best is trial 2 with value: 0.013119734608783172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Val loss: 0.013119734608783172\n",
      "Early stopping at epoch 24\n",
      "Best validation loss this trial: 0.013119734608783172\n",
      "Epoch: 0, Val loss: 0.01327836134025238\n",
      "Epoch: 1, Val loss: 0.013182869539237939\n",
      "Epoch: 2, Val loss: 0.013335156829581939\n",
      "Epoch: 3, Val loss: 0.013334817105792781\n",
      "Epoch: 4, Val loss: 0.013320462553340018\n",
      "Epoch: 5, Val loss: 0.013265026118367529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 06:52:48,746] Trial 3 finished with value: 0.013296562073847804 and parameters: {'lr': 0.001, 'batch_size': 32.0}. Best is trial 2 with value: 0.013119734608783172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Val loss: 0.013296562073847804\n",
      "Early stopping at epoch 6\n",
      "Best validation loss this trial: 0.013296562073847804\n",
      "Epoch: 0, Val loss: 0.013379126668293426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-30 06:52:49,279] Trial 4 failed with parameters: {'lr': 0.01, 'batch_size': 160.0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3441/3185484137.py\", line 2, in <lambda>\n",
      "    study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n",
      "  File \"/tmp/ipykernel_3441/689578029.py\", line 24, in linear_objective_NK\n",
      "    val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader,\n",
      "  File \"/home/ubuntu/nk-paper-2024-1/nk-ml-2024/hyperopt/hyperopt.py\", line 74, in optimise_hparams\n",
      "    predictions = model(x_batch)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\", line 23, in forward\n",
      "    x = self.linear(x)  # Shape: (batch_size, 1)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/nn/modules/module.py\", line 1528, in _wrapped_call_impl\n",
      "    def _wrapped_call_impl(self, *args, **kwargs):\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-30 06:52:49,280] Trial 4 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Val loss: 0.013429664372605211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3441/3185484137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_studies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3441/3185484137.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_studies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)\n",
      "\u001b[0;32m/tmp/ipykernel_3441/689578029.py\u001b[0m in \u001b[0;36mlinear_objective_NK\u001b[0;34m(trial, train_data, val_data, seq_length, amino_acids, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#run train/val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     val_loss = optimise_hparams(trial, model, loss_fn, optimizer, train_loader, \n\u001b[0m\u001b[1;32m     25\u001b[0m                                \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamino_acids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamino_acids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
      "\u001b[0;32m~/nk-paper-2024-1/nk-ml-2024/hyperopt/hyperopt.py\u001b[0m in \u001b[0;36moptimise_hparams\u001b[0;34m(trial, model, loss_fn, optimizer, train_dataloader, val_dataloader, seq_length, amino_acids, device, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nk-paper-2024-1/nk-ml-2024/src/architectures/architectures.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Flatten the input from (batch_size, sequence_length, alphabet_size) to (batch_size, sequence_length * alphabet_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Pass through the linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (batch_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, study in enumerate(linear_studies):\n",
    "    study.optimize(lambda trial: linear_objective_NK(trial, train_data= xy_train[index], val_data=xy_val[index], seq_length=SEQ_LEN,\n",
    "                                               amino_acids=AA_ALPHABET, n_epochs=50), n_trials=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e6cd91b-0322-4473-b9c2-75b25ff2fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'a':1, 'b':2, 'd':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60e99d95-f5b2-4034-8f54-d22ac23d70d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbd95a00-471d-4abf-91ec-c2c4308b7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a, b, c): \n",
    "    return a+b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8caa32b-6fc1-45da-9f49-6f076e0306c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def generate_valid_combinations_transformer(embed_dim_options, max_heads):\n",
    "    valid_combinations = []\n",
    "    \n",
    "    for embed_dim in embed_dim_options:\n",
    "        for num_heads in range(1, max_heads + 1):\n",
    "            if embed_dim % num_heads == 0:\n",
    "                valid_combinations.append((embed_dim, num_heads))\n",
    "    \n",
    "    return valid_combinations\n",
    "\n",
    "def objective_NK(trial, h_param_search_space, model, train_data, val_data, n_epochs=30, patience=5, min_delta=1e-5, device='cuda'):\n",
    "    \"\"\"\n",
    "    High-level function to perform hyperparameter optimisation on models. Define the search space in h_param_search_space (dict) \n",
    "    by specifying model parameter names as keys, and values as optuna trial samplers. Please also specify model parameters needed \n",
    "    for instantiation but that are not being optimised (otherwise model will return error). \n",
    "\n",
    "    trial:                           optuna trial object\n",
    "    h_param_search_space (dict):     dict of hyperparameters {hparam_name: hparam_value}. Specify search space with optuna.trial sampler as value if \n",
    "                                     wanting to optimise that hyperparameter. Example: {'learning_rate': trial.suggest_categorical('lr', [0.01, 0.001, 0.0001]), 'sequence_length':5 }\n",
    "    model (nn.Module):               model to optimise. Do NOT instantiate model with model() on passing.\n",
    "    train_data:                      train data\n",
    "    val_data:                        val data\n",
    "    n_epochs (int):                  number of epochs to train for \n",
    "    patience (int):                  patience for early stopping \n",
    "    mind_delta(float):               min_delta for early stopping   \n",
    "    \"\"\"           \n",
    "\n",
    "    #define search spaces based on model\n",
    "    hpss= h_param_search_space\n",
    "    learning_rate = trial.suggest_categorical('lr', hpss['learning_rate'])\n",
    "    batch_size    = trial.suggest_categorical('batch_size', hpss['batch_size'])\n",
    "    \n",
    "    if model==SequenceRegressionLinear:\n",
    "        model_instance = model(alphabet_size=hpss['alphabet_size'], sequence_length=hpss['sequence_length'])\n",
    "        \n",
    "    elif model==SequenceRegressionMLP:\n",
    "        n_hidden_layers = trial.suggest_int('n_hidden_layers',1, hpss['max_hidden_layers']) #max_hidden_sizes should be an int\n",
    "        hidden_sizes    = [int(trial.suggest_categorical(\"hidden{}_size\".format(i), hpss['hidden_sizes_categorical'])) #hidden_sizes_categorical should be a list of hidden sizes\n",
    "                            for i in range(n_hidden_layers)]\n",
    "        model_instance = model(alphabet_size=hpss['alphabet_size'], sequence_length=hpss['sequence_length'], hidden_sizes=hidden_sizes)\n",
    "\n",
    "    elif model==SequenceRegressionCNN:\n",
    "        num_conv_layers = trial.suggest_int('num_conv_layers', 1, hpss['max_conv_layers']) #max_conv_layers should be an int\n",
    "        n_kernels = [int(trial.suggest_discrete_uniform(\"n_kernels\", hpss['n_kernels_min'], hpss['n_kernels_max'] , hpss['n_kernels_step']))for i in range(num_conv_layers)]      \n",
    "        kernel_sizes = [int(trial.suggest_discrete_uniform(\"kernel_sizes\", hpss['kernel_sizes_min'], hpss['kernel_sizes_max'], 1))for i in range(num_conv_layers)]\n",
    "        model_instance = model(input_channels=hpss['alphabet_size'], sequence_length=hpss['sequence_length'], num_conv_layers=num_conv_layers,\n",
    "                              n_kernels=n_kernels, kernel_sizes=kernel_sizes)\n",
    "    elif model==SequenceRegressionLSTM: \n",
    "        num_layers     = trial.suggest_int('num_layers', 1, hpss['max_lstm_layers']) #max_lstm_layers should be int\n",
    "        hidden_size    = trial.suggest_categorical(\"hidden_size\", hpss['hidden_sizes']) #hidden_sizes should be a list of possible hidden layuer sizes\n",
    "        bidirectional  = hpss['bidirectional']               \n",
    "        model_instance = model(input_size=hpss['alphabet_size'], hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "    \n",
    "    elif model==SequenceRegressionTransformer:\n",
    "        embed_dim_options = hpss['embed_dim_options']\n",
    "        max_heads = hpss['max_heads']\n",
    "        valid_combinations = generate_valid_combinations(embed_dim_options, max_heads)\n",
    "    \n",
    "        d_model, nhead = trial.suggest_categorical(\"embed_dim_num_heads\", valid_combinations)\n",
    "            \n",
    "        num_layers      = trial.suggest_int('num_layers', 1, hpss['max_layers']) #should be int\n",
    "        dim_feedforward = trial.suggest_categorical('dim_feedforward', hpss['feedforward_dims']) # should be list of ints possible dims \n",
    "        max_seq_length  = trial.suggest_categorical(\"max_seq_length\", hpss['max_seq_lengths']) #shold be list of ints of possible max seq lengths                   \n",
    "        model_instance  = model(input_dim=hpss['alphabet_size'], d_model=d_model, nhead=nhead,dim_feedforward=dim_feedforward,\n",
    "                               max_seq_length=max_seq_length)\n",
    "        \n",
    "    # Initialize model with the trialâ€™s hyperparameters\n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_instance.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    #train and val loaders \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    #run train/val\n",
    "    val_loss = optimise_hparams(trial, model_instance, loss_fn, optimizer, train_loader, \n",
    "                               val_loader, n_epochs=n_epochs, patience=patience, min_delta=min_delta, device=device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f18e9420-59dc-4057-a404-3c1c1de71da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:13:32,862] A new study created in memory with name: no-name-25179e94-3cf6-45c8-a3e9-832b9d4f2c06\n",
      "[I 2024-10-30 08:13:32,863] A new study created in memory with name: no-name-b5288090-9856-4902-892c-2480807deac1\n",
      "[I 2024-10-30 08:13:32,864] A new study created in memory with name: no-name-b2a4c1da-9884-493d-a416-37539082c4ef\n",
      "[I 2024-10-30 08:13:32,865] A new study created in memory with name: no-name-24721ba4-7e88-4387-b105-a402bb30fe7b\n",
      "[I 2024-10-30 08:13:32,865] A new study created in memory with name: no-name-2e006a1b-300c-4d92-969f-bea2fa336508\n",
      "[I 2024-10-30 08:13:32,866] A new study created in memory with name: no-name-6c005807-a339-47ff-bf38-de1c11b6b1a8\n"
     ]
    }
   ],
   "source": [
    "linear_hparam_space = {'learning_rate': [0.01, 0.001, 0.0001], 'batch_size': [32, 64, 128, 256], \n",
    "                       'alphabet_size':6, 'sequence_length':6} \n",
    "mlp_hparam_space   = {'learning_rate': [0.01, 0.001, 0.0001], 'batch_size': [32, 64, 128, 256], \n",
    "                       'alphabet_size':6, 'sequence_length':6, 'max_hidden_layers':3,\n",
    "                     'hidden_sizes_categorical': [32,64, 128, 256]} \n",
    "\n",
    "#linear_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n",
    "mlp_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5517d1-be87-48f8-b432-ae5f5d260896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val loss: 0.00017392281043319567\n",
      "Epoch: 1, Val loss: 5.126011955464985e-05\n",
      "Epoch: 2, Val loss: 2.6944504097011334e-05\n",
      "Epoch: 3, Val loss: 1.6409008498219735e-05\n",
      "Epoch: 4, Val loss: 1.051667187173241e-05\n",
      "Epoch: 5, Val loss: 7.364031477926015e-06\n",
      "Epoch: 6, Val loss: 4.08083922023144e-06\n",
      "Epoch: 7, Val loss: 3.482142265981519e-06\n",
      "Epoch: 8, Val loss: 2.0205503842127224e-06\n",
      "Epoch: 9, Val loss: 1.6925319740085659e-06\n",
      "Epoch: 10, Val loss: 1.067970995856771e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:13:55,466] Trial 0 finished with value: 9.093471661244466e-07 and parameters: {'lr': 0.01, 'batch_size': 128, 'n_hidden_layers': 3, 'hidden0_size': 128, 'hidden1_size': 256, 'hidden2_size': 32}. Best is trial 0 with value: 9.093471661244466e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Val loss: 9.093471661244466e-07\n",
      "Early stopping at epoch 11\n",
      "Best validation loss this trial: 9.093471661244466e-07\n",
      "Epoch: 0, Val loss: 0.0009504179281190548\n",
      "Epoch: 1, Val loss: 0.00040564202463739895\n",
      "Epoch: 2, Val loss: 0.0003051735728413949\n",
      "Epoch: 3, Val loss: 0.00021636482633144842\n",
      "Epoch: 4, Val loss: 0.00017115103401010856\n",
      "Epoch: 5, Val loss: 0.0001525707238425429\n",
      "Epoch: 6, Val loss: 0.00013904524909861735\n",
      "Epoch: 7, Val loss: 7.99493770275273e-05\n",
      "Epoch: 8, Val loss: 6.163378236676034e-05\n",
      "Epoch: 9, Val loss: 6.351220561664065e-05\n",
      "Epoch: 10, Val loss: 4.9304417779760705e-05\n",
      "Epoch: 11, Val loss: 4.2378104831769655e-05\n",
      "Epoch: 12, Val loss: 4.0326292744591934e-05\n",
      "Epoch: 13, Val loss: 3.0692004952276264e-05\n",
      "Epoch: 14, Val loss: 3.102085472949737e-05\n",
      "Epoch: 15, Val loss: 3.8290762276096175e-05\n",
      "Epoch: 16, Val loss: 2.2425908969320222e-05\n",
      "Epoch: 17, Val loss: 2.862617916862369e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:14:07,179] Trial 0 finished with value: 2.2894871372589743e-05 and parameters: {'lr': 0.001, 'batch_size': 64, 'n_hidden_layers': 2, 'hidden0_size': 128, 'hidden1_size': 64}. Best is trial 0 with value: 2.2894871372589743e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Val loss: 2.2894871372589743e-05\n",
      "Early stopping at epoch 18\n",
      "Best validation loss this trial: 2.2894871372589743e-05\n",
      "Epoch: 0, Val loss: 0.007956855015980447\n",
      "Epoch: 1, Val loss: 0.005038755915016254\n",
      "Epoch: 2, Val loss: 0.0039705223549141464\n",
      "Epoch: 3, Val loss: 0.003580946085226332\n",
      "Epoch: 4, Val loss: 0.0048224216448461525\n",
      "Epoch: 5, Val loss: 0.003953304169122448\n",
      "Epoch: 6, Val loss: 0.00341567943075624\n",
      "Epoch: 7, Val loss: 0.003365459873810665\n",
      "Epoch: 8, Val loss: 0.003135322858320756\n",
      "Epoch: 9, Val loss: 0.0033804976334596355\n",
      "Epoch: 10, Val loss: 0.0031459285707499543\n",
      "Epoch: 11, Val loss: 0.0034445363726729574\n",
      "Epoch: 12, Val loss: 0.002786370167413201\n",
      "Epoch: 13, Val loss: 0.003170101470552767\n",
      "Epoch: 14, Val loss: 0.0028750169380472447\n",
      "Epoch: 15, Val loss: 0.002870042857606537\n",
      "Epoch: 16, Val loss: 0.002973034053057846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:14:25,179] Trial 0 finished with value: 0.0029827114498107415 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 1, 'hidden0_size': 256}. Best is trial 0 with value: 0.0029827114498107415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Val loss: 0.0029827114498107415\n",
      "Early stopping at epoch 17\n",
      "Best validation loss this trial: 0.0029827114498107415\n",
      "Epoch: 0, Val loss: 0.017838632612306084\n",
      "Epoch: 1, Val loss: 0.016260554705563392\n",
      "Epoch: 2, Val loss: 0.014092595987101523\n",
      "Epoch: 3, Val loss: 0.014102688216421211\n",
      "Epoch: 4, Val loss: 0.012350788568823129\n",
      "Epoch: 5, Val loss: 0.012262025993301446\n",
      "Epoch: 6, Val loss: 0.010714968389823522\n",
      "Epoch: 7, Val loss: 0.010431886441671314\n",
      "Epoch: 8, Val loss: 0.009608377837854573\n",
      "Epoch: 9, Val loss: 0.009360926826763103\n",
      "Epoch: 10, Val loss: 0.008979608794339957\n",
      "Epoch: 11, Val loss: 0.008701346516131591\n",
      "Epoch: 12, Val loss: 0.00844527532159486\n",
      "Epoch: 13, Val loss: 0.008192596477496192\n",
      "Epoch: 14, Val loss: 0.008238772741264194\n",
      "Epoch: 15, Val loss: 0.007836056532911383\n",
      "Epoch: 16, Val loss: 0.007676330473648113\n",
      "Epoch: 17, Val loss: 0.008153558713702373\n",
      "Epoch: 18, Val loss: 0.007631317220835222\n",
      "Epoch: 19, Val loss: 0.007745776819980616\n",
      "Epoch: 20, Val loss: 0.007472964725059131\n",
      "Epoch: 21, Val loss: 0.0074541593093870795\n",
      "Epoch: 22, Val loss: 0.00731719263160649\n",
      "Epoch: 23, Val loss: 0.006957829937848271\n",
      "Epoch: 24, Val loss: 0.007039446764800729\n",
      "Epoch: 25, Val loss: 0.00713912488367313\n",
      "Epoch: 26, Val loss: 0.006877246117776531\n",
      "Epoch: 27, Val loss: 0.006764068190629284\n",
      "Epoch: 28, Val loss: 0.007378285492046012\n",
      "Epoch: 29, Val loss: 0.006916626648077916\n",
      "Epoch: 30, Val loss: 0.006692037466937342\n",
      "Epoch: 31, Val loss: 0.006931861942140465\n",
      "Epoch: 32, Val loss: 0.006787344424300787\n",
      "Epoch: 33, Val loss: 0.006672434250099791\n",
      "Epoch: 34, Val loss: 0.006603838891809822\n",
      "Epoch: 35, Val loss: 0.0066676214566199574\n",
      "Epoch: 36, Val loss: 0.0066414779434219385\n",
      "Epoch: 37, Val loss: 0.006671451420212786\n",
      "Epoch: 38, Val loss: 0.006946488611925489\n",
      "Epoch: 39, Val loss: 0.00654652014048372\n",
      "Epoch: 40, Val loss: 0.006729409038925018\n",
      "Epoch: 41, Val loss: 0.006481029671362132\n",
      "Epoch: 42, Val loss: 0.006372909155140957\n",
      "Epoch: 43, Val loss: 0.006539556201802105\n",
      "Epoch: 44, Val loss: 0.006598982641584853\n",
      "Epoch: 45, Val loss: 0.006521967044657367\n",
      "Epoch: 46, Val loss: 0.006489160204003764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:28,981] Trial 0 finished with value: 0.006539100508452353 and parameters: {'lr': 0.001, 'batch_size': 32, 'n_hidden_layers': 3, 'hidden0_size': 32, 'hidden1_size': 256, 'hidden2_size': 256}. Best is trial 0 with value: 0.006539100508452353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Val loss: 0.006539100508452353\n",
      "Early stopping at epoch 47\n",
      "Best validation loss this trial: 0.006539100508452353\n",
      "Epoch: 0, Val loss: 0.01864490962276856\n",
      "Epoch: 1, Val loss: 0.018579411755005517\n",
      "Epoch: 2, Val loss: 0.01862091158206264\n",
      "Epoch: 3, Val loss: 0.01873551824440559\n",
      "Epoch: 4, Val loss: 0.01847749805698792\n",
      "Epoch: 5, Val loss: 0.01838134943197171\n",
      "Epoch: 6, Val loss: 0.0184380360879004\n",
      "Epoch: 7, Val loss: 0.018319017812609674\n",
      "Epoch: 8, Val loss: 0.01829816078146299\n",
      "Epoch: 9, Val loss: 0.018225470433632533\n",
      "Epoch: 10, Val loss: 0.018240938832362493\n",
      "Epoch: 11, Val loss: 0.018282002490013837\n",
      "Epoch: 12, Val loss: 0.018381074878076713\n",
      "Epoch: 13, Val loss: 0.01828776200612386\n",
      "Epoch: 14, Val loss: 0.018172867471973102\n",
      "Epoch: 15, Val loss: 0.018188741182287534\n",
      "Epoch: 16, Val loss: 0.018419481161981822\n",
      "Epoch: 17, Val loss: 0.018176968737194935\n",
      "Epoch: 18, Val loss: 0.018162983749061824\n",
      "Epoch: 19, Val loss: 0.018124219651023548\n",
      "Epoch: 20, Val loss: 0.01808136502901713\n",
      "Epoch: 21, Val loss: 0.018171203757325807\n",
      "Epoch: 22, Val loss: 0.0180281954507033\n",
      "Epoch: 23, Val loss: 0.018156274035573006\n",
      "Epoch: 24, Val loss: 0.018249166881044707\n",
      "Epoch: 25, Val loss: 0.018079485868414243\n",
      "Epoch: 26, Val loss: 0.018227955574790636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:34,076] Trial 0 finished with value: 0.018230341374874115 and parameters: {'lr': 0.01, 'batch_size': 256, 'n_hidden_layers': 2, 'hidden0_size': 32, 'hidden1_size': 32}. Best is trial 0 with value: 0.018230341374874115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Val loss: 0.018230341374874115\n",
      "Early stopping at epoch 27\n",
      "Best validation loss this trial: 0.018230341374874115\n",
      "Epoch: 0, Val loss: 0.019459166150126193\n",
      "Epoch: 1, Val loss: 0.01932551234992396\n",
      "Epoch: 2, Val loss: 0.019207427628402017\n",
      "Epoch: 3, Val loss: 0.019099191778427005\n",
      "Epoch: 4, Val loss: 0.01930389311323818\n",
      "Epoch: 5, Val loss: 0.01994352389731978\n",
      "Epoch: 6, Val loss: 0.019216043603980642\n",
      "Epoch: 7, Val loss: 0.01946127998172982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 08:15:43,086] Trial 0 finished with value: 0.019110098513018373 and parameters: {'lr': 0.01, 'batch_size': 32, 'n_hidden_layers': 1, 'hidden0_size': 128}. Best is trial 0 with value: 0.019110098513018373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Val loss: 0.019110098513018373\n",
      "Early stopping at epoch 8\n",
      "Best validation loss this trial: 0.019110098513018373\n"
     ]
    }
   ],
   "source": [
    "for index, study in enumerate(mlp_studies):\n",
    "    study.optimize(lambda trial: objective_NK(trial, mlp_hparam_space, SequenceRegressionMLP, \n",
    "                                              train_data= xy_train[index], val_data=xy_val[index], n_epochs=50), n_trials=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ff23f-bb00-414a-8ce1-4738626b008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    learning_rate = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "\n",
    "    batch_size    = int(trial.suggest_discrete_uniform('batch_size', 32, 256, 32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfcd997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebabf9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model distributions\n",
    "RF_param_dist = {\n",
    "    'max_features': uniform(loc=0.1, scale=0.9),\n",
    "    'n_estimators': randint(10,990)}\n",
    "\n",
    "GB_param_dist = {\n",
    "    'max_depth':     randint(1,11),\n",
    "    'n_estimators':  randint(10,990),\n",
    "    'learning_rate': uniform(loc=0.05, scale=0.25),\n",
    "    'subsample':     uniform(loc=0.4, scale=0.6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52a8fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_objective(trial, model_name, x_train, y_train, x_val, y_val):\n",
    "    \"\"\"\n",
    "    model_name (str):   either 'RF' or 'GB'\n",
    "    x_train(np.array):  np array of shape (samples, seq_length*alphabet_size)\n",
    "    y_train(np.array):  np array of shape (samples, )\n",
    "    x_val(np.array):    np array of shape (samples, seq_length*alphabet_size)\n",
    "    y_val(np.array):    np array of shape (samples, )\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name=='RF': \n",
    "        max_features = trial.suggest_float('max_features', 0.1, 1)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "        max_depth    = trial.suggest_int('max_depth', 1, 32)\n",
    "        model        = RandomForestRegressor(max_features=max_features, n_estimators=n_estimators, \n",
    "                                             max_depth=max_depth)\n",
    "    elif model_name=='GB': \n",
    "        max_depth     = trial.suggest_int('max_depth', 1, 32)\n",
    "        n_estimators  = trial.suggest_int('n_estimators', 10, 1000)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.001, 0.2) \n",
    "        model         = GradientBoostingRegressor(max_depth=max_depth, n_estimators=n_estimators, \n",
    "                                                  learning_rate=learning_rate)\n",
    "    else: \n",
    "        raise Exception('Invalid model name. Model name must be \"RF\" or \"GB\".')\n",
    "    \n",
    "    print('Fitting model in trial.')\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred   = model.predict(x_val)\n",
    "    val_loss =  mean_squared_error(y_val, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc59e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f035916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = landscapes_ohe_to_numpy(xy_train)\n",
    "x_val, y_val     = landscapes_ohe_to_numpy(xy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93a86259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29860, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d275dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:50:37,343] A new study created in memory with name: no-name-6166b32d-b282-432b-905e-c8abed6d388d\n",
      "[I 2024-10-30 22:50:37,343] A new study created in memory with name: no-name-1c06e222-323f-4750-96fc-5c02787f4bc0\n",
      "[I 2024-10-30 22:50:37,344] A new study created in memory with name: no-name-535909d4-eed6-47d6-84a3-0d5734c8908d\n",
      "[I 2024-10-30 22:50:37,344] A new study created in memory with name: no-name-c31d54c2-d05d-4c99-b570-1c7d1d143894\n",
      "[I 2024-10-30 22:50:37,345] A new study created in memory with name: no-name-41fecafc-4135-4ce4-89ca-735842a2558e\n",
      "[I 2024-10-30 22:50:37,345] A new study created in memory with name: no-name-a05aed48-3d1e-4d44-a2dc-f300f08e200e\n"
     ]
    }
   ],
   "source": [
    "RF_studies = [opt.create_study(direction='minimize') for i in LANDSCAPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e6e41ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:51:03,458] Trial 0 finished with value: 0.00016890712058731386 and parameters: {'max_features': 0.28880508402247385, 'n_estimators': 683, 'max_depth': 21}. Best is trial 0 with value: 0.00016890712058731386.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:51:08,131] Trial 1 finished with value: 0.00011861275169441352 and parameters: {'max_features': 0.9054510438863227, 'n_estimators': 67, 'max_depth': 30}. Best is trial 1 with value: 0.00011861275169441352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:51:09,794] Trial 2 finished with value: 0.00017508127120181884 and parameters: {'max_features': 0.35797697738755085, 'n_estimators': 51, 'max_depth': 21}. Best is trial 1 with value: 0.00011861275169441352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:51:59,604] Trial 3 finished with value: 0.00011814274082978396 and parameters: {'max_features': 0.8810350897025029, 'n_estimators': 640, 'max_depth': 32}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:05,409] Trial 4 finished with value: 0.00012450847127455224 and parameters: {'max_features': 0.9292891076107291, 'n_estimators': 81, 'max_depth': 30}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:25,612] Trial 5 finished with value: 0.004175895751941392 and parameters: {'max_features': 0.6798475232919872, 'n_estimators': 760, 'max_depth': 8}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:43,073] Trial 6 finished with value: 0.005761696989339968 and parameters: {'max_features': 0.9841963132272797, 'n_estimators': 506, 'max_depth': 7}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:52,041] Trial 7 finished with value: 0.0005364423772612715 and parameters: {'max_features': 0.46931077997484916, 'n_estimators': 265, 'max_depth': 14}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:54,229] Trial 8 finished with value: 0.0011495043772321483 and parameters: {'max_features': 0.2122251741612762, 'n_estimators': 104, 'max_depth': 13}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:52:55,942] Trial 9 finished with value: 0.018753167186406097 and parameters: {'max_features': 0.6926248941068196, 'n_estimators': 163, 'max_depth': 1}. Best is trial 3 with value: 0.00011814274082978396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model in trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-30 22:53:26,375] Trial 10 failed with parameters: {'max_features': 0.6950773370474254, 'n_estimators': 955, 'max_depth': 24} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_40543/174818296.py\", line 2, in <lambda>\n",
      "    study.optimize(lambda trial: sklearn_objective(trial, 'RF', x_train[index], y_train[index].ravel(),\n",
      "  File \"/tmp/ipykernel_40543/2770641570.py\", line 27, in sklearn_objective\n",
      "    model.fit(x_train, y_train)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 473, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1088, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 184, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 1247, in fit\n",
      "    super().fit(\n",
      "  File \"/home/mahakaran/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 379, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-30 22:53:26,377] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, study \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(RF_studies):\n\u001b[0;32m----> 2\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: sklearn_objective(trial, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m, x_train[index], y_train[index]\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[1;32m      3\u001b[0m                                                    x_val[index], y_val[index]\u001b[38;5;241m.\u001b[39mravel()), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, study \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(RF_studies):\n\u001b[0;32m----> 2\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43msklearn_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "Cell \u001b[0;32mIn [41], line 27\u001b[0m, in \u001b[0;36msklearn_objective\u001b[0;34m(trial, model_name, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid model name. Model name must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting model in trial.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m y_pred   \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[1;32m     29\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m  mean_squared_error(y_val, y_pred)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, study in enumerate(RF_studies):\n",
    "    study.optimize(lambda trial: sklearn_objective(trial, 'RF', x_train[index], y_train[index].ravel(),\n",
    "                                                   x_val[index], y_val[index].ravel()), n_trials=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e3dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
